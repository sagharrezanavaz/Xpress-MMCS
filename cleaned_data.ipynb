{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:14.069484Z",
     "start_time": "2025-11-21T13:13:14.056143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys"
   ],
   "id": "81268aaa4cb40cd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:18.786076Z",
     "start_time": "2025-11-21T13:13:14.071243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- USER CONFIG -----\n",
    "# Directory where your 36 CSV files live\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data')  # <- updated path\n",
    "# Output filename for the combined CSV\n",
    "OUTPUT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\combined_od_with_datew.csv')\n",
    "# Pattern to match files (will match filenames containing YYYY_MM or YYYY-MM)\n",
    "GLOB_PATTERN = '*_counts*.csv'\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "def extract_year_month_from_name(fname: str):\n",
    "    \n",
    "    \"\"\"Return (year, month) tuple if found in filename, else None.\"\"\"\n",
    "    # look for 4-digit year, separator (_ or -), 2-digit month\n",
    "    m = re.search(r'(\\d{4})[_-](\\d{2})', fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, month = m.group(1), m.group(2)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def main(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(f'No files found matching pattern {GLOB_PATTERN} in {input_dir.resolve()}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for f in files:\n",
    "        ym = extract_year_month_from_name(f.name)\n",
    "        if ym is None:\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        year, month = ym\n",
    "        # read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {f.name}: {e}')\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        # add date columns in two common formats:\n",
    "        # 'year_month' = 'YYYY-MM' and 'month_year' = 'MM/YYYY' (user asked for m/y)\n",
    "        df['year_month'] = f\"{year}-{month}\"\n",
    "        df['month_year'] = f\"{month}/{year}\"\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print('No CSVs successfully read (maybe filename pattern is different). Files skipped:')\n",
    "        print('\\n'.join(skipped))\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Optional: reorder so date columns are near the front\n",
    "    cols = list(combined.columns)\n",
    "    for col in ['year_month', 'month_year']:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # Save the combined CSV\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f'Combined {len(dfs)} files into {output_file} (total rows: {len(combined)})')\n",
    "    if skipped:\n",
    "        print('Skipped files (no YYYY_MM found or read error):')\n",
    "        print('\\n'.join(skipped))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(INPUT_DIR, OUTPUT_FILE)\n"
   ],
   "id": "7dc3b5da354ab4bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\counts-data\\combined_od_with_datew.csv (total rows: 268488)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:20.954080Z",
     "start_time": "2025-11-21T13:13:18.786076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "eb276d40c246370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268488 entries, 0 to 268487\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   month_year        268488 non-null  object \n",
      " 1   year_month        268488 non-null  object \n",
      " 2   start_station_id  268488 non-null  int64  \n",
      " 3   end_station_id    268488 non-null  int64  \n",
      " 4   hour              268488 non-null  float64\n",
      " 5   trip_count        268488 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 12.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       start_station_id  end_station_id           hour     trip_count\n",
       "count     268488.000000   268488.000000  268488.000000  268488.000000\n",
       "mean         930.490595      976.395388      13.844608       1.714162\n",
       "std          662.431144      671.219020       4.854848       1.803095\n",
       "min          171.000000      171.000000       0.000000       1.000000\n",
       "25%          260.000000      262.000000      11.000000       1.000000\n",
       "50%         1024.000000     1025.000000      14.000000       1.000000\n",
       "75%         1729.000000     1737.000000      17.000000       2.000000\n",
       "max         2268.000000     2268.000000      23.000000      84.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.490595</td>\n",
       "      <td>976.395388</td>\n",
       "      <td>13.844608</td>\n",
       "      <td>1.714162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>662.431144</td>\n",
       "      <td>671.219020</td>\n",
       "      <td>4.854848</td>\n",
       "      <td>1.803095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2268.000000</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:21.135198Z",
     "start_time": "2025-11-21T13:13:20.954080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REPORT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\data_quality_report.txt')\n",
    "# Ensure correct data types\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce').astype('Int64')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce').astype('Int64')\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['trip_count'] = pd.to_numeric(df['trip_count'], errors='coerce').astype('Int64')\n",
    "\n",
    "# ----- MISSING VALUES -----\n",
    "missing_summary = df.isna().sum()\n",
    "missing_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(f\"Found {len(missing_rows):,} rows with missing values — will drop them.\")\n",
    "    df = df.dropna()"
   ],
   "id": "c1a1ae76fbf985c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:21.369793Z",
     "start_time": "2025-11-21T13:13:21.135198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- DUPLICATES -----\n",
    "num_dupes = df.duplicated().sum()\n",
    "if num_dupes > 0:\n",
    "    print(f\"Dropping {num_dupes:,} duplicate rows.\")\n",
    "    df = df.drop_duplicates()"
   ],
   "id": "80502a5ea7e47868",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:21.389931Z",
     "start_time": "2025-11-21T13:13:21.369793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- VALUE VALIDATION -----\n",
    "# Check valid range for hour (0–23 expected)\n",
    "invalid_hours = df[~df['hour'].between(0, 24)]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours):,} rows with invalid hour values (outside 0–24). Fixing...\")\n",
    "    df = df[df['hour'].between(0, 24)]"
   ],
   "id": "211dc1e4af4c94a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:21.673956Z",
     "start_time": "2025-11-21T13:13:21.389931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check station ID ranges\n",
    "df_s=pd.read_csv(r'E:\\Uni_PGT\\station_data.csv')\n",
    "m=list(df_s['station_id'].unique())\n",
    "w=df['start_station_id'].unique()\n",
    "missing_ids = [s for s in w if s not in m]\n",
    "print(missing_ids)\n",
    "print(len(missing_ids))\n"
   ],
   "id": "29fbcd4e747abd69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(171), np.int64(255), np.int64(257), np.int64(261), np.int64(266), np.int64(273), np.int64(275), np.int64(277), np.int64(284), np.int64(285), np.int64(290), np.int64(297), np.int64(340), np.int64(341), np.int64(342), np.int64(343), np.int64(344), np.int64(345), np.int64(346), np.int64(347), np.int64(350), np.int64(351), np.int64(352), np.int64(353), np.int64(354), np.int64(355), np.int64(356), np.int64(357), np.int64(359), np.int64(365), np.int64(366), np.int64(648), np.int64(820), np.int64(860), np.int64(862), np.int64(863), np.int64(864), np.int64(865), np.int64(866), np.int64(867), np.int64(868), np.int64(869), np.int64(870), np.int64(871), np.int64(872), np.int64(873), np.int64(874), np.int64(875), np.int64(876), np.int64(877), np.int64(878), np.int64(879), np.int64(880), np.int64(881), np.int64(882), np.int64(885), np.int64(887), np.int64(888), np.int64(889), np.int64(883), np.int64(884), np.int64(890), np.int64(891), np.int64(901), np.int64(964), np.int64(965), np.int64(980), np.int64(981), np.int64(982), np.int64(991), np.int64(1018), np.int64(1026), np.int64(299), np.int64(1027), np.int64(1030), np.int64(1031), np.int64(1032), np.int64(1033), np.int64(1040), np.int64(1041), np.int64(1042), np.int64(1055), np.int64(1056), np.int64(1057), np.int64(1094), np.int64(1095), np.int64(1723), np.int64(1724), np.int64(1731), np.int64(1740), np.int64(1743), np.int64(1746), np.int64(1747), np.int64(1752), np.int64(1764), np.int64(1766), np.int64(1799), np.int64(1800), np.int64(1808), np.int64(1857), np.int64(1859), np.int64(1864), np.int64(1865), np.int64(1866), np.int64(1868), np.int64(1869), np.int64(1870), np.int64(1871), np.int64(1874), np.int64(1877), np.int64(1860)]\n",
      "111\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:21.737414Z",
     "start_time": "2025-11-21T13:13:21.683081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- REPORT -----\n",
    "with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('OD Matrix Data Quality Report\\n')\n",
    "    f.write('=' * 40 + '\\n\\n')\n",
    "    f.write(f'Total rows after cleaning: {len(df):,}\\n')\n",
    "    f.write(f'Duplicates removed: {num_dupes}\\n')\n",
    "    f.write(f'Missing rows removed: {len(missing_rows)}\\n')\n",
    "    f.write(f'Invalid hour rows removed: {len(invalid_hours)}\\n\\n')\n",
    "\n",
    "\n",
    "    f.write('Trip count summary (post-clean):\\n')\n",
    "    f.write(str(df['trip_count'].describe()) + '\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Data quality report saved to: {REPORT_FILE}')\n",
    "\n"
   ],
   "id": "c9a8a18a268937df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to: E:\\Uni_PGT\\counts-data\\data_quality_report.txt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:49.921231Z",
     "start_time": "2025-11-21T13:13:21.737414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "combine_36_csvs_no_date.py\n",
    "\n",
    "Reads all CSV files from cyclehire-cleandata named like 2018_10.csv ... 2021_09.csv\n",
    "(or containing that yyyy_mm pattern in the filename), concatenates them into a single CSV\n",
    "and saves it to cyclehire-cleandata\\combined_all_periods.csv\n",
    "\n",
    "Notes:\n",
    " - This script does NOT add a date column (as requested).\n",
    " - It will align columns by name; missing columns in some files will be filled with NaN.\n",
    " - It prints a short summary of files read and total rows combined.\n",
    "\n",
    "Usage:\n",
    "    python combine_36_csvs_no_date.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata')\n",
    "OUTPUT_FILE = INPUT_DIR / 'combined_all_periods.csv'\n",
    "GLOB_PATTERN = '*_*.csv'  # matches files with yyyy_mm in name like 2018_10.csv\n",
    "FILE_FILTER_REGEX = re.compile(r'(20\\d{2})[_-](0[1-9]|1[0-2])')  # restrict to yyyy_mm patterns\n",
    "# ------------------------\n",
    "\n",
    "# collect candidate files\n",
    "files = sorted(INPUT_DIR.glob(GLOB_PATTERN))\n",
    "selected_files = [f for f in files if FILE_FILTER_REGEX.search(f.name)]\n",
    "\n",
    "if not selected_files:\n",
    "    raise FileNotFoundError(f'No files matching yyyy_mm pattern found in {INPUT_DIR}')\n",
    "\n",
    "print(f'Found {len(selected_files)} files to combine:')\n",
    "for f in selected_files:\n",
    "    print(' -', f.name)\n",
    "\n",
    "# read and concatenate\n",
    "dfs = []\n",
    "for f in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name  # optional: keep which file the row came from\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to read {f.name}: {e}')\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError('No files were successfully read.')\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "combined.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f'Combined {len(dfs)} files into {OUTPUT_FILE} (total rows: {len(combined):,})')\n",
    "\n",
    "# optional quick sanity print\n",
    "print('\\nColumn summary (name : non-null count):')\n",
    "print(combined.notna().sum().sort_values(ascending=False).head(50))\n",
    "\n",
    "print('\\nDone.')\n"
   ],
   "id": "5973531e49f04c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 files to combine:\n",
      " - 2018_10.csv\n",
      " - 2018_11.csv\n",
      " - 2018_12.csv\n",
      " - 2019_01.csv\n",
      " - 2019_02.csv\n",
      " - 2019_03.csv\n",
      " - 2019_04.csv\n",
      " - 2019_05.csv\n",
      " - 2019_06.csv\n",
      " - 2019_07.csv\n",
      " - 2019_08.csv\n",
      " - 2019_09.csv\n",
      " - 2019_10.csv\n",
      " - 2019_11.csv\n",
      " - 2019_12.csv\n",
      " - 2020_01.csv\n",
      " - 2020_02.csv\n",
      " - 2020_03.csv\n",
      " - 2020_04.csv\n",
      " - 2020_05.csv\n",
      " - 2020_06.csv\n",
      " - 2020_07.csv\n",
      " - 2020_08.csv\n",
      " - 2020_09.csv\n",
      " - 2020_10.csv\n",
      " - 2020_11.csv\n",
      " - 2020_12.csv\n",
      " - 2021_01.csv\n",
      " - 2021_02.csv\n",
      " - 2021_03.csv\n",
      " - 2021_04.csv\n",
      " - 2021_05.csv\n",
      " - 2021_06.csv\n",
      " - 2021_07.csv\n",
      " - 2021_08.csv\n",
      " - 2021_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_21108\\2259740896.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\cyclehire-cleandata\\combined_all_periods.csv (total rows: 460,655)\n",
      "\n",
      "Column summary (name : non-null count):\n",
      "started_at                   460655\n",
      "ended_at                     460655\n",
      "duration                     460655\n",
      "start_station_id             460655\n",
      "start_station_name           460655\n",
      "start_station_latitude       460655\n",
      "end_station_latitude         460655\n",
      "start_station_longitude      460655\n",
      "end_station_id               460655\n",
      "end_station_name             460655\n",
      "__source_file                460655\n",
      "end_station_longitude        460655\n",
      "start_station_description    456167\n",
      "end_station_description      455560\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:13:50.710821Z",
     "start_time": "2025-11-21T13:13:49.925110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(combined.info())\n",
    "print(combined.describe())"
   ],
   "id": "3e84a0d30fe9b22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460655 entries, 0 to 460654\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   started_at                 460655 non-null  object \n",
      " 1   ended_at                   460655 non-null  object \n",
      " 2   duration                   460655 non-null  int64  \n",
      " 3   start_station_id           460655 non-null  int64  \n",
      " 4   start_station_name         460655 non-null  object \n",
      " 5   start_station_description  456167 non-null  object \n",
      " 6   start_station_latitude     460655 non-null  float64\n",
      " 7   start_station_longitude    460655 non-null  float64\n",
      " 8   end_station_id             460655 non-null  int64  \n",
      " 9   end_station_name           460655 non-null  object \n",
      " 10  end_station_description    455560 non-null  object \n",
      " 11  end_station_latitude       460655 non-null  float64\n",
      " 12  end_station_longitude      460655 non-null  float64\n",
      " 13  __source_file              460655 non-null  object \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 49.2+ MB\n",
      "None\n",
      "           duration  start_station_id  start_station_latitude  \\\n",
      "count  4.606550e+05     460655.000000           460655.000000   \n",
      "mean   1.945961e+03        936.878545               55.950615   \n",
      "std    5.531311e+03        671.956842                0.013497   \n",
      "min    6.100000e+01        171.000000               55.908404   \n",
      "25%    6.280000e+02        261.000000               55.940238   \n",
      "50%    1.166000e+03       1024.000000               55.947600   \n",
      "75%    2.527000e+03       1729.000000               55.958566   \n",
      "max    2.363348e+06       2268.000000               55.992957   \n",
      "\n",
      "       start_station_longitude  end_station_id  end_station_latitude  \\\n",
      "count            460655.000000   460655.000000         460655.000000   \n",
      "mean                 -3.196412      983.399353             55.952535   \n",
      "std                   0.039072      678.334765              0.015748   \n",
      "min                  -3.407156      171.000000             53.395525   \n",
      "25%                  -3.207964      262.000000             55.941791   \n",
      "50%                  -3.192444     1025.000000             55.951501   \n",
      "75%                  -3.180693     1737.000000             55.962487   \n",
      "max                  -3.058307     2268.000000             55.992957   \n",
      "\n",
      "       end_station_longitude  \n",
      "count          460655.000000  \n",
      "mean               -3.195134  \n",
      "std                 0.041796  \n",
      "min                -3.407156  \n",
      "25%                -3.208070  \n",
      "50%                -3.191421  \n",
      "75%                -3.176351  \n",
      "max                -2.990138  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:04.745707Z",
     "start_time": "2025-11-21T13:13:50.711997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# contiguous_hour_clustering_and_gravity_avg_per_day.py\n",
    "\"\"\"\n",
    "Same clustering + gravity script you provided, but changes to produce\n",
    "OD aggregated per cluster *averaged per calendar day*.\n",
    "\n",
    "Key change: after summing hourly OD matrices for a cluster we divide\n",
    "by `n_days` (the number of unique calendar dates present in the\n",
    "`combined` DataFrame). This turns cluster totals (total trips across\n",
    "all days in the dataset occurring during those cluster hours) into\n",
    "an average number of trips per calendar day for that cluster.\n",
    "\n",
    "Notes / caveats:\n",
    " - `n_days` is computed globally from `combined['started_at'].dt.date.nunique()`.\n",
    "   This is the simplest and most defensible choice: it gives average trips\n",
    "   per calendar day across the whole observation window. If you prefer to\n",
    "   compute `n_days` per-hour or per-cluster (e.g. count of days that actually\n",
    "   contained at least one trip in the cluster hours), see the comment below\n",
    "   and I can provide that variant.\n",
    " - The averaged `agg` (OD) is then used to compute O and D (origin/destination\n",
    "   totals) and run the gravity model. The gravity model will therefore predict\n",
    "   average trips per calendar day for that cluster period.\n",
    "\n",
    "Run this in the same session where `combined` (the concatenated trip DataFrame)\n",
    "exists in memory.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity_avg_per_day')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLUSTER_COUNT = 4            # number of contiguous clusters to form\n",
    "PCA_VARIANCE = 0.90          # keep PCA components explaining this fraction of variance\n",
    "BETA_1 = 0.0005\n",
    "ERROR_THRESHOLD = 0.01\n",
    "IMPROVEMENT_THRESHOLD = 1e-6\n",
    "MAX_ITERS = 2000\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# Ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first as `combined`.')\n",
    "\n",
    "# Prepare datetime and hour\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "if 'hour' not in combined.columns:\n",
    "    combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Compute number of calendar days present in the dataset (global)\n",
    "# This is used to convert cluster totals -> average trips per calendar day\n",
    "n_days = combined['started_at'].dt.date.nunique()\n",
    "print(f\"Dataset spans {n_days} calendar days (unique dates) — cluster OD will be averaged per calendar day.\")"
   ],
   "id": "4611b3dfd81ce088",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset spans 1083 calendar days (unique dates) — cluster OD will be averaged per calendar day.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:06.587985Z",
     "start_time": "2025-11-21T13:14:04.745707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Optional alternative (commented): compute n_days_per_cluster by counting unique dates\n",
    "# with at least one trip in cluster hours. This can yield slightly different averages\n",
    "# that only count days where cluster hours had any activity. If you prefer that,\n",
    "# uncomment and use the per-cluster approach shown later in a comment.\n",
    "\n",
    "# Precompute hourly OD pivot tables (raw counts per hour)\n",
    "hourly_pivots = {}\n",
    "for h in range(24):\n",
    "    sub = combined[combined['hour'] == h]\n",
    "    if sub.empty:\n",
    "        hourly_pivots[h] = pd.DataFrame()\n",
    "        continue\n",
    "    counts = sub.groupby(['start_station_id', 'end_station_id']).size().reset_index(name='count')\n",
    "    pivot = counts.pivot(index='start_station_id', columns='end_station_id', values='count').fillna(0)\n",
    "    hourly_pivots[h] = pivot\n",
    "\n",
    "# Build aligned feature vectors for each hour using union of station ids\n",
    "hours = sorted(hourly_pivots.keys())\n",
    "all_stations = sorted({int(s) for h in hours for s in (list(hourly_pivots[h].index) + list(hourly_pivots[h].columns))})\n",
    "if len(all_stations) == 0:\n",
    "    raise RuntimeError('No station IDs found in hourly pivots--check combined data')\n",
    "\n",
    "def pivot_to_aligned_vector(pivot_df, stations):\n",
    "    mat = pd.DataFrame(0.0, index=stations, columns=stations)\n",
    "    if not pivot_df.empty:\n",
    "        tmp = pivot_df.reindex(index=stations, columns=stations).fillna(0)\n",
    "        mat.iloc[:, :] = tmp.values\n",
    "    return mat.values.flatten()\n",
    "\n",
    "X_list = []\n",
    "for h in hours:\n",
    "    vec = pivot_to_aligned_vector(hourly_pivots[h], all_stations)\n",
    "    X_list.append(vec)\n",
    "X = np.vstack(X_list)  # shape (24, n_features)\n",
    "\n",
    "# Scale + PCA\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=PCA_VARIANCE, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Build chain connectivity (adjacency) for contiguous clustering\n",
    "n_hours = X_pca.shape[0]\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for i in range(n_hours - 1):\n",
    "    rows.extend([i, i+1])\n",
    "    cols.extend([i+1, i])\n",
    "    data.extend([1, 1])\n",
    "connectivity = csr_matrix((data, (rows, cols)), shape=(n_hours, n_hours))\n",
    "\n",
    "# Run contiguous agglomerative clustering\n",
    "agg = AgglomerativeClustering(n_clusters=CLUSTER_COUNT, linkage='ward', connectivity=connectivity)\n",
    "labels = agg.fit_predict(X_pca)\n",
    "\n",
    "hour_cluster_df = pd.DataFrame({'hour': hours, 'cluster': labels}).sort_values('hour')\n",
    "hour_cluster_df.to_csv(OUTPUT_DIR / 'cluster_membership.csv', index=False)\n",
    "print('Hour -> Cluster mapping saved to', OUTPUT_DIR / 'cluster_membership.csv')"
   ],
   "id": "813e6b3019c3a9e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour -> Cluster mapping saved to E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity_avg_per_day\\cluster_membership.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:09.815638Z",
     "start_time": "2025-11-21T13:14:06.588858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Helper: haversine pairwise\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Impedance function (single det1 used here)\n",
    "def new_cost1(cost_matrix, beta=BETA_1):\n",
    "    return np.exp(-beta * cost_matrix)\n",
    "\n",
    "# Gravity model (doubly-constrained IPF)\n",
    "def gravity_model(O, D, det, error_threshold=ERROR_THRESHOLD, improvement_threshold=IMPROVEMENT_THRESHOLD, max_iters=MAX_ITERS):\n",
    "    O = np.array(O, dtype=float).copy()\n",
    "    D = np.array(D, dtype=float).copy()\n",
    "    sum_O = O.sum(); sum_D = D.sum()\n",
    "    if sum_O <= 0 or sum_D <= 0:\n",
    "        raise ValueError('Origin or Destination totals sum to zero')\n",
    "    if abs(sum_O - sum_D) > 1e-9:\n",
    "        D = D * (sum_O / sum_D)\n",
    "    n = len(O)\n",
    "    Ai = np.ones(n)\n",
    "    Bj = np.ones(n)\n",
    "    prev_error = np.inf\n",
    "    Tij = np.zeros((n, n), dtype=float)\n",
    "    det_mat = np.array(det, dtype=float).copy()\n",
    "    det_mat[det_mat < EPS] = EPS\n",
    "    iteration = 0\n",
    "    while iteration < max_iters:\n",
    "        iteration += 1\n",
    "        denom_i = (det_mat * (Bj * D)[None, :]).sum(axis=1) + EPS\n",
    "        Ai = 1.0 / denom_i\n",
    "        denom_j = (det_mat * (Ai * O)[:, None]).sum(axis=0) + EPS\n",
    "        Bj_new = 1.0 / denom_j\n",
    "        Tij = (Ai * O)[:, None] * (Bj_new * D)[None, :] * det_mat\n",
    "        error = (np.abs(O - Tij.sum(axis=1)).sum() + np.abs(D - Tij.sum(axis=0)).sum()) / (sum_O + EPS)\n",
    "        improvement = abs(prev_error - error)\n",
    "        if error < error_threshold:\n",
    "            stop_reason = 'Error threshold met'\n",
    "            break\n",
    "        if improvement < improvement_threshold:\n",
    "            stop_reason = 'Slow improvement'\n",
    "            break\n",
    "        prev_error = error\n",
    "        Bj = Bj_new\n",
    "    else:\n",
    "        stop_reason = 'max_iters'\n",
    "    diagnostics = {'iterations': iteration, 'error': float(error), 'stop_reason': stop_reason}\n",
    "    return Tij, diagnostics\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(predicted_T, observed_T_df):\n",
    "    obs = observed_T_df.to_numpy().astype(float)\n",
    "    pred = np.array(predicted_T, dtype=float)\n",
    "    if obs.shape != pred.shape:\n",
    "        raise ValueError('Predicted and observed shapes differ')\n",
    "    obs_f = obs.flatten(); pred_f = pred.flatten()\n",
    "    mse = np.mean((obs_f - pred_f) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    ss_tot = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "    ss_res = np.sum((obs_f - pred_f) ** 2)\n",
    "    r2 = float(1.0 - (ss_res / (ss_tot + EPS)))\n",
    "    return {'rmse': rmse, 'r2': r2}\n",
    "\n",
    "# Aggregate OD per cluster and run gravity — now averaging per calendar day\n",
    "for c in sorted(hour_cluster_df['cluster'].unique()):\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster'] == c]['hour'].tolist()\n",
    "    print(f'Processing cluster {c}: hours = {hrs}')\n",
    "    # sum OD counts across hours in cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    agg = agg.fillna(0)\n",
    "\n",
    "    # ----- NEW: convert cluster totals -> average per calendar day -----\n",
    "    # Divide the aggregated matrix by the number of calendar days in the dataset\n",
    "    # so that values represent \"average trips per calendar day during the cluster hours\".\n",
    "    if n_days > 0:\n",
    "        agg = agg / float(n_days)\n",
    "    else:\n",
    "        print('Warning: n_days==0, skipping division (no date information)')\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    if agg.empty:\n",
    "        print(f'Cluster {c}: empty aggregated OD, skipping')\n",
    "        continue\n",
    "\n",
    "    # ensure square by intersection\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c}: too few common stations ({len(common)}), skipping')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "\n",
    "    # Save the averaged cluster OD (this CSV now contains average trips per calendar day for the cluster hours)\n",
    "    agg.to_csv(OUTPUT_DIR / f'cluster_od_avgperday_{c}.csv')\n",
    "\n",
    "    n = len(common)\n",
    "    print(f'Cluster {c}: n_stations={n} (averaged over {n_days} days)')\n",
    "    if n > 1500:\n",
    "        print('WARNING: cluster has many stations (>1500); this may be slow and memory-heavy')\n",
    "\n",
    "    # build coord df (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station lon/lat columns in combined')\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "    if len(coord_df) != n:\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        if missing:\n",
    "            print(f'Cluster {c}: dropping {len(missing)} stations with missing coords (sample): {list(missing)[:10]}')\n",
    "            keep = [s for s in common if s not in missing]\n",
    "            if len(keep) < 2:\n",
    "                print(f'Cluster {c}: too few stations after dropping, skipping')\n",
    "                continue\n",
    "            agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "            coord_df = coord_df.reindex(keep)\n",
    "            common = keep\n",
    "            n = len(common)\n",
    "\n",
    "    # compute cost matrix\n",
    "    lons = coord_df['lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df['lat'].to_numpy(dtype=float)\n",
    "    cost_m = haversine_pairwise(lons, lats)\n",
    "    pd.DataFrame(cost_m, index=common, columns=common).to_csv(OUTPUT_DIR / f'cost_matrix_cluster_{c}.csv')\n",
    "\n",
    "    # deterrence matrix\n",
    "    det1 = new_cost1(cost_m, beta=BETA_1)\n",
    "\n",
    "    # totals (these totals are now average trips per calendar day for the cluster hours)\n",
    "    O = agg.sum(axis=1).to_numpy()\n",
    "    D = agg.sum(axis=0).to_numpy()\n",
    "\n",
    "    # run gravity\n",
    "    Tij1, diag1 = gravity_model(O.copy(), D.copy(), det1)\n",
    "    pred1_df = pd.DataFrame(Tij1, index=agg.index, columns=agg.columns)\n",
    "    pred1_df.to_csv(OUTPUT_DIR / f'predicted_gravity_det1_cluster_{c}_avgperday.csv')\n",
    "    metrics1 = calculate_metrics(pred1_df, agg)\n",
    "\n",
    "    pd.DataFrame([diag1]).to_csv(OUTPUT_DIR / f'diagnostics_det1_cluster_{c}.csv', index=False)\n",
    "    pd.DataFrame([metrics1]).to_csv(OUTPUT_DIR / f'metrics_det1_cluster_{c}.csv', index=False)\n",
    "\n",
    "    print(f'Cluster {c}: done. metrics_det1={metrics1}')\n",
    "\n",
    "print('All clusters processed. Outputs in', OUTPUT_DIR)\n"
   ],
   "id": "e8359de2fc7a9129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 0: hours = [11, 12, 13, 14, 15]\n",
      "Cluster 0: n_stations=191 (averaged over 1083 days)\n",
      "Cluster 0: done. metrics_det1={'rmse': 0.0165686802815451, 'r2': 0.5223031333344694}\n",
      "Processing cluster 1: hours = [16, 17]\n",
      "Cluster 1: n_stations=186 (averaged over 1083 days)\n",
      "Cluster 1: done. metrics_det1={'rmse': 0.006940969466649425, 'r2': 0.5754631478891745}\n",
      "Processing cluster 2: hours = [18, 19, 20, 21, 22, 23]\n",
      "Cluster 2: n_stations=182 (averaged over 1083 days)\n",
      "Cluster 2: done. metrics_det1={'rmse': 0.008103426581723673, 'r2': 0.6272312037982717}\n",
      "Processing cluster 3: hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Cluster 3: n_stations=187 (averaged over 1083 days)\n",
      "Cluster 3: done. metrics_det1={'rmse': 0.00854909312389969, 'r2': 0.5308287108154313}\n",
      "All clusters processed. Outputs in E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity_avg_per_day\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:14.258852Z",
     "start_time": "2025-11-21T13:14:09.816663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# beta_grid_det1_avg_per_day.py\n",
    "# Updated beta-grid search script that expects cluster-aggregated OD to be\n",
    "# averaged per calendar day before running gravity model.\n",
    "# This version computes n_days from the `combined` DataFrame and divides the\n",
    "# aggregated cluster OD by n_days (so O/D represent average trips per calendar day).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- USER / RUN-TIME CONFIG ----------\n",
    "beta_grid = np.logspace(-6, -2, 25)   # grid to search for beta (adjust if desired)\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity')  # match your cluster script\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EPS = 1e-12\n",
    "# --------------------------------------------\n",
    "\n",
    "# Basic haversine (meters)\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Normalized RMSE helper\n",
    "def normalized_rmse(pred_df, obs_df):\n",
    "    obs = obs_df.to_numpy(dtype=float)\n",
    "    pred = pred_df.to_numpy(dtype=float)\n",
    "    mse = np.mean((obs - pred) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mean_obs = float(np.mean(obs))\n",
    "    if mean_obs == 0:\n",
    "        return {'rmse': rmse, 'nrmse': float('inf')}\n",
    "    return {'rmse': rmse, 'nrmse': float(rmse / mean_obs)}\n",
    "\n",
    "# Ensure gravity_model & calculate_metrics exist (from your previous script). If not, provide safe fallback.\n",
    "try:\n",
    "    gravity_model  # noqa\n",
    "except NameError:\n",
    "    raise RuntimeError(\"gravity_model is not defined in the session. Run the clustering/gravity script first (which defines gravity_model).\")\n",
    "\n",
    "try:\n",
    "    calculate_metrics  # noqa\n",
    "except NameError:\n",
    "    # fallback basic metrics (should be similar to your earlier calculate_metrics)\n",
    "    def calculate_metrics(pred_df, obs_df):\n",
    "        obs = obs_df.to_numpy(dtype=float)\n",
    "        pred = pred_df.to_numpy(dtype=float)\n",
    "        if obs.shape != pred.shape:\n",
    "            raise ValueError('Predicted and observed shapes differ')\n",
    "        obs_f = obs.flatten()\n",
    "        pred_f = pred.flatten()\n",
    "        mse = np.mean((obs_f - pred_f) ** 2)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        ss_total = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "        ss_residual = np.sum((obs_f - pred_f) ** 2)\n",
    "        r_squared = float(1.0 - (ss_residual / (ss_total + EPS)))\n",
    "        return {'rmse': rmse, 'r2': r_squared}\n",
    "\n",
    "# Core function: runs grid search for det1 on a single aggregated cluster\n",
    "def run_beta_grid_on_cluster(agg_df, coord_df, cost_func, det_func_factory, beta_values, gravity_model_func, metrics_func):\n",
    "    \"\"\"\n",
    "    Returns best = {'beta', 'metrics', 'pred_df', 'diag'} using primary=r2, secondary=-nrmse\n",
    "    \"\"\"\n",
    "    common = [int(x) for x in agg_df.index.tolist()]\n",
    "    lons = coord_df.loc[common, 'lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df.loc[common, 'lat'].to_numpy(dtype=float)\n",
    "    cost_m = cost_func(lons, lats)\n",
    "\n",
    "    O = agg_df.sum(axis=1).to_numpy()\n",
    "    D = agg_df.sum(axis=0).to_numpy()\n",
    "\n",
    "    best = {'beta': None, 'metrics': None, 'pred_df': None, 'diag': None}\n",
    "    best_score = None\n",
    "\n",
    "    for beta in beta_values:\n",
    "        det = det_func_factory(cost_m, beta=beta)\n",
    "        try:\n",
    "            Tij, diag = gravity_model_func(O.copy(), D.copy(), det)\n",
    "        except Exception:\n",
    "            # solver failure for this beta — skip\n",
    "            continue\n",
    "        pred_df = pd.DataFrame(Tij, index=agg_df.index, columns=agg_df.columns)\n",
    "        mets = metrics_func(pred_df, agg_df)    # should include 'r2'\n",
    "        nr = normalized_rmse(pred_df, agg_df)\n",
    "        mets.update(nr)\n",
    "        score = (mets.get('r2', -9999), - (mets.get('nrmse', np.inf) if np.isfinite(mets.get('nrmse', np.inf)) else np.inf))\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best = {'beta': float(beta), 'metrics': mets, 'pred_df': pred_df.copy(), 'diag': diag}\n",
    "    return best\n",
    "\n",
    "# DET1 factory (exponential), no det2 per your request\n",
    "def det1_factory(costm, beta):\n",
    "    return np.exp(-beta * costm)\n",
    "\n",
    "# Containers for results\n",
    "best_betas = {}          # {cluster: {'beta':..., 'metrics':..., 'diag':...}}\n",
    "predicted_matrices = {}  # {cluster: predicted_df}\n",
    "\n",
    "# Compute number of calendar days in the combined dataset (used to convert cluster totals -> avg per calendar day)\n",
    "try:\n",
    "    n_days = int(combined['started_at'].dt.date.nunique())\n",
    "except Exception:\n",
    "    # fallback: if combined not present or started_at not parsed, set to 1 (no scaling)\n",
    "    n_days = 1\n",
    "\n",
    "clusters = sorted(hour_cluster_df['cluster'].unique())\n",
    "\n",
    "for c in clusters:\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster']==c]['hour'].tolist()\n",
    "    # aggregate OD across hours in this cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    if agg is None:\n",
    "        print(f'Cluster {c}: empty aggregation, skipping')\n",
    "        continue\n",
    "    agg = agg.fillna(0)\n",
    "\n",
    "    # --- NEW: convert aggregated cluster totals to average trips per calendar day ---\n",
    "    if n_days > 1:\n",
    "        agg = agg / float(n_days)\n",
    "    # save the averaged cluster OD for diagnostic purposes\n",
    "    agg.to_csv(OUTPUT_DIR / f'cluster_od_avg_per_day_{c}.csv')\n",
    "\n",
    "    # square matrix by intersection of rows & cols\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c} skipped (too few common stations)')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "\n",
    "    # build coordinate DF for these common stations (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station longitude/latitude columns in `combined`.')\n",
    "\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "\n",
    "    if len(coord_df) != len(common):\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        print(f'Cluster {c}: dropping {len(missing)} stations missing coords (sample): {list(missing)[:6]}')\n",
    "        keep = [s for s in common if s not in missing]\n",
    "        agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "        coord_df = coord_df.reindex(keep)\n",
    "        common = keep\n",
    "        if len(common) < 2:\n",
    "            print(f'Cluster {c}: too few stations after dropping coords, skipping')\n",
    "            continue\n",
    "\n",
    "    # run grid search for det1 only\n",
    "    best1 = run_beta_grid_on_cluster(agg, coord_df, haversine_pairwise, det1_factory, beta_grid, gravity_model, calculate_metrics)\n",
    "\n",
    "    # save best1 outputs\n",
    "    if best1['beta'] is None:\n",
    "        print(f'Cluster {c}: no successful beta found (solver failed for all candidates)')\n",
    "        continue\n",
    "\n",
    "    best_betas[c] = {'beta_det1': best1['beta'], 'metrics_det1': best1['metrics'], 'diag_det1': best1['diag']}\n",
    "    predicted_matrices[c] = best1['pred_df']\n",
    "\n",
    "    # persist to disk\n",
    "    best1['pred_df'].to_csv(OUTPUT_DIR / f'best_pred_det1_clusterr_{c}.csv')\n",
    "    pd.DataFrame([best1['metrics']]).to_csv(OUTPUT_DIR / f'best_metrics_det1_clusterr_{c}.csv', index=False)\n",
    "\n",
    "    print(f\"Cluster {c}: done. best_beta_det1={best1['beta']:.2e}, metrics={best1['metrics']}\")\n",
    "\n",
    "# save summary CSV\n",
    "summary = []\n",
    "for c, info in best_betas.items():\n",
    "    summary.append({'cluster': c, 'best_beta_det1': info['beta_det1'], **{f\"metrics_{k}\": v for k,v in info['metrics_det1'].items()}})\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv', index=False)\n",
    "\n",
    "print('Grid search (det1 only) completed. Summary saved to:', OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv')\n",
    "print('Best betas dict: variable `best_betas` (in memory).')\n",
    "print('Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).')"
   ],
   "id": "37462ec51c3d6ba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: done. best_beta_det1=3.16e-04, metrics={'rmse': 0.01511597184838509, 'r2': 0.6023978664038023, 'nrmse': 3.3690183047494133}\n",
      "Cluster 1: done. best_beta_det1=4.64e-04, metrics={'rmse': 0.0068731200005717336, 'r2': 0.583722459017577, 'nrmse': 3.1473002820950526}\n",
      "Cluster 2: done. best_beta_det1=6.81e-04, metrics={'rmse': 0.007998473455017589, 'r2': 0.6368246502400101, 'nrmse': 2.905371397098946}\n",
      "Cluster 3: done. best_beta_det1=4.64e-04, metrics={'rmse': 0.0085127296413361, 'r2': 0.5348114533084086, 'nrmse': 3.1500856842441234}\n",
      "Grid search (det1 only) completed. Summary saved to: E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity\\beta_gridsearch_summary_det1_only.csv\n",
      "Best betas dict: variable `best_betas` (in memory).\n",
      "Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:14.275019Z",
     "start_time": "2025-11-21T13:14:14.259344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "station_to_poi_od_from_predicted_fixed.py\n",
    "\n",
    "Fixed version of the POI-OD conversion that uses gravity-model predicted\n",
    "station->station OD matrices (per cluster), and avoids the ValueError you ran\n",
    "into by correctly sizing the POI index for each output matrix.\n",
    "\n",
    "Assumptions:\n",
    " - `predicted_matrices` is a dict in memory mapping cluster -> pandas.DataFrame\n",
    "   with station IDs as index and columns (square) containing predicted Tij counts.\n",
    " - `combined` DataFrame is in memory (for station coords).\n",
    " - POIs CSV is at POI_FILE and contains columns ['lat','lon','category'] and\n",
    "   optionally 'poi_id'.\n",
    "\n",
    "Saves per-cluster POI OD CSVs (only for POIs that receive >0 allocation for\n",
    "stations in that cluster), plus an aggregated POI OD across clusters.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ],
   "id": "8d08ba7ec5dac7e0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:14.297287Z",
     "start_time": "2025-11-21T13:14:14.278086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- load POIs ---\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "CATEGORY_WEIGHTS_uniform = {\n",
    "    'library': 1,\n",
    "    'school': 1,\n",
    "    'university': 1,\n",
    "    'residential': 1,\n",
    "    'commercial': 1,\n",
    "    'hospital': 1\n",
    "}\n"
   ],
   "id": "a26a37250cfaa580",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:14.315561Z",
     "start_time": "2025-11-21T13:14:14.297287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a mapping matrix (these are allocation percentages)\n",
    "TRAVEL_TO_POI_MAPPING = {\n",
    "    'Commuting (Travel)': {\n",
    "        'residential': 0.40,    # Trip origins from home\n",
    "        'commercial': 0.35,     # Work destinations\n",
    "        'university': 0.15,     # Student commuting\n",
    "        'school': 0.10      # Government jobs\n",
    "    },\n",
    "    'Business (Travel)': {\n",
    "        'commercial': 0.90,     # Business meetings, offices\n",
    "        'hospital': 0.10        # Business at medical facilities\n",
    "    },\n",
    "    'Education/escort education (Travel)': {\n",
    "        'school': 0.4,         # Primary/secondary schools\n",
    "        'university': 0.6     # Higher education\n",
    "    },\n",
    "    'Shopping (Leisure)': {\n",
    "        'commercial': 1.00      # All shopping trips\n",
    "    },\n",
    "    'Other escort (Travel)': {\n",
    "        'school': 0.30,         # Dropping off kids\n",
    "        'hospital': 0.30,       # Medical appointments\n",
    "        'commercial': 0.20,     # Errands while escorting\n",
    "        'residential': 0.20    # To/from homes\n",
    "    },\n",
    "    'Personal business (Travel)': {\n",
    "        'commercial': 0.35,     # Banks, services\n",
    "        'hospital': 0.35,       # Medical\n",
    "        'library': 0.20,        # Community services\n",
    "        'residential': 0.10    # To/from homes\n",
    "\n",
    "    },\n",
    "    'Leisure': {\n",
    "        'commercial': 0.75,     # Restaurants, entertainment\n",
    "        'library': 0.10,        # Libraries, museums\n",
    "        'residential': 0.15 \n",
    "    }\n",
    "}"
   ],
   "id": "22233a31ceb3592f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:14.334136Z",
     "start_time": "2025-11-21T13:14:14.315561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use your actual travel percentages\n",
    "travel_data = {\n",
    "    'Commuting (Travel)': 0.28,\n",
    "    'Business (Travel)': 0.02,\n",
    "    'Education/escort education (Travel)': 0.11,\n",
    "    'Shopping (Leisure)': 0.10,\n",
    "    'Other escort (Travel)': 0.02,\n",
    "    'Personal business (Travel)': 0.06,\n",
    "    'Leisure': 0.43\n",
    "}\n",
    "\n",
    "# Calculate final weights\n",
    "poi_weights = {}\n",
    "for travel_purpose, travel_pct in travel_data.items():\n",
    "    allocation = TRAVEL_TO_POI_MAPPING[travel_purpose]\n",
    "    for poi_category, alloc_pct in allocation.items():\n",
    "        poi_weights[poi_category] = poi_weights.get(poi_category, 0) + (travel_pct * alloc_pct)\n",
    "\n",
    "# Normalize\n",
    "total = sum(poi_weights.values())\n",
    "category_weights_pred = {k: v/total for k, v in poi_weights.items()}\n",
    "print(category_weights_pred)"
   ],
   "id": "216293640984f0f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'residential': 0.18284313725490198, 'commercial': 0.5524509803921569, 'university': 0.10588235294117648, 'school': 0.07647058823529412, 'hospital': 0.028431372549019604, 'library': 0.05392156862745098}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:20.707627Z",
     "start_time": "2025-11-21T13:14:14.334136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- DBSCAN CLUSTERING ---\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Commercial-focused weights (commercial POIs weighted higher)\n",
    "CATEGORY_WEIGHTS_COMMERCIAL = {\n",
    "    'library': 1,\n",
    "    'school': 1,\n",
    "    'university': 1,\n",
    "    'residential': 1,\n",
    "    'commercial': 15,  # Commercial POIs get more weight\n",
    "    'hospital': 1\n",
    "}\n",
    "def diff_weights(cat_weight):\n",
    "    EPS = 1e-12\n",
    "    if cat_weight['commercial']==cat_weight['library']:\n",
    "        t='uniform'\n",
    "    elif cat_weight['commercial']<15*cat_weight['library']:\n",
    "        t='pred'\n",
    "    else:\n",
    "        t='commercial'\n",
    "    pois = pd.read_csv(POI_FILE)\n",
    "    pois = pois.dropna(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "    if 'poi_id' not in pois.columns:\n",
    "        pois['poi_id'] = pois.index.astype(int)\n",
    "    pois['category'] = pois['category'].astype(str).fillna('commercial').str.lower().str.strip()\n",
    "    pois['weight'] = pois['category'].map(cat_weight).fillna(1.0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"DBSCAN clustering...\")\n",
    "    \n",
    "    original_pois = pois.copy()\n",
    "    clustered_pois_list = []\n",
    "    current_poi_id = 0\n",
    "    \n",
    "    # Process in smaller batches by category to reduce memory usage\n",
    "    for category in pois['category'].unique():\n",
    "        category_pois = pois[pois['category'] == category].copy().reset_index(drop=True)\n",
    "        \n",
    "        if len(category_pois) <= 1:\n",
    "            # Single POI in this category - keep as is\n",
    "            for _, poi in category_pois.iterrows():\n",
    "                clustered_pois_list.append({\n",
    "                    'poi_id': current_poi_id,\n",
    "                    'lat': poi['lat'],\n",
    "                    'lon': poi['lon'],\n",
    "                    'category': category,\n",
    "                    'weight': poi['weight'],\n",
    "                    'original_poi_count': 1,\n",
    "                    'original_poi_ids': [poi['poi_id']]\n",
    "                })\n",
    "                current_poi_id += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Processing {category}: {len(category_pois)} POIs\")\n",
    "        \n",
    "        # Convert to radians for haversine distance\n",
    "        coords_rad = np.radians(category_pois[['lat', 'lon']].values)\n",
    "        \n",
    "        # Use DBSCAN with optimized parameters\n",
    "        # eps in radians: 200 meters / Earth radius\n",
    "        eps_rad = 100 / 6371000\n",
    "        \n",
    "        # Use algorithm='ball_tree' for better memory efficiency with haversine\n",
    "        dbscan = DBSCAN(\n",
    "            eps=eps_rad,\n",
    "            min_samples=1,  # Every point forms a cluster\n",
    "            metric='haversine',\n",
    "            algorithm='ball_tree',  # More memory efficient for haversine\n",
    "            n_jobs=-1  # Use all available cores\n",
    "        )\n",
    "        \n",
    "        # Fit and get labels\n",
    "        labels = dbscan.fit_predict(coords_rad)\n",
    "        \n",
    "        # Create clusters from labels\n",
    "        clusters = {}\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(i)\n",
    "        \n",
    "        # Create clustered POIs\n",
    "        for label, indices in clusters.items():\n",
    "            cluster_data = category_pois.iloc[indices]\n",
    "            \n",
    "            if len(cluster_data) == 1:\n",
    "                poi = cluster_data.iloc[0]\n",
    "                clustered_pois_list.append({\n",
    "                    'poi_id': current_poi_id,\n",
    "                    'lat': poi['lat'], 'lon': poi['lon'],\n",
    "                    'category': category, 'weight': poi['weight'],\n",
    "                    'original_poi_count': 1,\n",
    "                    'original_poi_ids': [poi['poi_id']]\n",
    "                })\n",
    "            else:\n",
    "                clustered_pois_list.append({\n",
    "                    'poi_id': current_poi_id,\n",
    "                    'lat': cluster_data['lat'].mean(),\n",
    "                    'lon': cluster_data['lon'].mean(),\n",
    "                    'category': category,\n",
    "                    'weight': cluster_data['weight'].sum(),\n",
    "                    'original_poi_count': len(cluster_data),\n",
    "                    'original_poi_ids': cluster_data['poi_id'].tolist()\n",
    "                })\n",
    "            current_poi_id += 1\n",
    "    \n",
    "    pois = pd.DataFrame(clustered_pois_list)\n",
    "    print(f\"Reduced from {len(original_pois)} to {len(pois)} POI clusters\")\n",
    "    output=Path(r'E:\\Uni_PGT\\visualisation_outputs')\n",
    "    pois.to_csv(output/ f'pois_{t}.csv')\n",
    "    print('Saved reduced pois to', r'E:\\Uni_PGT\\visualisation_outputs')\n",
    "    # --- build station coords (median of start/end) ---\n",
    "    print(original_pois.head())\n",
    "    print(pois.head())\n",
    "    return pois\n",
    "pois_uniform=diff_weights(CATEGORY_WEIGHTS_uniform)\n",
    "pois_pred=diff_weights(category_weights_pred)\n",
    "pois_commercial=diff_weights(CATEGORY_WEIGHTS_COMMERCIAL)"
   ],
   "id": "9f4a85be77f4d896",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering...\n",
      "  Processing library: 49 POIs\n",
      "  Processing school: 188 POIs\n",
      "  Processing university: 28 POIs\n",
      "  Processing residential: 32837 POIs\n",
      "  Processing commercial: 551 POIs\n",
      "  Processing hospital: 16 POIs\n",
      "Reduced from 33669 to 885 POI clusters\n",
      "Saved reduced pois to E:\\Uni_PGT\\visualisation_outputs\n",
      "                                name                       geometry  \\\n",
      "0              Wester Hailes Library  POINT (-3.2851455 55.9162285)   \n",
      "1          Pirniehall Primary School  POINT (-3.2510739 55.9737056)   \n",
      "2               Corstorphine Library  POINT (-3.2810362 55.9407099)   \n",
      "3  Fettes College Preparatory School  POINT (-3.2259783 55.9666246)   \n",
      "4                           Haywired  POINT (-3.1233362 55.9344588)   \n",
      "\n",
      "         lat       lon category  poi_id  weight  \n",
      "0  55.916229 -3.285146  library       0       1  \n",
      "1  55.973706 -3.251074   school       1       1  \n",
      "2  55.940710 -3.281036  library       2       1  \n",
      "3  55.966625 -3.225978   school       3       1  \n",
      "4  55.934459 -3.123336   school       4       1  \n",
      "   poi_id        lat       lon category  weight  original_poi_count  \\\n",
      "0       0  55.916229 -3.285146  library       1                   1   \n",
      "1       1  55.940710 -3.281036  library       1                   1   \n",
      "2       2  55.942745 -3.056334  library       1                   1   \n",
      "3       3  55.947419 -3.187286  library       2                   2   \n",
      "4       4  55.933194 -3.136747  library       1                   1   \n",
      "\n",
      "  original_poi_ids  \n",
      "0              [0]  \n",
      "1              [2]  \n",
      "2              [5]  \n",
      "3          [7, 36]  \n",
      "4             [10]  \n",
      "DBSCAN clustering...\n",
      "  Processing library: 49 POIs\n",
      "  Processing school: 188 POIs\n",
      "  Processing university: 28 POIs\n",
      "  Processing residential: 32837 POIs\n",
      "  Processing commercial: 551 POIs\n",
      "  Processing hospital: 16 POIs\n",
      "Reduced from 33669 to 885 POI clusters\n",
      "Saved reduced pois to E:\\Uni_PGT\\visualisation_outputs\n",
      "                                name                       geometry  \\\n",
      "0              Wester Hailes Library  POINT (-3.2851455 55.9162285)   \n",
      "1          Pirniehall Primary School  POINT (-3.2510739 55.9737056)   \n",
      "2               Corstorphine Library  POINT (-3.2810362 55.9407099)   \n",
      "3  Fettes College Preparatory School  POINT (-3.2259783 55.9666246)   \n",
      "4                           Haywired  POINT (-3.1233362 55.9344588)   \n",
      "\n",
      "         lat       lon category  poi_id    weight  \n",
      "0  55.916229 -3.285146  library       0  0.053922  \n",
      "1  55.973706 -3.251074   school       1  0.076471  \n",
      "2  55.940710 -3.281036  library       2  0.053922  \n",
      "3  55.966625 -3.225978   school       3  0.076471  \n",
      "4  55.934459 -3.123336   school       4  0.076471  \n",
      "   poi_id        lat       lon category    weight  original_poi_count  \\\n",
      "0       0  55.916229 -3.285146  library  0.053922                   1   \n",
      "1       1  55.940710 -3.281036  library  0.053922                   1   \n",
      "2       2  55.942745 -3.056334  library  0.053922                   1   \n",
      "3       3  55.947419 -3.187286  library  0.107843                   2   \n",
      "4       4  55.933194 -3.136747  library  0.053922                   1   \n",
      "\n",
      "  original_poi_ids  \n",
      "0              [0]  \n",
      "1              [2]  \n",
      "2              [5]  \n",
      "3          [7, 36]  \n",
      "4             [10]  \n",
      "DBSCAN clustering...\n",
      "  Processing library: 49 POIs\n",
      "  Processing school: 188 POIs\n",
      "  Processing university: 28 POIs\n",
      "  Processing residential: 32837 POIs\n",
      "  Processing commercial: 551 POIs\n",
      "  Processing hospital: 16 POIs\n",
      "Reduced from 33669 to 885 POI clusters\n",
      "Saved reduced pois to E:\\Uni_PGT\\visualisation_outputs\n",
      "                                name                       geometry  \\\n",
      "0              Wester Hailes Library  POINT (-3.2851455 55.9162285)   \n",
      "1          Pirniehall Primary School  POINT (-3.2510739 55.9737056)   \n",
      "2               Corstorphine Library  POINT (-3.2810362 55.9407099)   \n",
      "3  Fettes College Preparatory School  POINT (-3.2259783 55.9666246)   \n",
      "4                           Haywired  POINT (-3.1233362 55.9344588)   \n",
      "\n",
      "         lat       lon category  poi_id  weight  \n",
      "0  55.916229 -3.285146  library       0       1  \n",
      "1  55.973706 -3.251074   school       1       1  \n",
      "2  55.940710 -3.281036  library       2       1  \n",
      "3  55.966625 -3.225978   school       3       1  \n",
      "4  55.934459 -3.123336   school       4       1  \n",
      "   poi_id        lat       lon category  weight  original_poi_count  \\\n",
      "0       0  55.916229 -3.285146  library       1                   1   \n",
      "1       1  55.940710 -3.281036  library       1                   1   \n",
      "2       2  55.942745 -3.056334  library       1                   1   \n",
      "3       3  55.947419 -3.187286  library       2                   2   \n",
      "4       4  55.933194 -3.136747  library       1                   1   \n",
      "\n",
      "  original_poi_ids  \n",
      "0              [0]  \n",
      "1              [2]  \n",
      "2              [5]  \n",
      "3          [7, 36]  \n",
      "4             [10]  \n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:14:25.845024Z",
     "start_time": "2025-11-21T13:14:20.712581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- build station coords (median of start/end) ---\n",
    "lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "\n",
    "coord_parts = []\n",
    "if start_lon and start_lat:\n",
    "    tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "    tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "    coord_parts.append(tmp)\n",
    "if end_lon and end_lat:\n",
    "    tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "    tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "    coord_parts.append(tmp2)\n",
    "if not coord_parts:\n",
    "    raise RuntimeError('No station lon/lat columns found in `combined`.')\n",
    "\n",
    "stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "stations_df = stations_df.rename_axis('station_id')\n",
    "stations_df.index = stations_df.index.astype(int)\n",
    "# restrict to stations present in any predicted matrix to avoid waste\n",
    "pred_station_ids = set()\n",
    "for df in predicted_matrices.values():\n",
    "    pred_station_ids.update([int(x) for x in df.index.astype(int).tolist()])\n",
    "stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "if stations_df.empty:\n",
    "    raise RuntimeError('No station coordinates available for stations present in predicted_matrices.')\n",
    "station_ids = stations_df.index.astype(int).tolist()\n",
    "\n",
    "# --- helper: haversine (vectorized) ---\n",
    "def haversine_matrix(lonA, latA, lonB, latB):\n",
    "    R = 6371000.0\n",
    "    lonA = np.radians(np.asarray(lonA, dtype=float))\n",
    "    latA = np.radians(np.asarray(latA, dtype=float))\n",
    "    lonB = np.radians(np.asarray(lonB, dtype=float))\n",
    "    latB = np.radians(np.asarray(latB, dtype=float))\n",
    "    dlon = lonA[:, None] - lonB[None, :]\n",
    "    dlat = latA[:, None] - latB[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(latA)[:, None] * np.cos(latB)[None, :] * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "def poi_poi_od(poi,t):\n",
    "    # --- precompute nearest POIs for stations using haversine via sklearn's haversine metric ---\n",
    "    poi_coords = poi[['lat', 'lon']].to_numpy()\n",
    "    station_coords = stations_df[['lat', 'lon']].to_numpy()\n",
    "    \n",
    "    # use scikit-learn NearestNeighbors with haversine (expects radians)\n",
    "    nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "    nbrs.fit(np.radians(poi_coords))\n",
    "    dists_r, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "    # convert radian distances to meters\n",
    "    dists_m = dists_r * 6371000.0\n",
    "    \n",
    "    # --- build allocations: station_id -> dict(poi_index -> weight) ---\n",
    "    allocations = {}\n",
    "    for i, sid in enumerate(station_ids):\n",
    "        dists = dists_m[i]\n",
    "        idx = idxs[i]\n",
    "        # choose close ones if any inside CLOSE_RADIUS_METERS\n",
    "        mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "        if mask_close.any():\n",
    "            chosen_idx = idx[mask_close]\n",
    "            chosen_dists = dists[mask_close]\n",
    "        else:\n",
    "            chosen_idx = idx\n",
    "            chosen_dists = dists\n",
    "    \n",
    "        cat_weights = poi.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "        inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "        raw_scores = cat_weights * inv_dist\n",
    "        if raw_scores.sum() <= 0:\n",
    "            weights = np.ones_like(raw_scores) / len(raw_scores)\n",
    "        else:\n",
    "            weights = raw_scores / raw_scores.sum()\n",
    "    \n",
    "        allocations[int(sid)] = {int(p_idx): float(w) for p_idx, w in zip(chosen_idx.tolist(), weights.tolist())}\n",
    "    \n",
    "    # Save allocations (flat)\n",
    "    alloc_rows = []\n",
    "    for s, pdict in allocations.items():\n",
    "        for pidx, w in pdict.items():\n",
    "            alloc_rows.append({'station_id': int(s), 'poi_index': int(pidx), 'poi_id': int(poi.at[pidx, 'poi_id']), 'weight': float(w)})\n",
    "    alloc_df = pd.DataFrame(alloc_rows)\n",
    "    alloc_df.to_csv(OUTPUT_DIR / 'station_poi_allocations.csv', index=False)\n",
    "    print('Saved station->POI allocation table to', OUTPUT_DIR / f'station_poi_allocations_type{t}.csv')\n",
    "    \n",
    "    # --- Convert predicted station OD -> POI OD per cluster (using only used POIs per cluster) ---\n",
    "    poi_od_per_cluster = {}\n",
    "    for c, pred_df in predicted_matrices.items():\n",
    "        # ensure pred_df index/cols are ints\n",
    "        pred_df = pred_df.copy()\n",
    "        pred_df.index = pred_df.index.astype(int)\n",
    "        pred_df.columns = pred_df.columns.astype(int)\n",
    "    \n",
    "        # intersect stations with allocations\n",
    "        common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "        if len(common_stations) < 2:\n",
    "            print(f'Cluster {c}: too few stations with allocations ({len(common_stations)}), skipping')\n",
    "            continue\n",
    "    \n",
    "        # build station order and Tij matrix accordingly\n",
    "        Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "    \n",
    "        # build station->poi allocation matrix A (nS x nP_used)\n",
    "        # find all POI indices used by these stations\n",
    "        poi_indices_used = sorted({pidx for s in common_stations for pidx in allocations[s].keys()})\n",
    "        if len(poi_indices_used) == 0:\n",
    "            print(f'Cluster {c}: no POIs assigned to these stations, skipping')\n",
    "            continue\n",
    "    \n",
    "        nS = len(common_stations)\n",
    "        nP = len(poi_indices_used)\n",
    "        A = np.zeros((nS, nP), dtype=float)\n",
    "        station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "        poi_to_col = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "    \n",
    "        for s in common_stations:\n",
    "            r = station_to_row[s]\n",
    "            for pidx, w in allocations[s].items():\n",
    "                if pidx in poi_to_col:\n",
    "                    A[r, poi_to_col[pidx]] = w\n",
    "    \n",
    "        # compute POI OD: P = A^T * Tij * A\n",
    "        P = A.T.dot(Tij).dot(A)\n",
    "    \n",
    "        # build DataFrame: rows/cols labelled by poi_id (not global pois index)\n",
    "        poi_ids = [int(poi.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "        poi_od_df = pd.DataFrame(P, index=poi_ids, columns=poi_ids)\n",
    "        poi_od_per_cluster[c] = poi_od_df\n",
    "        poi_od_df.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}_type{t}.csv')\n",
    "        print(f'Cluster {c}: saved POI OD with shape {poi_od_df.shape} (n_pois={len(poi_ids)})')\n",
    "    \n",
    "    # --- aggregate across clusters (align on poi_id union) ---\n",
    "    if poi_od_per_cluster:\n",
    "        all_poi_ids = sorted({pid for df in poi_od_per_cluster.values() for pid in df.index.tolist()})\n",
    "        agg_mat = pd.DataFrame(0.0, index=all_poi_ids, columns=all_poi_ids)\n",
    "        for c, df in poi_od_per_cluster.items():\n",
    "            agg_mat = agg_mat.add(df.reindex(index=all_poi_ids, columns=all_poi_ids, fill_value=0), fill_value=0)\n",
    "        agg_mat.to_csv(OUTPUT_DIR / f'poi_od_aggregated_all_clusters_type{t}.csv')\n",
    "        print('Saved aggregated POI OD for all clusters to', OUTPUT_DIR / f'poi_od_aggregated_all_clusters_type{t}.csv')\n",
    "    else:\n",
    "        print('No POI OD outputs (no clusters had usable data).')\n",
    "    \n",
    "    print('Done. Outputs in', OUTPUT_DIR)\n",
    "poi_od_u=poi_poi_od(pois_uniform, 'uniform')\n",
    "poi_od_pred=poi_poi_od(pois_pred,'pred')\n",
    "poi_od_c=poi_poi_od(pois_commercial,'commercial')\n"
   ],
   "id": "69ebab7b73aa6ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved station->POI allocation table to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\station_poi_allocations_typeuniform.csv\n",
      "Cluster 0: saved POI OD with shape (238, 238) (n_pois=238)\n",
      "Cluster 1: saved POI OD with shape (234, 234) (n_pois=234)\n",
      "Cluster 2: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Cluster 3: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Saved aggregated POI OD for all clusters to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\poi_od_aggregated_all_clusters_typeuniform.csv\n",
      "Done. Outputs in E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\n",
      "Saved station->POI allocation table to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\station_poi_allocations_typepred.csv\n",
      "Cluster 0: saved POI OD with shape (238, 238) (n_pois=238)\n",
      "Cluster 1: saved POI OD with shape (234, 234) (n_pois=234)\n",
      "Cluster 2: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Cluster 3: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Saved aggregated POI OD for all clusters to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\poi_od_aggregated_all_clusters_typepred.csv\n",
      "Done. Outputs in E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\n",
      "Saved station->POI allocation table to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\station_poi_allocations_typecommercial.csv\n",
      "Cluster 0: saved POI OD with shape (238, 238) (n_pois=238)\n",
      "Cluster 1: saved POI OD with shape (234, 234) (n_pois=234)\n",
      "Cluster 2: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Cluster 3: saved POI OD with shape (232, 232) (n_pois=232)\n",
      "Saved aggregated POI OD for all clusters to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\poi_od_aggregated_all_clusters_typecommercial.csv\n",
      "Done. Outputs in E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:15:44.693561Z",
     "start_time": "2025-11-21T13:14:25.847263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# poi_od_sparse_with_baseline.py\n",
    "#\n",
    "# Memory-efficient conversion of predicted station->station OD (per cluster)\n",
    "# into POI->POI OD using sparse matrices and a \"baseline-to-k-nearest\" strategy\n",
    "# for neglected POIs so we avoid creating a dense 33k x 33k matrix.\n",
    "#\n",
    "# Requirements: scipy, scikit-learn, pandas, numpy\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3                # used for station->poi allocation\n",
    "K_BASELINE_NEIGH = 10          # number of POIs to distribute baseline for neglected POI\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "\n",
    "BASELINE_RATIO = 0.1   # baseline magnitude relative to mean observed cell\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# sanity: required objects\n",
    "try:\n",
    "    predicted_matrices\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`predicted_matrices` not in memory. Run gravity step first.\")\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`combined` not in memory. Load combined dataframe first.\")\n",
    "\n",
    "# load POIs\n",
    "def poi_od_w_baseline(pois,t):\n",
    "    # canonical lists and lookups\n",
    "    all_poi_ids = pois['poi_id'].astype(int).tolist()\n",
    "    poi_index_to_id = {i: int(pois.at[i, 'poi_id']) for i in pois.index}\n",
    "    poi_id_to_index = {int(pois.at[i, 'poi_id']): i for i in pois.index}\n",
    "    \n",
    "    n_all = len(all_poi_ids)\n",
    "    print(f\"POIs loaded type {t}: {n_all} (will keep results sparse)\")\n",
    "    \n",
    "    # build station coords as median of start/end\n",
    "    lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "    lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "    start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "    start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "    end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "    end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('No station lon/lat columns found in combined')\n",
    "    stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    stations_df.index = stations_df.index.astype(int)\n",
    "    \n",
    "    # restrict to stations that appear in at least one predicted matrix\n",
    "    pred_station_ids = set()\n",
    "    for m in predicted_matrices.values():\n",
    "        pred_station_ids.update([int(x) for x in m.index.astype(int).tolist()])\n",
    "    stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "    station_ids = stations_df.index.astype(int).tolist()\n",
    "    print(f\"Using {len(station_ids)} stations for allocation\")\n",
    "    \n",
    "    # precompute station->nearest POIs (small K)\n",
    "    poi_coords = pois[['lat','lon']].to_numpy()\n",
    "    station_coords = stations_df[['lat','lon']].to_numpy()\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "    nbrs.fit(np.radians(poi_coords))\n",
    "    d_radians, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "    dists_m = d_radians * 6371000.0\n",
    "    \n",
    "    # build allocations mapping station_id -> list of (poi_index, weight)\n",
    "    allocations = {}\n",
    "    for i, sid in enumerate(station_ids):\n",
    "        dists = dists_m[i]\n",
    "        idx = idxs[i]\n",
    "        mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "        if mask_close.any():\n",
    "            chosen_idx = idx[mask_close]\n",
    "            chosen_dists = dists[mask_close]\n",
    "        else:\n",
    "            chosen_idx = idx\n",
    "            chosen_dists = dists\n",
    "        cat_weights = pois.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "        inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "        raw = cat_weights * inv_dist\n",
    "        if raw.sum() <= 0:\n",
    "            w = np.ones_like(raw) / len(raw)\n",
    "        else:\n",
    "            w = raw / raw.sum()\n",
    "        allocations[int(sid)] = list(zip([int(p) for p in chosen_idx.tolist()], [float(x) for x in w.tolist()]))\n",
    "    \n",
    "    # Save allocations summary\n",
    "    alloc_rows = []\n",
    "    for s, lst in allocations.items():\n",
    "        for pidx, w in lst:\n",
    "            alloc_rows.append({'station_id': s, 'poi_index': int(pidx), 'poi_id': int(pois.at[pidx,'poi_id']), 'weight': w})\n",
    "    alloc_df = pd.DataFrame(alloc_rows)\n",
    "    alloc_df.to_csv(OUTPUT_DIR / f'station_poi_allocations_type{t}.csv', index=False)\n",
    "    print('Saved station->POI allocations (small)')\n",
    "    \n",
    "    # Build POI neighbor index for baseline distribution (k baseline neighbors on POI network)\n",
    "    poi_nbrs = NearestNeighbors(n_neighbors=K_BASELINE_NEIGH, metric='haversine')\n",
    "    poi_nbrs.fit(np.radians(poi_coords))\n",
    "    poi_d_rad, poi_idxs = poi_nbrs.kneighbors(np.radians(poi_coords))\n",
    "    poi_d_m = poi_d_rad * 6371000.0\n",
    "    \n",
    "    # helper: function to add entries to sparse accumulator lists\n",
    "    \n",
    "    def append_entries(rows_list, cols_list, data_list, from_poi_ids, to_poi_ids, values_matrix):\n",
    "        # from_poi_ids and to_poi_ids are lists of global poi_id labels (not dataframe indices)\n",
    "        for i, pid in enumerate(from_poi_ids):\n",
    "            for j, qid in enumerate(to_poi_ids):\n",
    "                val = values_matrix[i, j]\n",
    "                if val != 0 and not np.isnan(val):\n",
    "                    rows_list.append(poi_id_to_index[pid])\n",
    "                    cols_list.append(poi_id_to_index[qid])\n",
    "                    data_list.append(float(val))\n",
    "\n",
    "    # main conversion loop: produce sparse COO components per cluster\n",
    "    from scipy.sparse import coo_matrix, save_npz\n",
    "    poi_od_sparse_per_cluster = {}\n",
    "    poi_od_sparse_per_hour_per_cluster = {}  # NEW: for per-hour matrices\n",
    "    \n",
    "    for c, pred_df in predicted_matrices.items():\n",
    "        print(f'Processing cluster {c}...')\n",
    "        pred_df = pred_df.copy()\n",
    "        pred_df.index = pred_df.index.astype(int)\n",
    "        pred_df.columns = pred_df.columns.astype(int)\n",
    "\n",
    "        # Get hours in this cluster for per-hour calculation\n",
    "        hrs_in_cluster = hour_cluster_df[hour_cluster_df['cluster'] == c]['hour'].tolist()\n",
    "        n_hrs_in_cluster = len(hrs_in_cluster)\n",
    "        print(f' Cluster {c} has {n_hrs_in_cluster} hours: {hrs_in_cluster}')\n",
    "\n",
    "        # stations in this predicted matrix that have allocations\n",
    "        common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "        if len(common_stations) < 2:\n",
    "            print(f' Cluster {c}: too few stations with allocations, skipping')\n",
    "            continue\n",
    "\n",
    "        Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "        # build A_used (nS x nP_used) where nP_used is number of distinct POIs assigned to these stations\n",
    "        poi_indices_used = sorted({pidx for s in common_stations for pidx, _ in allocations[s]})\n",
    "        if len(poi_indices_used) == 0:\n",
    "            print(f' Cluster {c}: no POIs used by these stations, skipping')\n",
    "            continue\n",
    "\n",
    "        station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "        poi_to_col_used = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "        nS = len(common_stations); nP_used = len(poi_indices_used)\n",
    "        A_used = np.zeros((nS, nP_used), dtype=float)\n",
    "        for s in common_stations:\n",
    "            r = station_to_row[s]\n",
    "            for pidx, w in allocations[s]:\n",
    "                if pidx in poi_to_col_used:\n",
    "                    A_used[r, poi_to_col_used[pidx]] = w\n",
    "\n",
    "        # compute P_used (nP_used x nP_used) — this is small (few hundreds)\n",
    "        P_used = A_used.T.dot(Tij).dot(A_used)\n",
    "        \n",
    "        # NEW: Compute per-hour matrix\n",
    "        if n_hrs_in_cluster > 0:\n",
    "            P_used_per_hour = P_used / n_hrs_in_cluster\n",
    "        else:\n",
    "            P_used_per_hour = P_used  # fallback if no hours found\n",
    "\n",
    "        # Now map P_used into global sparse lists (TOTAL version)\n",
    "        rows_total = []\n",
    "        cols_total = []\n",
    "        data_total = []\n",
    "\n",
    "        # map used POI local -> global poi_id\n",
    "        poi_ids_used = [int(pois.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "\n",
    "        # append P_used block into sparse lists (TOTAL)\n",
    "        for i_local, global_pidx in enumerate(poi_indices_used):\n",
    "            pid = int(pois.at[global_pidx, 'poi_id'])\n",
    "            for j_local, global_qidx in enumerate(poi_indices_used):\n",
    "                qid = int(pois.at[global_qidx, 'poi_id'])\n",
    "                val = P_used[i_local, j_local]\n",
    "                if val != 0 and not np.isnan(val):\n",
    "                    rows_total.append(poi_id_to_index[pid])\n",
    "                    cols_total.append(poi_id_to_index[qid])\n",
    "                    data_total.append(float(val))\n",
    "\n",
    "        # NEW: Create separate lists for per-hour version\n",
    "        rows_per_hour = []\n",
    "        cols_per_hour = []\n",
    "        data_per_hour = []\n",
    "\n",
    "        # append P_used_per_hour block into sparse lists (PER-HOUR)\n",
    "        for i_local, global_pidx in enumerate(poi_indices_used):\n",
    "            pid = int(pois.at[global_pidx, 'poi_id'])\n",
    "            for j_local, global_qidx in enumerate(poi_indices_used):\n",
    "                qid = int(pois.at[global_qidx, 'poi_id'])\n",
    "                val = P_used_per_hour[i_local, j_local]\n",
    "                if val != 0 and not np.isnan(val):\n",
    "                    rows_per_hour.append(poi_id_to_index[pid])\n",
    "                    cols_per_hour.append(poi_id_to_index[qid])\n",
    "                    data_per_hour.append(float(val))\n",
    "\n",
    "        # compute baseline per-cell scalar (mean observed cell over used block)\n",
    "        observed_sum = P_used.sum()\n",
    "        if observed_sum <= 0:\n",
    "            mean_cell = 0.0\n",
    "        else:\n",
    "            mean_cell = observed_sum / (n_all)  # normalized to full universe\n",
    "\n",
    "        mean_weight = pois['weight'].mean()\n",
    "\n",
    "        # identify neglected POIs (those that have no entries in current sparse lists)\n",
    "        used_poi_set = set(poi_ids_used)\n",
    "        all_poi_set = set(all_poi_ids)\n",
    "        neglected = sorted(list(all_poi_set - used_poi_set))\n",
    "        print(f' Cluster {c}: used POIs={len(used_poi_set)}, neglected POIs={len(neglected)}')\n",
    "\n",
    "        # For each neglected POI, distribute baseline to its K_BASELINE_NEIGH nearest POIs (by index in pois)\n",
    "        if len(neglected) > 0 and mean_cell > 0:\n",
    "            # we have precomputed poi_idxs and poi_d_m arrays (POI->neighbors)\n",
    "            for pid in neglected:\n",
    "                pidx = poi_id_to_index[pid]\n",
    "                # neighbors indices (including itself) from poi_idxs array\n",
    "                neigh_local = poi_idxs[pidx, 1:K_BASELINE_NEIGH+1]  # skip self (first neighbor)\n",
    "                neigh_d = poi_d_m[pidx, 1:K_BASELINE_NEIGH+1]\n",
    "                # weights: inverse distance * category weight\n",
    "                neigh_weights = pois.loc[neigh_local, 'weight'].to_numpy(dtype=float)\n",
    "                invd = 1.0 / (neigh_d + EPS)\n",
    "                raw = neigh_weights * invd\n",
    "                if raw.sum() <= 0:\n",
    "                    wnorm = np.ones_like(raw) / len(raw)\n",
    "                else:\n",
    "                    wnorm = raw / raw.sum()\n",
    "                # baseline total per neglected POI (outflow) distribute across neighbors\n",
    "                baseline_total = mean_cell * BASELINE_RATIO * (pois.at[pidx, 'weight'] / mean_weight)\n",
    "                \n",
    "                # NEW: baseline per hour\n",
    "                baseline_per_hour = baseline_total / n_hrs_in_cluster if n_hrs_in_cluster > 0 else baseline_total\n",
    "\n",
    "                # distribute baseline_total to outgoing links pid -> neigh (TOTAL)\n",
    "                for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                    qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                    val = baseline_total * wnorm[k_idx]\n",
    "                    rows_total.append(pidx)\n",
    "                    cols_total.append(neigh_idx)\n",
    "                    data_total.append(float(val))\n",
    "                # similarly distribute baseline inflow: neighbors -> pid (TOTAL)\n",
    "                for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                    qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                    val = baseline_total * wnorm[k_idx]\n",
    "                    rows_total.append(neigh_idx)\n",
    "                    cols_total.append(pidx)\n",
    "                    data_total.append(float(val))\n",
    "                    \n",
    "                # NEW: distribute baseline_per_hour (PER-HOUR version)\n",
    "                for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                    qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                    val = baseline_per_hour * wnorm[k_idx]\n",
    "                    rows_per_hour.append(pidx)\n",
    "                    cols_per_hour.append(neigh_idx)\n",
    "                    data_per_hour.append(float(val))\n",
    "                for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                    qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                    val = baseline_per_hour * wnorm[k_idx]\n",
    "                    rows_per_hour.append(neigh_idx)\n",
    "                    cols_per_hour.append(pidx)\n",
    "                    data_per_hour.append(float(val))\n",
    "\n",
    "        # build sparse COO and save (TOTAL version)\n",
    "        coo_total = coo_matrix((data_total, (rows_total, cols_total)), shape=(n_all, n_all))\n",
    "        csr_total = coo_total.tocsr()\n",
    "        save_npz(OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}.npz', csr_total)\n",
    "\n",
    "        # also save edge-list CSV (only nonzero entries) for TOTAL\n",
    "        coo_nz_total = coo_total.tocoo()\n",
    "        edge_df_total = pd.DataFrame({'from_poi_index': coo_nz_total.row, 'to_poi_index': coo_nz_total.col, 'flow': coo_nz_total.data})\n",
    "        edge_df_total['from_poi_id'] = edge_df_total['from_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_df_total['to_poi_id'] = edge_df_total['to_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_df_total = edge_df_total[['from_poi_id','to_poi_id','flow']]\n",
    "        edge_df_total.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}_edgelists.csv', index=False)\n",
    "\n",
    "        # NEW: build sparse COO and save (PER-HOUR version)\n",
    "        coo_per_hour = coo_matrix((data_per_hour, (rows_per_hour, cols_per_hour)), shape=(n_all, n_all))\n",
    "        csr_per_hour = coo_per_hour.tocsr()\n",
    "        save_npz(OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}_per_hour.npz', csr_per_hour)\n",
    "\n",
    "        # NEW: also save edge-list CSV for PER-HOUR\n",
    "        coo_nz_per_hour = coo_per_hour.tocoo()\n",
    "        edge_df_per_hour = pd.DataFrame({'from_poi_index': coo_nz_per_hour.row, 'to_poi_index': coo_nz_per_hour.col, 'flow': coo_nz_per_hour.data})\n",
    "        edge_df_per_hour['from_poi_id'] = edge_df_per_hour['from_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_df_per_hour['to_poi_id'] = edge_df_per_hour['to_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_df_per_hour = edge_df_per_hour[['from_poi_id','to_poi_id','flow']]\n",
    "        edge_df_per_hour.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}_per_hour_edgelists.csv', index=False)\n",
    "\n",
    "        poi_od_sparse_per_cluster[c] = {'sparse': csr_total, 'edge_csv': OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}_edgelist.csv'}\n",
    "        poi_od_sparse_per_hour_per_cluster[c] = {'sparse': csr_per_hour, 'edge_csv': OUTPUT_DIR / f'poi_od_cluster_{c}_type_{t}_per_hour_edgelist.csv'}  # NEW\n",
    "        \n",
    "        print(f' Cluster {c}: saved sparse POI OD (nnz={csr_total.nnz}) and per-hour POI OD (nnz={csr_per_hour.nnz})')\n",
    "\n",
    "    # aggregate across clusters (sum sparse matrices) - TOTAL version\n",
    "    if poi_od_sparse_per_cluster:\n",
    "        first = True\n",
    "        agg_csr_total = None\n",
    "        for c, info in poi_od_sparse_per_cluster.items():\n",
    "            mat = info['sparse']\n",
    "            if first:\n",
    "                agg_csr_total = mat.copy()\n",
    "                first = False\n",
    "            else:\n",
    "                agg_csr_total = agg_csr_total + mat\n",
    "        # save aggregated TOTAL\n",
    "        save_npz(OUTPUT_DIR / f'poi_od_aggregated_all_clusters_sparse_type_{t}.npz', agg_csr_total)\n",
    "        # also save aggregated edge list for TOTAL\n",
    "        coo_agg_total = agg_csr_total.tocoo()\n",
    "        edge_agg_total = pd.DataFrame({'from_idx': coo_agg_total.row, 'to_idx': coo_agg_total.col, 'flow': coo_agg_total.data})\n",
    "        edge_agg_total['from_poi_id'] = edge_agg_total['from_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_agg_total['to_poi_id'] = edge_agg_total['to_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_agg_total[['from_poi_id','to_poi_id','flow']].to_csv(OUTPUT_DIR / f'poi_od_aggregated_all_clusters_edgelists_type_{t}.csv', index=False)\n",
    "        \n",
    "        # NEW: aggregate across clusters for PER-HOUR version\n",
    "        first = True\n",
    "        agg_csr_per_hour = None\n",
    "        for c, info in poi_od_sparse_per_hour_per_cluster.items():\n",
    "            mat = info['sparse']\n",
    "            if first:\n",
    "                agg_csr_per_hour = mat.copy()\n",
    "                first = False\n",
    "            else:\n",
    "                agg_csr_per_hour = agg_csr_per_hour + mat\n",
    "        # save aggregated PER-HOUR\n",
    "        save_npz(OUTPUT_DIR / f'poi_od_aggregated_all_clusters_sparse_type_{t}_per_hour.npz', agg_csr_per_hour)\n",
    "        # also save aggregated edge list for PER-HOUR\n",
    "        coo_agg_per_hour = agg_csr_per_hour.tocoo()\n",
    "        edge_agg_per_hour = pd.DataFrame({'from_idx': coo_agg_per_hour.row, 'to_idx': coo_agg_per_hour.col, 'flow': coo_agg_per_hour.data})\n",
    "        edge_agg_per_hour['from_poi_id'] = edge_agg_per_hour['from_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_agg_per_hour['to_poi_id'] = edge_agg_per_hour['to_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "        edge_agg_per_hour[['from_poi_id','to_poi_id','flow']].to_csv(OUTPUT_DIR / f'poi_od_aggregated_all_clusters_edgelists_type_{t}_per_hour.csv', index=False)\n",
    "        \n",
    "        print('Saved aggregated sparse POI OD and per-hour POI OD with edge lists')\n",
    "\n",
    "    print('Done. Outputs (sparse .npz + edgelists) in', OUTPUT_DIR)\n",
    "    \n",
    "poi_od_w_baseline(pois_uniform,'uniform')\n",
    "poi_od_w_baseline(pois_pred,'pred')\n",
    "poi_od_w_baseline(pois_commercial,'commercial')\n"
   ],
   "id": "e67a8c942a632705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POIs loaded type uniform: 885 (will keep results sparse)\n",
      "Using 194 stations for allocation\n",
      "Saved station->POI allocations (small)\n",
      "Processing cluster 0...\n",
      " Cluster 0 has 5 hours: [11, 12, 13, 14, 15]\n",
      " Cluster 0: used POIs=238, neglected POIs=647\n",
      " Cluster 0: saved sparse POI OD (nnz=64154) and per-hour POI OD (nnz=64154)\n",
      "Processing cluster 1...\n",
      " Cluster 1 has 2 hours: [16, 17]\n",
      " Cluster 1: used POIs=234, neglected POIs=651\n",
      " Cluster 1: saved sparse POI OD (nnz=62288) and per-hour POI OD (nnz=62288)\n",
      "Processing cluster 2...\n",
      " Cluster 2 has 6 hours: [18, 19, 20, 21, 22, 23]\n",
      " Cluster 2: used POIs=232, neglected POIs=653\n",
      " Cluster 2: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Processing cluster 3...\n",
      " Cluster 3 has 11 hours: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      " Cluster 3: used POIs=232, neglected POIs=653\n",
      " Cluster 3: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Saved aggregated sparse POI OD and per-hour POI OD with edge lists\n",
      "Done. Outputs (sparse .npz + edgelists) in E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\n",
      "POIs loaded type pred: 885 (will keep results sparse)\n",
      "Using 194 stations for allocation\n",
      "Saved station->POI allocations (small)\n",
      "Processing cluster 0...\n",
      " Cluster 0 has 5 hours: [11, 12, 13, 14, 15]\n",
      " Cluster 0: used POIs=238, neglected POIs=647\n",
      " Cluster 0: saved sparse POI OD (nnz=64154) and per-hour POI OD (nnz=64154)\n",
      "Processing cluster 1...\n",
      " Cluster 1 has 2 hours: [16, 17]\n",
      " Cluster 1: used POIs=234, neglected POIs=651\n",
      " Cluster 1: saved sparse POI OD (nnz=62288) and per-hour POI OD (nnz=62288)\n",
      "Processing cluster 2...\n",
      " Cluster 2 has 6 hours: [18, 19, 20, 21, 22, 23]\n",
      " Cluster 2: used POIs=232, neglected POIs=653\n",
      " Cluster 2: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Processing cluster 3...\n",
      " Cluster 3 has 11 hours: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      " Cluster 3: used POIs=232, neglected POIs=653\n",
      " Cluster 3: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Saved aggregated sparse POI OD and per-hour POI OD with edge lists\n",
      "Done. Outputs (sparse .npz + edgelists) in E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\n",
      "POIs loaded type commercial: 885 (will keep results sparse)\n",
      "Using 194 stations for allocation\n",
      "Saved station->POI allocations (small)\n",
      "Processing cluster 0...\n",
      " Cluster 0 has 5 hours: [11, 12, 13, 14, 15]\n",
      " Cluster 0: used POIs=238, neglected POIs=647\n",
      " Cluster 0: saved sparse POI OD (nnz=64154) and per-hour POI OD (nnz=64154)\n",
      "Processing cluster 1...\n",
      " Cluster 1 has 2 hours: [16, 17]\n",
      " Cluster 1: used POIs=234, neglected POIs=651\n",
      " Cluster 1: saved sparse POI OD (nnz=62288) and per-hour POI OD (nnz=62288)\n",
      "Processing cluster 2...\n",
      " Cluster 2 has 6 hours: [18, 19, 20, 21, 22, 23]\n",
      " Cluster 2: used POIs=232, neglected POIs=653\n",
      " Cluster 2: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Processing cluster 3...\n",
      " Cluster 3 has 11 hours: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      " Cluster 3: used POIs=232, neglected POIs=653\n",
      " Cluster 3: saved sparse POI OD (nnz=61366) and per-hour POI OD (nnz=61366)\n",
      "Saved aggregated sparse POI OD and per-hour POI OD with edge lists\n",
      "Done. Outputs (sparse .npz + edgelists) in E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:15:56.985827Z",
     "start_time": "2025-11-21T13:15:44.693561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# kmeans_candidates_no_snap.py\n",
    "# Demand-weighted KMeans (NO snapping to POIs) to produce P candidate stations\n",
    "# Inputs:\n",
    "#  - POI_FILE: CSV with columns ['poi_id'(optional),'lat','lon',...]\n",
    "#  - POI_EDGELIST: combined POI edgelist CSV with columns [from_poi_id,to_poi_id,flow]\n",
    "# Outputs saved to OUTPUT_DIR:\n",
    "#  - candidate_stations_P{P}_kmeans_no_snap.csv (centroid lon/lat + demand stats)\n",
    "#  - candidate_eval_P{P}_kmeans_no_snap.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------- USER CONFIG ----------\n",
    "\n",
    "POI_EDGELIST =Path( r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline')\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXISTING_STATIONS_FILE = r'E:\\Uni_PGT\\station_data.csv'  # New: path to existing stations\n",
    "\n",
    "P = 150                          # number of candidate centroids to produce\n",
    "KMEANS_RANDOM_STATE = 42\n",
    "KM_INIT = 'k-means++'           # 'k-means++' or 'random'\n",
    "COVERAGE_RADIUS_METERS = 800.0  # evaluation radius for coverage (walk radius)\n",
    "SNAP_DISTANCE_METERS = 40.0     # distance threshold for snapping to existing stations\n",
    "# --------------------------------\n",
    "\n",
    "# Utility: small haversine helpers (meters)\n",
    "def haversine_matrix(lonsA, latsA, lonsB, latsB):\n",
    "    R = 6371000.0\n",
    "    lonA = np.radians(np.asarray(lonsA, dtype=float))\n",
    "    latA = np.radians(np.asarray(latsA, dtype=float))\n",
    "    lonB = np.radians(np.asarray(lonsB, dtype=float))\n",
    "    latB = np.radians(np.asarray(latsB, dtype=float))\n",
    "    dlon = lonA[:, None] - lonB[None, :]\n",
    "    dlat = latA[:, None] - latB[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(latA)[:, None] * np.cos(latB)[None, :] * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# --- Load existing stations ---\n",
    "print('Loading existing stations...')\n",
    "try:\n",
    "    existing_stations = pd.read_csv(EXISTING_STATIONS_FILE )\n",
    "    # Try to detect station ID and coordinate columns\n",
    "    station_id_col = None\n",
    "    lat_col = None\n",
    "    lon_col = None\n",
    "    \n",
    "    # Common column name patterns\n",
    "    for col in existing_stations.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'id' in col_lower and station_id_col is None:\n",
    "            station_id_col = col\n",
    "        elif 'lat' in col_lower and lat_col is None:\n",
    "            lat_col = col\n",
    "        elif 'lon' in col_lower and lon_col is None:\n",
    "            lon_col = col\n",
    "    \n",
    "    # If not found, use first columns\n",
    "    if station_id_col is None:\n",
    "        station_id_col = existing_stations.columns[0]\n",
    "    if lat_col is None:\n",
    "        lat_col = existing_stations.columns[1]\n",
    "    if lon_col is None:\n",
    "        lon_col = existing_stations.columns[2]\n",
    "    \n",
    "    print(f\"Using columns - ID: {station_id_col}, Lat: {lat_col}, Lon: {lon_col}\")\n",
    "    \n",
    "    existing_stations = existing_stations[[station_id_col, lat_col, lon_col]].copy()\n",
    "    existing_stations.columns = ['station_id', 'lat', 'lon']\n",
    "    existing_stations = existing_stations.dropna(subset=['lat', 'lon'])\n",
    "    \n",
    "    print(f\"Loaded {len(existing_stations)} existing stations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading existing stations: {e}\")\n",
    "    existing_stations = pd.DataFrame(columns=['station_id', 'lat', 'lon'])\n",
    "\n",
    "# --- load POIs and compute demand per POI from edgelist ---\n",
    "def candidate_gen(pois,t,p):   \n",
    "    if 'poi_id' not in pois.columns:\n",
    "        pois['poi_id'] = pois.index.astype(int)\n",
    "    pois['poi_id'] = pois['poi_id'].astype(int)\n",
    "    \n",
    "    print(f'Loading POI type {t} edgelist to compute demand per POI ...')\n",
    "    edges = pd.read_csv(POI_EDGELIST/ f'poi_od_aggregated_all_clusters_edgelists_type_{t}.csv')\n",
    "    # detect flow column\n",
    "    cols_lower = {c.lower(): c for c in edges.columns}\n",
    "    flow_col = None\n",
    "    for candidate in ('flow', 'demand', 'count', 'weight'):\n",
    "        if candidate in cols_lower:\n",
    "            flow_col = cols_lower[candidate]\n",
    "            break\n",
    "    if flow_col is None:\n",
    "        numeric_cols = [c for c in edges.columns if np.issubdtype(edges[c].dtype, np.number)]\n",
    "        if numeric_cols:\n",
    "            flow_col = numeric_cols[-1]\n",
    "        else:\n",
    "            raise RuntimeError('Could not detect numeric flow column in edgelist.')\n",
    "    print('Flow column detected:', flow_col)\n",
    "    \n",
    "    inflow = edges.groupby('to_poi_id')[flow_col].sum()\n",
    "    outflow = edges.groupby('from_poi_id')[flow_col].sum()\n",
    "    all_ids = sorted(set(inflow.index.tolist()) | set(outflow.index.tolist()) | set(pois['poi_id'].tolist()))\n",
    "    \n",
    "    # demand = inflow + outflow (symmetric importance for being origin/destination)\n",
    "    demand_series = pd.Series(0.0, index=all_ids, dtype=float)\n",
    "    if not inflow.empty:\n",
    "        demand_series.loc[inflow.index] += inflow\n",
    "    if not outflow.empty:\n",
    "        demand_series.loc[outflow.index] += outflow\n",
    "    \n",
    "    poi_demand = pois.set_index('poi_id').reindex(all_ids).copy()\n",
    "    poi_demand['demand'] = demand_series.reindex(poi_demand.index).fillna(0.0)\n",
    "    \n",
    "    # Remove POIs with missing coordinates\n",
    "    poi_demand = poi_demand.dropna(subset=['lat', 'lon'])\n",
    "    \n",
    "    poi_ids = poi_demand.index.astype(int).tolist()\n",
    "    lats = poi_demand['lat'].to_numpy(dtype=float)\n",
    "    lons = poi_demand['lon'].to_numpy(dtype=float)\n",
    "    weights = poi_demand['demand'].to_numpy(dtype=float)\n",
    "    \n",
    "    total_demand = weights.sum()\n",
    "    print(f'POIs loaded: {len(poi_ids)} POIs, total demand = {total_demand:.2f}')\n",
    "    \n",
    "    # avoid zero-weight issues\n",
    "    weights_safe = weights.copy()\n",
    "    weights_safe[weights_safe <= 0] = 1e-6\n",
    "    \n",
    "    # Project lon/lat to a simple local metric space (equirectangular approx) for KMeans\n",
    "    mean_lat = np.mean(lats)\n",
    "    xm = (lons - np.mean(lons)) * (111320 * np.cos(np.radians(mean_lat)))\n",
    "    ym = (lats - np.mean(lats)) * 110540\n",
    "    X_m = np.column_stack([xm, ym])\n",
    "    \n",
    "    # Run weighted KMeans (sample_weight)\n",
    "    print('Running demand-weighted KMeans (no snapping) ...')\n",
    "    km = KMeans(n_clusters=p, init=KM_INIT, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    km.fit(X_m, sample_weight=weights_safe)\n",
    "    centers_m = km.cluster_centers_\n",
    "    \n",
    "    # Map centers back to lon/lat approx\n",
    "    centers_lon = (centers_m[:,0] / (111320 * np.cos(np.radians(mean_lat)))) + np.mean(lons)\n",
    "    centers_lat = (centers_m[:,1] / 110540) + np.mean(lats)\n",
    "    \n",
    "    # Create initial candidates DataFrame\n",
    "    candidates = pd.DataFrame({\n",
    "        'candidate_id': range(len(centers_lon)),\n",
    "        'centroid_lon': centers_lon,\n",
    "        'centroid_lat': centers_lat,\n",
    "        'is_existing_station': False,  # Initialize as new stations\n",
    "        'original_candidate_id': range(len(centers_lon))  # Track original KMeans ID\n",
    "    })\n",
    "    \n",
    "    # --- Snap candidates to existing stations if within 20 meters ---\n",
    "    if len(existing_stations) > 0:\n",
    "        print(f\"Snapping candidates to existing stations within {SNAP_DISTANCE_METERS} meters...\")\n",
    "        \n",
    "        # Calculate distance matrix between candidates and existing stations\n",
    "        candidate_coords = candidates[['centroid_lat', 'centroid_lon']].values\n",
    "        existing_coords = existing_stations[['lat', 'lon']].values\n",
    "        \n",
    "        # Use haversine distance\n",
    "        distances = haversine_matrix(\n",
    "            candidate_coords[:, 1], candidate_coords[:, 0],  # lon, lat\n",
    "            existing_coords[:, 1], existing_coords[:, 0]     # lon, lat\n",
    "        )\n",
    "        \n",
    "        # Find nearest existing station for each candidate\n",
    "        min_distances = distances.min(axis=1)\n",
    "        nearest_station_indices = distances.argmin(axis=1)\n",
    "        \n",
    "        snapped_count = 0\n",
    "        for i in range(len(candidates)):\n",
    "            if min_distances[i] <= SNAP_DISTANCE_METERS:\n",
    "                # Snap to existing station\n",
    "                station_idx = nearest_station_indices[i]\n",
    "                candidates.at[i, 'centroid_lat'] = existing_stations.iloc[station_idx]['lat']\n",
    "                candidates.at[i, 'centroid_lon'] = existing_stations.iloc[station_idx]['lon']\n",
    "                candidates.at[i, 'is_existing_station'] = True\n",
    "                candidates.at[i, 'snapped_station_id'] = existing_stations.iloc[station_idx]['station_id']\n",
    "                snapped_count += 1\n",
    "        \n",
    "        print(f\"Snapped {snapped_count} candidates to existing stations\")\n",
    "    \n",
    "    # --- Add any missing existing stations ---\n",
    "    if len(existing_stations) > 0:\n",
    "        print(\"Adding any missing existing stations to candidate set...\")\n",
    "        \n",
    "        # Check which existing stations are not already in candidates\n",
    "        existing_coords = existing_stations[['lat', 'lon']].values\n",
    "        candidate_coords = candidates[['centroid_lat', 'centroid_lon']].values\n",
    "        \n",
    "        # Calculate distances between existing stations and all candidates\n",
    "        distances = haversine_matrix(\n",
    "            existing_coords[:, 1], existing_coords[:, 0],  # lon, lat\n",
    "            candidate_coords[:, 1], candidate_coords[:, 0] # lon, lat\n",
    "        )\n",
    "        \n",
    "        # Find existing stations that are not close to any candidate\n",
    "        min_distances_to_candidates = distances.min(axis=1)\n",
    "        missing_existing_mask = min_distances_to_candidates > SNAP_DISTANCE_METERS\n",
    "        \n",
    "        missing_existing_count = missing_existing_mask.sum()\n",
    "        if missing_existing_count > 0:\n",
    "            print(f\"Adding {missing_existing_count} missing existing stations\")\n",
    "            \n",
    "            # Add missing existing stations\n",
    "            missing_stations = existing_stations[missing_existing_mask].copy()\n",
    "            new_candidates = pd.DataFrame({\n",
    "                'candidate_id': range(len(candidates), len(candidates) + len(missing_stations)),\n",
    "                'centroid_lon': missing_stations['lon'],\n",
    "                'centroid_lat': missing_stations['lat'],\n",
    "                'is_existing_station': True,\n",
    "                'snapped_station_id': missing_stations['station_id'],\n",
    "                'original_candidate_id': -1  # Mark as not from KMeans\n",
    "            })\n",
    "            \n",
    "            candidates = pd.concat([candidates, new_candidates], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    print(f\"Final candidate set: {len(candidates)} stations ({candidates['is_existing_station'].sum()} existing, {len(candidates) - candidates['is_existing_station'].sum()} new)\")\n",
    "    \n",
    "    # Evaluate coverage: distance from each POI to nearest candidate\n",
    "    candidate_coords_final = candidates[['centroid_lat', 'centroid_lon']].values\n",
    "    D = haversine_matrix(lons, lats, candidate_coords_final[:, 1], candidate_coords_final[:, 0])\n",
    "    min_dist = D.min(axis=1)\n",
    "    within = (min_dist <= COVERAGE_RADIUS_METERS)\n",
    "    covered_demand = weights[within].sum()\n",
    "    coverage_fraction = covered_demand / (weights.sum() + 1e-12)\n",
    "    avg_weighted_dist = np.sum(min_dist * weights) / (weights.sum() + 1e-12)\n",
    "    median_dist = np.median(min_dist)\n",
    "    \n",
    "    eval_metrics = {\n",
    "        'n_candidates': len(candidates),\n",
    "        'n_existing_stations': int(candidates['is_existing_station'].sum()),\n",
    "        'n_new_stations': len(candidates) - candidates['is_existing_station'].sum(),\n",
    "        'coverage_fraction_demand': float(coverage_fraction),\n",
    "        'avg_weighted_distance_m': float(avg_weighted_dist),\n",
    "        'median_distance_m': float(median_dist),\n",
    "        'num_pois_total': len(poi_ids),\n",
    "        'total_demand': float(weights.sum())\n",
    "    }\n",
    "    \n",
    "    # Save outputs\n",
    "    cand_out = OUTPUT_DIR / f'candidate_stations_P{p}_type_{t}.csv'\n",
    "    candidates.to_csv(cand_out, index=False)\n",
    "    pd.Series(eval_metrics).to_frame('value').to_csv(OUTPUT_DIR / f'candidate_eval_P{p}_type_{t}.csv')\n",
    "    \n",
    "    print('Saved candidates to:', cand_out)\n",
    "    print('Saved evaluation to:', OUTPUT_DIR / f'candidate_eval_P{p}_type_{t}.csv')\n",
    "    print('Done.')\n",
    "candidate_gen(pois_uniform,'uniform',50)\n",
    "candidate_gen(pois_uniform,'uniform',85)\n",
    "candidate_gen(pois_uniform,'uniform',100)\n",
    "candidate_gen(pois_uniform,'uniform',150)\n",
    "candidate_gen(pois_uniform,'uniform',200)\n",
    "candidate_gen(pois_uniform,'uniform',250)\n",
    "candidate_gen(pois_uniform,'uniform',300)\n",
    "candidate_gen(pois_pred,'pred',50)\n",
    "candidate_gen(pois_pred,'pred',85)\n",
    "candidate_gen(pois_pred,'pred',100)\n",
    "candidate_gen(pois_pred,'pred',150)\n",
    "candidate_gen(pois_pred,'pred',200)\n",
    "candidate_gen(pois_pred,'pred',250)\n",
    "candidate_gen(pois_pred,'pred',300)\n",
    "candidate_gen(pois_commercial,'commercial',50)\n",
    "candidate_gen(pois_commercial,'commercial',85)\n",
    "candidate_gen(pois_commercial,'commercial',100)\n",
    "candidate_gen(pois_commercial,'commercial',150)\n",
    "candidate_gen(pois_commercial,'commercial',200)\n",
    "candidate_gen(pois_commercial,'commercial',250)\n",
    "candidate_gen(pois_commercial,'commercial',300)"
   ],
   "id": "8515afc5bf23f643",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing stations...\n",
      "Using columns - ID: station_id, Lat: lat, Lon: lon\n",
      "Loaded 85 existing stations\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 1 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 84 missing existing stations\n",
      "Final candidate set: 134 stations (85 existing, 49 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P50_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P50_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 5 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 80 missing existing stations\n",
      "Final candidate set: 165 stations (85 existing, 80 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P85_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P85_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 5 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 80 missing existing stations\n",
      "Final candidate set: 180 stations (85 existing, 95 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P100_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P100_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 6 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 79 missing existing stations\n",
      "Final candidate set: 229 stations (85 existing, 144 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P150_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P150_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 3 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 82 missing existing stations\n",
      "Final candidate set: 282 stations (85 existing, 197 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P200_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P200_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 4 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 81 missing existing stations\n",
      "Final candidate set: 331 stations (85 existing, 246 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P250_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P250_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type uniform edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.99\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 2 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 83 missing existing stations\n",
      "Final candidate set: 383 stations (85 existing, 298 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P300_type_uniform.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P300_type_uniform.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 0 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 85 missing existing stations\n",
      "Final candidate set: 135 stations (85 existing, 50 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P50_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P50_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 1 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 84 missing existing stations\n",
      "Final candidate set: 169 stations (85 existing, 84 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P85_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P85_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 0 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 85 missing existing stations\n",
      "Final candidate set: 185 stations (85 existing, 100 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P100_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P100_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 2 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 83 missing existing stations\n",
      "Final candidate set: 233 stations (85 existing, 148 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P150_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P150_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 3 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 82 missing existing stations\n",
      "Final candidate set: 282 stations (85 existing, 197 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P200_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P200_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 5 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 80 missing existing stations\n",
      "Final candidate set: 330 stations (85 existing, 245 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P250_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P250_type_pred.csv\n",
      "Done.\n",
      "Loading POI type pred edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 982.40\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 2 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 83 missing existing stations\n",
      "Final candidate set: 383 stations (85 existing, 298 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P300_type_pred.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P300_type_pred.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 0 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 85 missing existing stations\n",
      "Final candidate set: 135 stations (85 existing, 50 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P50_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P50_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 0 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 85 missing existing stations\n",
      "Final candidate set: 170 stations (85 existing, 85 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P85_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P85_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 0 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 85 missing existing stations\n",
      "Final candidate set: 185 stations (85 existing, 100 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P100_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P100_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 2 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 83 missing existing stations\n",
      "Final candidate set: 233 stations (85 existing, 148 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P150_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P150_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 5 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 80 missing existing stations\n",
      "Final candidate set: 280 stations (85 existing, 195 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P200_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P200_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 1 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 84 missing existing stations\n",
      "Final candidate set: 334 stations (85 existing, 249 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P250_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P250_type_commercial.csv\n",
      "Done.\n",
      "Loading POI type commercial edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 885 POIs, total demand = 978.97\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Snapping candidates to existing stations within 40.0 meters...\n",
      "Snapped 2 candidates to existing stations\n",
      "Adding any missing existing stations to candidate set...\n",
      "Adding 83 missing existing stations\n",
      "Final candidate set: 383 stations (85 existing, 298 new)\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_stations_P300_type_commercial.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_with_existing\\candidate_eval_P300_type_commercial.csv\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T13:19:31.117066Z",
     "start_time": "2025-11-21T13:19:28.370023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate POI demand as max(inflow, outflow) for both normal and per-hour versions\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_poi_demand(pois, t, c):\n",
    "    \"\"\"\n",
    "    Calculate demand for each POI as max(sum(inflow), sum(outflow))\n",
    "    for both normal and per-hour POI OD matrices PER CLUSTER\n",
    "    \"\"\"\n",
    "    # Paths to the edge list files\n",
    "    POI_EDGELIST_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline')\n",
    "    \n",
    "    # Files for normal (total) demand - now cluster-specific\n",
    "    normal_edges_file = POI_EDGELIST_DIR / f'poi_od_cluster_{c}_type_{t}_edgelists.csv'\n",
    "    \n",
    "    # Files for per-hour demand - now cluster-specific  \n",
    "    per_hour_edges_file = POI_EDGELIST_DIR / f'poi_od_cluster_{c}_type_{t}_per_hour_edgelists.csv'\n",
    "    \n",
    "    # Output directory\n",
    "    OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_demand_analysis')\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process normal (total) demand\n",
    "    print(f\"Processing normal demand for type {t} cluster {c}...\")\n",
    "    if normal_edges_file.exists():\n",
    "        edges = pd.read_csv(normal_edges_file)\n",
    "        \n",
    "        # Calculate inflow (sum of flows where POI is destination)\n",
    "        inflow = edges.groupby('to_poi_id')['flow'].sum().reset_index()\n",
    "        inflow.columns = ['poi_id', 'inflow']\n",
    "        \n",
    "        # Calculate outflow (sum of flows where POI is origin)\n",
    "        outflow = edges.groupby('from_poi_id')['flow'].sum().reset_index()\n",
    "        outflow.columns = ['poi_id', 'outflow']\n",
    "        \n",
    "        # Merge inflow and outflow\n",
    "        demand_df = pd.merge(inflow, outflow, on='poi_id', how='outer').fillna(0)\n",
    "        \n",
    "        # Calculate demand as max(inflow, outflow)\n",
    "        demand_df['demand'] = demand_df[['inflow', 'outflow']].max(axis=1)\n",
    "        \n",
    "        # Add POI category information\n",
    "        poi_info = pois[['poi_id', 'category', 'lat', 'lon']].copy()\n",
    "        demand_df = pd.merge(demand_df, poi_info, on='poi_id', how='left')\n",
    "        \n",
    "        # Save normal demand with cluster identifier\n",
    "        demand_df.to_csv(OUTPUT_DIR / f'poi_demand_normal_type_{t}_cluster{c}.csv', index=False)\n",
    "        print(f\"Saved normal demand for {len(demand_df)} POIs in cluster {c}\")\n",
    "        \n",
    "        # Summary by category for this cluster\n",
    "        category_summary = demand_df.groupby('category').agg({\n",
    "            'poi_id': 'count',\n",
    "            'inflow': 'sum',\n",
    "            'outflow': 'sum', \n",
    "            'demand': 'sum'\n",
    "        }).rename(columns={'poi_id': 'poi_count'})\n",
    "        category_summary.to_csv(OUTPUT_DIR / f'poi_demand_normal_category_summary_type_{t}_cluster{c}.csv')\n",
    "        print(f\"Saved category summary for normal demand in cluster {c}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Warning: Normal edges file not found: {normal_edges_file}\")\n",
    "        demand_df = pd.DataFrame()\n",
    "    \n",
    "    # Process per-hour demand\n",
    "    print(f\"Processing per-hour demand for type {t} cluster {c}...\")\n",
    "    if per_hour_edges_file.exists():\n",
    "        edges_per_hour = pd.read_csv(per_hour_edges_file)\n",
    "        \n",
    "        # Calculate inflow (sum of flows where POI is destination)\n",
    "        inflow_ph = edges_per_hour.groupby('to_poi_id')['flow'].sum().reset_index()\n",
    "        inflow_ph.columns = ['poi_id', 'inflow_per_hour']\n",
    "        \n",
    "        # Calculate outflow (sum of flows where POI is origin)\n",
    "        outflow_ph = edges_per_hour.groupby('from_poi_id')['flow'].sum().reset_index()\n",
    "        outflow_ph.columns = ['poi_id', 'outflow_per_hour']\n",
    "        \n",
    "        # Merge inflow and outflow\n",
    "        demand_ph_df = pd.merge(inflow_ph, outflow_ph, on='poi_id', how='outer').fillna(0)\n",
    "        \n",
    "        # Calculate demand as max(inflow, outflow)\n",
    "        demand_ph_df['demand_per_hour'] = demand_ph_df[['inflow_per_hour', 'outflow_per_hour']].sum(axis=1)\n",
    "        \n",
    "        # Add POI category information\n",
    "        poi_info = pois[['poi_id', 'category', 'lat', 'lon']].copy()\n",
    "        demand_ph_df = pd.merge(demand_ph_df, poi_info, on='poi_id', how='left')\n",
    "        \n",
    "        # Save per-hour demand with cluster identifier\n",
    "        demand_ph_df.to_csv(OUTPUT_DIR / f'poi_demand_per_hour_type_{t}_cluster{c}.csv', index=False)\n",
    "        print(f\"Saved per-hour demand for {len(demand_ph_df)} POIs in cluster {c}\")\n",
    "        \n",
    "        # Summary by category for per-hour in this cluster\n",
    "        category_summary_ph = demand_ph_df.groupby('category').agg({\n",
    "            'poi_id': 'count',\n",
    "            'inflow_per_hour': 'sum',\n",
    "            'outflow_per_hour': 'sum',\n",
    "            'demand_per_hour': 'sum'\n",
    "        }).rename(columns={'poi_id': 'poi_count'})\n",
    "        category_summary_ph.to_csv(OUTPUT_DIR / f'poi_demand_per_hour_category_summary_type_{t}_cluster{c}.csv')\n",
    "        print(f\"Saved category summary for per-hour demand in cluster {c}\")\n",
    "        \n",
    "        # Combine both normal and per-hour demands for comparison for this cluster\n",
    "        if not demand_df.empty:\n",
    "            combined_demand = pd.merge(\n",
    "                demand_df[['poi_id', 'category', 'inflow', 'outflow', 'demand']],\n",
    "                demand_ph_df[['poi_id', 'inflow_per_hour', 'outflow_per_hour', 'demand_per_hour']],\n",
    "                on='poi_id', \n",
    "                how='outer'\n",
    "            ).fillna(0)\n",
    "            \n",
    "            # Calculate ratio of per-hour to normal demand\n",
    "            combined_demand['demand_ratio'] = combined_demand['demand_per_hour'] / (combined_demand['demand'] + 1e-12)\n",
    "            \n",
    "            combined_demand.to_csv(OUTPUT_DIR / f'poi_demand_combined_type_{t}_cluster{c}.csv', index=False)\n",
    "            print(f\"Saved combined demand analysis for cluster {c}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Warning: Per-hour edges file not found: {per_hour_edges_file}\")\n",
    "    \n",
    "    print(f\"Completed demand analysis for type {t} cluster {c}\")\n",
    "    return OUTPUT_DIR\n",
    "\n",
    "def create_summary_across_types_and_clusters(clusters):\n",
    "    \"\"\"Create a summary comparing demand across all three weight types and all clusters\"\"\"\n",
    "    OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_demand_analysis')\n",
    "    \n",
    "    summary_data = []\n",
    "    for t in ['uniform', 'pred', 'commercial']:\n",
    "        for c in clusters:\n",
    "            # Try to load normal demand files for this cluster and type\n",
    "            normal_file = OUTPUT_DIR / f'poi_demand_normal_type_{t}_cluster{c}.csv'\n",
    "            per_hour_file = OUTPUT_DIR / f'poi_demand_per_hour_type_{t}_cluster{c}.csv'\n",
    "            \n",
    "            if normal_file.exists():\n",
    "                df_normal = pd.read_csv(normal_file)\n",
    "                total_demand_normal = df_normal['demand'].sum()\n",
    "                avg_demand_normal = df_normal['demand'].mean()\n",
    "                max_demand_normal = df_normal['demand'].max()\n",
    "                n_pois_normal = len(df_normal)\n",
    "            else:\n",
    "                total_demand_normal = avg_demand_normal = max_demand_normal = 0\n",
    "                n_pois_normal = 0\n",
    "                \n",
    "            if per_hour_file.exists():\n",
    "                df_per_hour = pd.read_csv(per_hour_file)\n",
    "                total_demand_ph = df_per_hour['demand_per_hour'].sum()\n",
    "                avg_demand_ph = df_per_hour['demand_per_hour'].mean()\n",
    "                max_demand_ph = df_per_hour['demand_per_hour'].max()\n",
    "                n_pois_ph = len(df_per_hour)\n",
    "            else:\n",
    "                total_demand_ph = avg_demand_ph = max_demand_ph = 0\n",
    "                n_pois_ph = 0\n",
    "                \n",
    "            summary_data.append({\n",
    "                'type': t,\n",
    "                'cluster': c,\n",
    "                'total_demand_normal': total_demand_normal,\n",
    "                'avg_demand_normal': avg_demand_normal,\n",
    "                'max_demand_normal': max_demand_normal,\n",
    "                'total_demand_per_hour': total_demand_ph,\n",
    "                'avg_demand_per_hour': avg_demand_ph,\n",
    "                'max_demand_per_hour': max_demand_ph,\n",
    "                'n_pois_normal': n_pois_normal,\n",
    "                'n_pois_per_hour': n_pois_ph\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_csv(OUTPUT_DIR / 'demand_summary_across_types_and_clusters.csv', index=False)\n",
    "    print(\"Saved cross-type and cross-cluster summary\")\n",
    "    return summary_df\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting POI demand analysis per cluster...\")\n",
    "\n",
    "# Define the clusters we have (based on your clustering results)\n",
    "clusters = [0, 1, 2, 3]\n",
    "\n",
    "# Run for all three weight types and all clusters\n",
    "output_dirs = {}\n",
    "for t, pois_data in [('uniform', pois_uniform), ('pred', pois_pred), ('commercial', pois_commercial)]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing demand for type: {t}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for c in clusters:\n",
    "        print(f\"\\n--- Processing Cluster {c} ---\")\n",
    "        output_dirs[f\"{t}_cluster{c}\"] = calculate_poi_demand(pois_data, t, c)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL CLUSTER DEMAND ANALYSES COMPLETED!\")\n",
    "print(\"Results saved in:\", output_dirs[list(output_dirs.keys())[0]] if output_dirs else \"No output directories\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Create comprehensive summary across all types and clusters\n",
    "print(\"\\nCreating comprehensive summary across all types and clusters...\")\n",
    "summary_df = create_summary_across_types_and_clusters(clusters)\n",
    "print(\"\\nSummary across all types and clusters:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Additional: Create cluster-specific summaries\n",
    "print(\"\\nCreating individual cluster summaries...\")\n",
    "for c in clusters:\n",
    "    cluster_summary = summary_df[summary_df['cluster'] == c]\n",
    "    print(f\"\\nCluster {c} Summary:\")\n",
    "    print(cluster_summary[['type', 'total_demand_normal', 'total_demand_per_hour', 'n_pois_normal']])"
   ],
   "id": "246bb5cd5a117fab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting POI demand analysis per cluster...\n",
      "\n",
      "==================================================\n",
      "Analyzing demand for type: uniform\n",
      "==================================================\n",
      "\n",
      "--- Processing Cluster 0 ---\n",
      "Processing normal demand for type uniform cluster 0...\n",
      "Saved normal demand for 885 POIs in cluster 0\n",
      "Saved category summary for normal demand in cluster 0\n",
      "Processing per-hour demand for type uniform cluster 0...\n",
      "Saved per-hour demand for 885 POIs in cluster 0\n",
      "Saved category summary for per-hour demand in cluster 0\n",
      "Saved combined demand analysis for cluster 0\n",
      "Completed demand analysis for type uniform cluster 0\n",
      "\n",
      "--- Processing Cluster 1 ---\n",
      "Processing normal demand for type uniform cluster 1...\n",
      "Saved normal demand for 885 POIs in cluster 1\n",
      "Saved category summary for normal demand in cluster 1\n",
      "Processing per-hour demand for type uniform cluster 1...\n",
      "Saved per-hour demand for 885 POIs in cluster 1\n",
      "Saved category summary for per-hour demand in cluster 1\n",
      "Saved combined demand analysis for cluster 1\n",
      "Completed demand analysis for type uniform cluster 1\n",
      "\n",
      "--- Processing Cluster 2 ---\n",
      "Processing normal demand for type uniform cluster 2...\n",
      "Saved normal demand for 885 POIs in cluster 2\n",
      "Saved category summary for normal demand in cluster 2\n",
      "Processing per-hour demand for type uniform cluster 2...\n",
      "Saved per-hour demand for 885 POIs in cluster 2\n",
      "Saved category summary for per-hour demand in cluster 2\n",
      "Saved combined demand analysis for cluster 2\n",
      "Completed demand analysis for type uniform cluster 2\n",
      "\n",
      "--- Processing Cluster 3 ---\n",
      "Processing normal demand for type uniform cluster 3...\n",
      "Saved normal demand for 885 POIs in cluster 3\n",
      "Saved category summary for normal demand in cluster 3\n",
      "Processing per-hour demand for type uniform cluster 3...\n",
      "Saved per-hour demand for 885 POIs in cluster 3\n",
      "Saved category summary for per-hour demand in cluster 3\n",
      "Saved combined demand analysis for cluster 3\n",
      "Completed demand analysis for type uniform cluster 3\n",
      "\n",
      "==================================================\n",
      "Analyzing demand for type: pred\n",
      "==================================================\n",
      "\n",
      "--- Processing Cluster 0 ---\n",
      "Processing normal demand for type pred cluster 0...\n",
      "Saved normal demand for 885 POIs in cluster 0\n",
      "Saved category summary for normal demand in cluster 0\n",
      "Processing per-hour demand for type pred cluster 0...\n",
      "Saved per-hour demand for 885 POIs in cluster 0\n",
      "Saved category summary for per-hour demand in cluster 0\n",
      "Saved combined demand analysis for cluster 0\n",
      "Completed demand analysis for type pred cluster 0\n",
      "\n",
      "--- Processing Cluster 1 ---\n",
      "Processing normal demand for type pred cluster 1...\n",
      "Saved normal demand for 885 POIs in cluster 1\n",
      "Saved category summary for normal demand in cluster 1\n",
      "Processing per-hour demand for type pred cluster 1...\n",
      "Saved per-hour demand for 885 POIs in cluster 1\n",
      "Saved category summary for per-hour demand in cluster 1\n",
      "Saved combined demand analysis for cluster 1\n",
      "Completed demand analysis for type pred cluster 1\n",
      "\n",
      "--- Processing Cluster 2 ---\n",
      "Processing normal demand for type pred cluster 2...\n",
      "Saved normal demand for 885 POIs in cluster 2\n",
      "Saved category summary for normal demand in cluster 2\n",
      "Processing per-hour demand for type pred cluster 2...\n",
      "Saved per-hour demand for 885 POIs in cluster 2\n",
      "Saved category summary for per-hour demand in cluster 2\n",
      "Saved combined demand analysis for cluster 2\n",
      "Completed demand analysis for type pred cluster 2\n",
      "\n",
      "--- Processing Cluster 3 ---\n",
      "Processing normal demand for type pred cluster 3...\n",
      "Saved normal demand for 885 POIs in cluster 3\n",
      "Saved category summary for normal demand in cluster 3\n",
      "Processing per-hour demand for type pred cluster 3...\n",
      "Saved per-hour demand for 885 POIs in cluster 3\n",
      "Saved category summary for per-hour demand in cluster 3\n",
      "Saved combined demand analysis for cluster 3\n",
      "Completed demand analysis for type pred cluster 3\n",
      "\n",
      "==================================================\n",
      "Analyzing demand for type: commercial\n",
      "==================================================\n",
      "\n",
      "--- Processing Cluster 0 ---\n",
      "Processing normal demand for type commercial cluster 0...\n",
      "Saved normal demand for 885 POIs in cluster 0\n",
      "Saved category summary for normal demand in cluster 0\n",
      "Processing per-hour demand for type commercial cluster 0...\n",
      "Saved per-hour demand for 885 POIs in cluster 0\n",
      "Saved category summary for per-hour demand in cluster 0\n",
      "Saved combined demand analysis for cluster 0\n",
      "Completed demand analysis for type commercial cluster 0\n",
      "\n",
      "--- Processing Cluster 1 ---\n",
      "Processing normal demand for type commercial cluster 1...\n",
      "Saved normal demand for 885 POIs in cluster 1\n",
      "Saved category summary for normal demand in cluster 1\n",
      "Processing per-hour demand for type commercial cluster 1...\n",
      "Saved per-hour demand for 885 POIs in cluster 1\n",
      "Saved category summary for per-hour demand in cluster 1\n",
      "Saved combined demand analysis for cluster 1\n",
      "Completed demand analysis for type commercial cluster 1\n",
      "\n",
      "--- Processing Cluster 2 ---\n",
      "Processing normal demand for type commercial cluster 2...\n",
      "Saved normal demand for 885 POIs in cluster 2\n",
      "Saved category summary for normal demand in cluster 2\n",
      "Processing per-hour demand for type commercial cluster 2...\n",
      "Saved per-hour demand for 885 POIs in cluster 2\n",
      "Saved category summary for per-hour demand in cluster 2\n",
      "Saved combined demand analysis for cluster 2\n",
      "Completed demand analysis for type commercial cluster 2\n",
      "\n",
      "--- Processing Cluster 3 ---\n",
      "Processing normal demand for type commercial cluster 3...\n",
      "Saved normal demand for 885 POIs in cluster 3\n",
      "Saved category summary for normal demand in cluster 3\n",
      "Processing per-hour demand for type commercial cluster 3...\n",
      "Saved per-hour demand for 885 POIs in cluster 3\n",
      "Saved category summary for per-hour demand in cluster 3\n",
      "Saved combined demand analysis for cluster 3\n",
      "Completed demand analysis for type commercial cluster 3\n",
      "\n",
      "============================================================\n",
      "ALL CLUSTER DEMAND ANALYSES COMPLETED!\n",
      "Results saved in: E:\\Uni_PGT\\visualisation_outputs\\poi_demand_analysis\n",
      "============================================================\n",
      "\n",
      "Creating comprehensive summary across all types and clusters...\n",
      "Saved cross-type and cross-cluster summary\n",
      "\n",
      "Summary across all types and clusters:\n",
      "          type  cluster  total_demand_normal  avg_demand_normal  \\\n",
      "0      uniform        0           205.538541           0.232247   \n",
      "1      uniform        1           100.680865           0.113764   \n",
      "2      uniform        2           121.759505           0.137581   \n",
      "3      uniform        3           128.379217           0.145061   \n",
      "4         pred        0           205.262574           0.231935   \n",
      "5         pred        1           100.661262           0.113742   \n",
      "6         pred        2           121.739685           0.137559   \n",
      "7         pred        3           128.016567           0.144651   \n",
      "8   commercial        0           204.666744           0.231262   \n",
      "9   commercial        1           100.369794           0.113412   \n",
      "10  commercial        2           121.363877           0.137134   \n",
      "11  commercial        3           127.681973           0.144273   \n",
      "\n",
      "    max_demand_normal  total_demand_per_hour  avg_demand_per_hour  \\\n",
      "0           10.751294              75.727593             0.085568   \n",
      "1            3.308702              87.388932             0.098745   \n",
      "2            4.275284              35.159787             0.039729   \n",
      "3            4.994883              19.874044             0.022457   \n",
      "4           10.069111              75.681333             0.085516   \n",
      "5            3.101068              87.336952             0.098686   \n",
      "6            4.409489              35.139218             0.039705   \n",
      "7            5.155615              19.862418             0.022443   \n",
      "8            7.304109              75.410776             0.085210   \n",
      "9            3.107054              87.032816             0.098342   \n",
      "10           4.275284              35.018499             0.039569   \n",
      "11           4.994883              19.794182             0.022366   \n",
      "\n",
      "    max_demand_per_hour  n_pois_normal  n_pois_per_hour  \n",
      "0              3.502384            885              885  \n",
      "1              3.145802            885              885  \n",
      "2              1.369560            885              885  \n",
      "3              0.710829            885              885  \n",
      "4              3.280930            885              885  \n",
      "5              2.947867            885              885  \n",
      "6              1.412705            885              885  \n",
      "7              0.733110            885              885  \n",
      "8              2.381889            885              885  \n",
      "9              2.810571            885              885  \n",
      "10             1.369560            885              885  \n",
      "11             0.710829            885              885  \n",
      "\n",
      "Creating individual cluster summaries...\n",
      "\n",
      "Cluster 0 Summary:\n",
      "         type  total_demand_normal  total_demand_per_hour  n_pois_normal\n",
      "0     uniform           205.538541              75.727593            885\n",
      "4        pred           205.262574              75.681333            885\n",
      "8  commercial           204.666744              75.410776            885\n",
      "\n",
      "Cluster 1 Summary:\n",
      "         type  total_demand_normal  total_demand_per_hour  n_pois_normal\n",
      "1     uniform           100.680865              87.388932            885\n",
      "5        pred           100.661262              87.336952            885\n",
      "9  commercial           100.369794              87.032816            885\n",
      "\n",
      "Cluster 2 Summary:\n",
      "          type  total_demand_normal  total_demand_per_hour  n_pois_normal\n",
      "2      uniform           121.759505              35.159787            885\n",
      "6         pred           121.739685              35.139218            885\n",
      "10  commercial           121.363877              35.018499            885\n",
      "\n",
      "Cluster 3 Summary:\n",
      "          type  total_demand_normal  total_demand_per_hour  n_pois_normal\n",
      "3      uniform           128.379217              19.874044            885\n",
      "7         pred           128.016567              19.862418            885\n",
      "11  commercial           127.681973              19.794182            885\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
