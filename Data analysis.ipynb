{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:39:39.155814Z",
     "start_time": "2025-11-02T10:39:39.147567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys"
   ],
   "id": "81268aaa4cb40cd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:39:47.417116Z",
     "start_time": "2025-11-02T10:39:42.626145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- USER CONFIG -----\n",
    "# Directory where your 36 CSV files live\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data')  # <- updated path\n",
    "# Output filename for the combined CSV\n",
    "OUTPUT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "# Pattern to match files (will match filenames containing YYYY_MM or YYYY-MM)\n",
    "GLOB_PATTERN = '*_counts*.csv'\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "def extract_year_month_from_name(fname: str):\n",
    "    \n",
    "    \"\"\"Return (year, month) tuple if found in filename, else None.\"\"\"\n",
    "    # look for 4-digit year, separator (_ or -), 2-digit month\n",
    "    m = re.search(r'(\\d{4})[_-](\\d{2})', fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, month = m.group(1), m.group(2)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def main(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(f'No files found matching pattern {GLOB_PATTERN} in {input_dir.resolve()}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for f in files:\n",
    "        ym = extract_year_month_from_name(f.name)\n",
    "        if ym is None:\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        year, month = ym\n",
    "        # read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {f.name}: {e}')\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        # add date columns in two common formats:\n",
    "        # 'year_month' = 'YYYY-MM' and 'month_year' = 'MM/YYYY' (user asked for m/y)\n",
    "        df['year_month'] = f\"{year}-{month}\"\n",
    "        df['month_year'] = f\"{month}/{year}\"\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print('No CSVs successfully read (maybe filename pattern is different). Files skipped:')\n",
    "        print('\\n'.join(skipped))\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Optional: reorder so date columns are near the front\n",
    "    cols = list(combined.columns)\n",
    "    for col in ['year_month', 'month_year']:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # Save the combined CSV\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f'Combined {len(dfs)} files into {output_file} (total rows: {len(combined)})')\n",
    "    if skipped:\n",
    "        print('Skipped files (no YYYY_MM found or read error):')\n",
    "        print('\\n'.join(skipped))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(INPUT_DIR, OUTPUT_FILE)\n"
   ],
   "id": "7dc3b5da354ab4bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv (total rows: 268488)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:39:53.918829Z",
     "start_time": "2025-11-02T10:39:53.439683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "eb276d40c246370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268488 entries, 0 to 268487\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   month_year        268488 non-null  object \n",
      " 1   year_month        268488 non-null  object \n",
      " 2   start_station_id  268488 non-null  int64  \n",
      " 3   end_station_id    268488 non-null  int64  \n",
      " 4   hour              268488 non-null  float64\n",
      " 5   trip_count        268488 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 12.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       start_station_id  end_station_id           hour     trip_count\n",
       "count     268488.000000   268488.000000  268488.000000  268488.000000\n",
       "mean         930.490595      976.395388      13.844608       1.714162\n",
       "std          662.431144      671.219020       4.854848       1.803095\n",
       "min          171.000000      171.000000       0.000000       1.000000\n",
       "25%          260.000000      262.000000      11.000000       1.000000\n",
       "50%         1024.000000     1025.000000      14.000000       1.000000\n",
       "75%         1729.000000     1737.000000      17.000000       2.000000\n",
       "max         2268.000000     2268.000000      23.000000      84.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.490595</td>\n",
       "      <td>976.395388</td>\n",
       "      <td>13.844608</td>\n",
       "      <td>1.714162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>662.431144</td>\n",
       "      <td>671.219020</td>\n",
       "      <td>4.854848</td>\n",
       "      <td>1.803095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2268.000000</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:39:58.268749Z",
     "start_time": "2025-11-02T10:39:58.186500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REPORT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\data_quality_report.txt')\n",
    "# Ensure correct data types\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce').astype('Int64')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce').astype('Int64')\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['trip_count'] = pd.to_numeric(df['trip_count'], errors='coerce').astype('Int64')\n",
    "\n",
    "# ----- MISSING VALUES -----\n",
    "missing_summary = df.isna().sum()\n",
    "missing_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(f\"Found {len(missing_rows):,} rows with missing values — will drop them.\")\n",
    "    df = df.dropna()"
   ],
   "id": "c1a1ae76fbf985c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:40:02.710379Z",
     "start_time": "2025-11-02T10:40:02.141406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- DUPLICATES -----\n",
    "num_dupes = df.duplicated().sum()\n",
    "if num_dupes > 0:\n",
    "    print(f\"Dropping {num_dupes:,} duplicate rows.\")\n",
    "    df = df.drop_duplicates()"
   ],
   "id": "80502a5ea7e47868",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:40:05.547056Z",
     "start_time": "2025-11-02T10:40:05.536667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- VALUE VALIDATION -----\n",
    "# Check valid range for hour (0–23 expected)\n",
    "invalid_hours = df[~df['hour'].between(0, 24)]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours):,} rows with invalid hour values (outside 0–24). Fixing...\")\n",
    "    df = df[df['hour'].between(0, 24)]"
   ],
   "id": "211dc1e4af4c94a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:40:08.643542Z",
     "start_time": "2025-11-02T10:40:08.549217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check station ID ranges\n",
    "df_s=pd.read_csv(r'E:\\Uni_PGT\\station_data.csv')\n",
    "m=list(df_s['station_id'].unique())\n",
    "w=df['start_station_id'].unique()\n",
    "missing_ids = [s for s in w if s not in m]\n",
    "print(missing_ids)\n",
    "print(len(missing_ids))\n"
   ],
   "id": "29fbcd4e747abd69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(171), np.int64(255), np.int64(257), np.int64(261), np.int64(266), np.int64(273), np.int64(275), np.int64(277), np.int64(284), np.int64(285), np.int64(290), np.int64(297), np.int64(340), np.int64(341), np.int64(342), np.int64(343), np.int64(344), np.int64(345), np.int64(346), np.int64(347), np.int64(350), np.int64(351), np.int64(352), np.int64(353), np.int64(354), np.int64(355), np.int64(356), np.int64(357), np.int64(359), np.int64(365), np.int64(366), np.int64(648), np.int64(820), np.int64(860), np.int64(862), np.int64(863), np.int64(864), np.int64(865), np.int64(866), np.int64(867), np.int64(868), np.int64(869), np.int64(870), np.int64(871), np.int64(872), np.int64(873), np.int64(874), np.int64(875), np.int64(876), np.int64(877), np.int64(878), np.int64(879), np.int64(880), np.int64(881), np.int64(882), np.int64(885), np.int64(887), np.int64(888), np.int64(889), np.int64(883), np.int64(884), np.int64(890), np.int64(891), np.int64(901), np.int64(964), np.int64(965), np.int64(980), np.int64(981), np.int64(982), np.int64(991), np.int64(1018), np.int64(1026), np.int64(299), np.int64(1027), np.int64(1030), np.int64(1031), np.int64(1032), np.int64(1033), np.int64(1040), np.int64(1041), np.int64(1042), np.int64(1055), np.int64(1056), np.int64(1057), np.int64(1094), np.int64(1095), np.int64(1723), np.int64(1724), np.int64(1731), np.int64(1740), np.int64(1743), np.int64(1746), np.int64(1747), np.int64(1752), np.int64(1764), np.int64(1766), np.int64(1799), np.int64(1800), np.int64(1808), np.int64(1857), np.int64(1859), np.int64(1864), np.int64(1865), np.int64(1866), np.int64(1868), np.int64(1869), np.int64(1870), np.int64(1871), np.int64(1874), np.int64(1877), np.int64(1860)]\n",
      "111\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:40:12.160071Z",
     "start_time": "2025-11-02T10:40:12.111630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- REPORT -----\n",
    "with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('OD Matrix Data Quality Report\\n')\n",
    "    f.write('=' * 40 + '\\n\\n')\n",
    "    f.write(f'Total rows after cleaning: {len(df):,}\\n')\n",
    "    f.write(f'Duplicates removed: {num_dupes}\\n')\n",
    "    f.write(f'Missing rows removed: {len(missing_rows)}\\n')\n",
    "    f.write(f'Invalid hour rows removed: {len(invalid_hours)}\\n\\n')\n",
    "\n",
    "\n",
    "    f.write('Trip count summary (post-clean):\\n')\n",
    "    f.write(str(df['trip_count'].describe()) + '\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Data quality report saved to: {REPORT_FILE}')\n",
    "\n"
   ],
   "id": "c9a8a18a268937df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to: E:\\Uni_PGT\\counts-data\\data_quality_report.txt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:41:27.856244Z",
     "start_time": "2025-11-02T10:40:15.134168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "generate_od_heatmaps.py\n",
    "\n",
    "Generates OD heatmap images for each month (36 files) plus one combined OD heatmap\n",
    "from the combined OD CSV created earlier.\n",
    "\n",
    "Notes / behavior:\n",
    " - The script reads 'combined_od_with_date.csv' and expects columns:\n",
    "   ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    " - It will order stations by numeric station id (ascending). If you have a station\n",
    "   reference file and want a specific ordering, set STATION_REF_FILE.\n",
    " - For visualization, the script uses np.log1p on counts to reduce skew. The saved\n",
    "   CSVs keep raw aggregated counts.\n",
    "\n",
    "Run:\n",
    "    python generate_od_heatmaps.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps')\n",
    "STATION_REF_FILE = Path(r'E:\\Uni_PGT\\station_data.csv')  # set to None to order by numeric id\n",
    "LOG_DISPLAY = True   # use log1p for display to reduce skew\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# If no station reference, build station list from data\n",
    "all_stations = np.union1d(df['start_station_id'].unique(), df['end_station_id'].unique()).astype(int)\n",
    "all_stations_sorted = sorted([int(x) for x in all_stations])\n",
    "n_stations = len(all_stations_sorted)\n",
    "stations = sorted([int(x) for x in all_stations])\n",
    "print(f'Total stations used for matrices: {n_stations}')\n",
    "\n",
    "# helper to make pivot and plot\n",
    "\n",
    "def make_pivot(df_subset, stations, aggcol='trip_count'):\n",
    "    # aggregate counts to ensure one cell per pair\n",
    "    agg = df_subset.groupby(['start_station_id', 'end_station_id'])[aggcol].sum().reset_index()\n",
    "    pivot = agg.pivot(index='start_station_id', columns='end_station_id', values=aggcol).reindex(index=stations, columns=stations).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "\n",
    "def plot_matrix(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    # dynamic figsize: cap sizes to avoid enormous images\n",
    "    height, width = arr.shape\n",
    "    figsize = (min(20, max(6, width/10)), min(20, max(6, height/10)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('end_station_id (ordered)')\n",
    "    ax.set_ylabel('start_station_id (ordered)')\n",
    "\n",
    "    # reduce tick labels for readability: show first, middle, last\n",
    "    if width <= 30:\n",
    "        xticks = range(width)\n",
    "        xtick_labels = [str(int(v)) for v in matrix_df.columns]\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xtick_labels, rotation=90, fontsize=6)\n",
    "    else:\n",
    "        ax.set_xticks([0, width//2, width-1])\n",
    "        ax.set_xticklabels([str(int(matrix_df.columns[0])), str(int(matrix_df.columns[width//2])), str(int(matrix_df.columns[-1]))], fontsize=8)\n",
    "\n",
    "    if height <= 30:\n",
    "        yticks = range(height)\n",
    "        ytick_labels = [str(int(v)) for v in matrix_df.index]\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(ytick_labels, fontsize=6)\n",
    "    else:\n",
    "        ax.set_yticks([0, height//2, height-1])\n",
    "        ax.set_yticklabels([str(int(matrix_df.index[0])), str(int(matrix_df.index[height//2])), str(int(matrix_df.index[-1]))], fontsize=8)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved heatmap: {outpath}')\n",
    "\n",
    "\n",
    "# 1) Per-month heatmaps\n",
    "unique_months = sorted(df['year_month'].unique())\n",
    "print(f'Found {len(unique_months)} unique months (expected 36): {unique_months}')\n",
    "\n",
    "for ym in unique_months:\n",
    "    df_month = df[df['year_month'] == ym]\n",
    "    pivot = make_pivot(df_month, stations)\n",
    "    outpath = OUTPUT_DIR / f'heatmap_{ym}.png'\n",
    "    title = f'OD heatmap {ym} (log display={LOG_DISPLAY})'\n",
    "    plot_matrix(pivot, title, outpath)\n",
    "\n",
    "# 2) Combined heatmap for all data\n",
    "pivot_combined = make_pivot(df, stations)\n",
    "# save numeric combined matrix\n",
    "pivot_combined.to_csv(OUTPUT_DIR / 'od_matrix_combined.csv')\n",
    "plot_matrix(pivot_combined, 'OD heatmap combined (log display={})'.format(LOG_DISPLAY), OUTPUT_DIR / 'heatmap_combined.png')\n",
    "\n",
    "print('All done. Generated per-month and combined heatmaps in:')\n",
    "print(OUTPUT_DIR.resolve())"
   ],
   "id": "4d87a987f75f2586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations used for matrices: 198\n",
      "Found 36 unique months (expected 36): ['2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09']\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_combined.png\n",
      "All done. Generated per-month and combined heatmaps in:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:41:57.012485Z",
     "start_time": "2025-11-02T10:41:54.498509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "seasonal_hourly_heatmaps.py\n",
    "\n",
    "Generates seasonal-hourly OD trip heatmaps:\n",
    " - One heatmap per year (season x hour matrix) for each year in the data\n",
    " - One combined heatmap over all years\n",
    " - Saves matrices as CSV and images to OUTPUT_DIR\n",
    "\n",
    "Seasons used (Northern Hemisphere standard):\n",
    " - Winter: Dec, Jan, Feb (DJF)\n",
    " - Spring: Mar, Apr, May (MAM)\n",
    " - Summer: Jun, Jul, Aug (JJA)\n",
    " - Autumn: Sep, Oct, Nov (SON)\n",
    "\n",
    "Expect input columns: ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    "\n",
    "Run:\n",
    "    python seasonal_hourly_heatmaps.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps_seasonal')\n",
    "DPI = 150\n",
    "LOG_DISPLAY = False  # For seasonal-hour heatmaps we keep linear counts by default\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_year_month(ym):\n",
    "    if isinstance(ym, str):\n",
    "        if '-' in ym:\n",
    "            parts = ym.split('-')\n",
    "            return int(parts[0]), int(parts[1])\n",
    "        if '/' in ym:\n",
    "            parts = ym.split('/')\n",
    "            # assume MM/YYYY -> return (YYYY, MM)\n",
    "            return int(parts[1]), int(parts[0])\n",
    "    raise ValueError(f'Unrecognized year_month format: {ym}')\n",
    "\n",
    "parsed = df['year_month'].apply(parse_year_month)\n",
    "df['year'] = parsed.apply(lambda x: x[0])\n",
    "df['month'] = parsed.apply(lambda x: x[1])\n",
    "\n",
    "# season mapping\n",
    "season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "              3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "              6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "              9: 'Autumn', 10: 'Autumn', 11: 'Autumn'}\n",
    "\n",
    "df['season'] = df['month'].map(season_map)\n",
    "\n",
    "# Helper: build season-hour pivot for a given dataframe\n",
    "\n",
    "def season_hour_pivot(df_subset):\n",
    "    # aggregate trip counts by season and hour\n",
    "    agg = df_subset.groupby(['season', 'hour'])['trip_count'].sum().reset_index()\n",
    "    # ensure all seasons and hours 0-23 present\n",
    "    seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    hours = list(range(24))\n",
    "    pivot = agg.pivot(index='season', columns='hour', values='trip_count').reindex(index=seasons, columns=hours).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "# plotting helper\n",
    "\n",
    "def plot_season_hour(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Hour of day')\n",
    "    ax.set_ylabel('Season')\n",
    "\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "    ax.set_xticklabels([str(h) for h in range(0, 24, 2)])\n",
    "\n",
    "    ax.set_yticks(range(len(matrix_df.index)))\n",
    "    ax.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved: {outpath}')\n",
    "\n",
    "# 1) Per-year seasonal-hour heatmaps\n",
    "years = sorted(df['year'].unique())\n",
    "print(f'Found years: {years}')\n",
    "\n",
    "for y in years:\n",
    "    df_year = df[df['year'] == y]\n",
    "    pivot = season_hour_pivot(df_year)\n",
    "    csv_out = OUTPUT_DIR / f'season_hour_matrix_{y}.csv'\n",
    "    img_out = OUTPUT_DIR / f'season_hour_heatmap_{y}.png'\n",
    "    pivot.to_csv(csv_out)\n",
    "    plot_season_hour(pivot, f'Season vs Hour - {y}', img_out)\n",
    "\n",
    "# 2) Combined (all years)\n",
    "pivot_all = season_hour_pivot(df)\n",
    "pivot_all.to_csv(OUTPUT_DIR / 'season_hour_matrix_all_years.csv')\n",
    "plot_season_hour(pivot_all, 'Season vs Hour - All years', OUTPUT_DIR / 'season_hour_heatmap_all_years.png')\n",
    "\n",
    "print('Done. Results saved to:')\n",
    "print(OUTPUT_DIR.resolve())\n"
   ],
   "id": "160179ad6a1012bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found years: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021)]\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2018.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2019.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2020.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2021.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_all_years.png\n",
      "Done. Results saved to:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:42:14.630852Z",
     "start_time": "2025-11-02T10:42:02.164024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "combine_36_csvs_no_date.py\n",
    "\n",
    "Reads all CSV files from cyclehire-cleandata named like 2018_10.csv ... 2021_09.csv\n",
    "(or containing that yyyy_mm pattern in the filename), concatenates them into a single CSV\n",
    "and saves it to cyclehire-cleandata\\combined_all_periods.csv\n",
    "\n",
    "Notes:\n",
    " - This script does NOT add a date column (as requested).\n",
    " - It will align columns by name; missing columns in some files will be filled with NaN.\n",
    " - It prints a short summary of files read and total rows combined.\n",
    "\n",
    "Usage:\n",
    "    python combine_36_csvs_no_date.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata')\n",
    "OUTPUT_FILE = INPUT_DIR / 'combined_all_periods.csv'\n",
    "GLOB_PATTERN = '*_*.csv'  # matches files with yyyy_mm in name like 2018_10.csv\n",
    "FILE_FILTER_REGEX = re.compile(r'(20\\d{2})[_-](0[1-9]|1[0-2])')  # restrict to yyyy_mm patterns\n",
    "# ------------------------\n",
    "\n",
    "# collect candidate files\n",
    "files = sorted(INPUT_DIR.glob(GLOB_PATTERN))\n",
    "selected_files = [f for f in files if FILE_FILTER_REGEX.search(f.name)]\n",
    "\n",
    "if not selected_files:\n",
    "    raise FileNotFoundError(f'No files matching yyyy_mm pattern found in {INPUT_DIR}')\n",
    "\n",
    "print(f'Found {len(selected_files)} files to combine:')\n",
    "for f in selected_files:\n",
    "    print(' -', f.name)\n",
    "\n",
    "# read and concatenate\n",
    "dfs = []\n",
    "for f in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name  # optional: keep which file the row came from\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to read {f.name}: {e}')\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError('No files were successfully read.')\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "combined.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f'Combined {len(dfs)} files into {OUTPUT_FILE} (total rows: {len(combined):,})')\n",
    "\n",
    "# optional quick sanity print\n",
    "print('\\nColumn summary (name : non-null count):')\n",
    "print(combined.notna().sum().sort_values(ascending=False).head(50))\n",
    "\n",
    "print('\\nDone.')\n"
   ],
   "id": "5973531e49f04c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_4716\\2259740896.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 files to combine:\n",
      " - 2018_10.csv\n",
      " - 2018_11.csv\n",
      " - 2018_12.csv\n",
      " - 2019_01.csv\n",
      " - 2019_02.csv\n",
      " - 2019_03.csv\n",
      " - 2019_04.csv\n",
      " - 2019_05.csv\n",
      " - 2019_06.csv\n",
      " - 2019_07.csv\n",
      " - 2019_08.csv\n",
      " - 2019_09.csv\n",
      " - 2019_10.csv\n",
      " - 2019_11.csv\n",
      " - 2019_12.csv\n",
      " - 2020_01.csv\n",
      " - 2020_02.csv\n",
      " - 2020_03.csv\n",
      " - 2020_04.csv\n",
      " - 2020_05.csv\n",
      " - 2020_06.csv\n",
      " - 2020_07.csv\n",
      " - 2020_08.csv\n",
      " - 2020_09.csv\n",
      " - 2020_10.csv\n",
      " - 2020_11.csv\n",
      " - 2020_12.csv\n",
      " - 2021_01.csv\n",
      " - 2021_02.csv\n",
      " - 2021_03.csv\n",
      " - 2021_04.csv\n",
      " - 2021_05.csv\n",
      " - 2021_06.csv\n",
      " - 2021_07.csv\n",
      " - 2021_08.csv\n",
      " - 2021_09.csv\n",
      "Combined 36 files into E:\\Uni_PGT\\cyclehire-cleandata\\combined_all_periods.csv (total rows: 460,655)\n",
      "\n",
      "Column summary (name : non-null count):\n",
      "started_at                   460655\n",
      "ended_at                     460655\n",
      "duration                     460655\n",
      "start_station_id             460655\n",
      "start_station_name           460655\n",
      "start_station_latitude       460655\n",
      "end_station_latitude         460655\n",
      "start_station_longitude      460655\n",
      "end_station_id               460655\n",
      "end_station_name             460655\n",
      "__source_file                460655\n",
      "end_station_longitude        460655\n",
      "start_station_description    456167\n",
      "end_station_description      455560\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:42:25.237649Z",
     "start_time": "2025-11-02T10:42:24.674555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(combined.info())\n",
    "print(combined.describe())"
   ],
   "id": "3e84a0d30fe9b22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460655 entries, 0 to 460654\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   started_at                 460655 non-null  object \n",
      " 1   ended_at                   460655 non-null  object \n",
      " 2   duration                   460655 non-null  int64  \n",
      " 3   start_station_id           460655 non-null  int64  \n",
      " 4   start_station_name         460655 non-null  object \n",
      " 5   start_station_description  456167 non-null  object \n",
      " 6   start_station_latitude     460655 non-null  float64\n",
      " 7   start_station_longitude    460655 non-null  float64\n",
      " 8   end_station_id             460655 non-null  int64  \n",
      " 9   end_station_name           460655 non-null  object \n",
      " 10  end_station_description    455560 non-null  object \n",
      " 11  end_station_latitude       460655 non-null  float64\n",
      " 12  end_station_longitude      460655 non-null  float64\n",
      " 13  __source_file              460655 non-null  object \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 49.2+ MB\n",
      "None\n",
      "           duration  start_station_id  start_station_latitude  \\\n",
      "count  4.606550e+05     460655.000000           460655.000000   \n",
      "mean   1.945961e+03        936.878545               55.950615   \n",
      "std    5.531311e+03        671.956842                0.013497   \n",
      "min    6.100000e+01        171.000000               55.908404   \n",
      "25%    6.280000e+02        261.000000               55.940238   \n",
      "50%    1.166000e+03       1024.000000               55.947600   \n",
      "75%    2.527000e+03       1729.000000               55.958566   \n",
      "max    2.363348e+06       2268.000000               55.992957   \n",
      "\n",
      "       start_station_longitude  end_station_id  end_station_latitude  \\\n",
      "count            460655.000000   460655.000000         460655.000000   \n",
      "mean                 -3.196412      983.399353             55.952535   \n",
      "std                   0.039072      678.334765              0.015748   \n",
      "min                  -3.407156      171.000000             53.395525   \n",
      "25%                  -3.207964      262.000000             55.941791   \n",
      "50%                  -3.192444     1025.000000             55.951501   \n",
      "75%                  -3.180693     1737.000000             55.962487   \n",
      "max                  -3.058307     2268.000000             55.992957   \n",
      "\n",
      "       end_station_longitude  \n",
      "count          460655.000000  \n",
      "mean               -3.195134  \n",
      "std                 0.041796  \n",
      "min                -3.407156  \n",
      "25%                -3.208070  \n",
      "50%                -3.191421  \n",
      "75%                -3.176351  \n",
      "max                -2.990138  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:42:37.275477Z",
     "start_time": "2025-11-02T10:42:29.409104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path where your data is stored\n",
    "data_path = r'E:\\Uni_PGT\\cyclehire-cleandata'\n",
    "\n",
    "# Convert started_at column to datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "\n",
    "# Extract weekday and hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Assign seasons based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "combined['season'] = combined['started_at'].dt.month.apply(get_season)\n",
    "\n",
    "# Order weekdays for consistent plotting\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create output folder for heatmaps\n",
    "output_dir = os.path.join(data_path, 'heatmaps_weekday_hour')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to plot and save heatmap\n",
    "def plot_heatmap(data, title, filename):\n",
    "    pivot_table = data.pivot_table(index='weekday', columns='hour', values='duration', aggfunc='count').fillna(0)\n",
    "    pivot_table = pivot_table.reindex(weekday_order)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(pivot_table, cmap='YlGnBu')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Weekday')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Generate heatmap for each season\n",
    "for season, data in combined.groupby('season'):\n",
    "    plot_heatmap(data, f'Trip Count by Hour and Weekday - {season}', f'heatmap_{season}.png')\n",
    "\n",
    "# Generate heatmap for all data combined\n",
    "plot_heatmap(combined, 'Trip Count by Hour and Weekday - All Data', 'heatmap_all_data.png')\n",
    "\n",
    "print(f\"Heatmaps saved in: {output_dir}\")"
   ],
   "id": "ffcd021ad0f256a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps saved in: E:\\Uni_PGT\\cyclehire-cleandata\\heatmaps_weekday_hour\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:42:42.718528Z",
     "start_time": "2025-11-02T10:42:39.707615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "duration_and_od_analysis.py\n",
    "\n",
    "Produces:\n",
    " - Histogram of trip durations (linear and log-scaled)\n",
    " - CSV and plot for average duration by hour of day\n",
    " - CSV and plot for average duration by weekday\n",
    " - OD heatmap (average duration per start_station_id x end_station_id)\n",
    "\n",
    "Assumptions:\n",
    " - Combined CSV of the 36 files exists at: cyclehire-cleandata\\combined_all_periods.csv\n",
    " - Columns include: started_at, ended_at, duration, start_station_id, end_station_id\n",
    "\n",
    "Outputs are written to cyclehire-cleandata\\analysis_outputs\n",
    "\n",
    "Run:\n",
    "    python duration_and_od_analysis.py\n",
    "\"\"\"\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs')\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Convert started_at to datetime (coerce errors)\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "# Filter out rows with missing datetime or non-positive duration\n",
    "initial_rows = len(combined)\n",
    "combined = combined[combined['duration'].notna()]\n",
    "combined = combined[combined['duration'] > 0]\n",
    "combined = combined[combined['started_at'].notna()]\n",
    "print(f'Kept {len(combined):,} rows (removed {initial_rows - len(combined):,} invalid rows)')\n",
    "\n",
    "# --- 1) Histograms of duration ---\n",
    "# Linear histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(combined['duration'], bins=100, range=(0, combined['duration'].quantile(0.99)))\n",
    "plt.title('Trip duration distribution (0-99th percentile)')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_linear.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Log-scaled histogram (log1p)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(np.log1p(combined['duration']), bins=100)\n",
    "plt.title('Trip duration distribution (log1p)')\n",
    "plt.xlabel('log1p(Duration)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_log.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Save basic stats\n",
    "desc = combined['duration'].describe()\n",
    "desc.to_csv(OUTPUT_DIR / 'duration_summary_stats.csv')\n",
    "\n",
    "# --- 2) Average duration by hour of day ---\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "avg_by_hour = combined.groupby('hour')['duration'].mean().reindex(range(24)).fillna(0)\n",
    "avg_by_hour.to_csv(OUTPUT_DIR / 'avg_duration_by_hour.csv')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(avg_by_hour.index, avg_by_hour.values, marker='o')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Average duration')\n",
    "plt.title('Average trip duration by hour of day')\n",
    "plt.xticks(range(0,24))\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_hour.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 3) Average duration by weekday ---\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "weekday_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "avg_by_weekday = combined.groupby('weekday')['duration'].mean().reindex(weekday_order)\n",
    "avg_by_weekday.to_csv(OUTPUT_DIR / 'avg_duration_by_weekday.csv')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(avg_by_weekday.index, avg_by_weekday.values)\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Average duration (seconds)')\n",
    "plt.title('Average trip duration by weekday')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_weekday.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 4) OD heatmap: average duration per (start_station_id x end_station_id) ---\n",
    "# To limit memory use, we will restrict to top N stations by activity, but also\n",
    "# save a CSV of aggregated averages for all pairs.\n",
    "\n",
    "# Aggregate per pair\n",
    "pair_agg = combined.groupby(['start_station_id','end_station_id'])['duration'].agg(['mean','count']).reset_index()\n",
    "pair_agg.rename(columns={'mean':'avg_duration','count':'trip_count'}, inplace=True)\n",
    "pair_agg.to_csv(OUTPUT_DIR / 'od_pair_avg_duration_all_pairs.csv', index=False)\n",
    "\n",
    "# Choose top stations by total trips (to make a manageable heatmap)\n",
    "station_activity = pd.concat([combined['start_station_id'], combined['end_station_id']]).value_counts()\n",
    "top_n = 100  # adjust if you want larger/smaller matrix\n",
    "top_stations = station_activity.index[:top_n].astype(int).tolist()\n",
    "print(f'Creating OD heatmap for top {len(top_stations)} stations by activity')\n",
    "\n",
    "# Pivot for top stations\n",
    "subset = pair_agg[pair_agg['start_station_id'].isin(top_stations) & pair_agg['end_station_id'].isin(top_stations)]\n",
    "heat = subset.pivot(index='start_station_id', columns='end_station_id', values='avg_duration').reindex(index=top_stations, columns=top_stations).fillna(0)\n",
    "\n",
    "# Plot heatmap (use log scale for color or linear depending on spread)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(np.log1p(heat.values), aspect='auto')\n",
    "plt.colorbar(label='log1p(avg_duration)')\n",
    "plt.title(f'OD average duration heatmap (top {len(top_stations)} stations)')\n",
    "plt.xlabel('end_station_id (ordered by activity)')\n",
    "plt.ylabel('start_station_id (ordered by activity)')\n",
    "# keep tick labels sparse for readability\n",
    "n = len(top_stations)\n",
    "plt.xticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.yticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'od_avg_duration_heatmap_top{top_n}.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "print('Analysis complete. Outputs saved to:', OUTPUT_DIR.resolve())\n"
   ],
   "id": "a1785be491b730e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_4716\\3145085024.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 460,232 rows (removed 423 invalid rows)\n",
      "Creating OD heatmap for top 100 stations by activity\n",
      "Analysis complete. Outputs saved to: E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:43:28.082197Z",
     "start_time": "2025-11-02T10:43:00.634873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "save_hourly_maps_with_basemap.py\n",
    "\n",
    "Saves per-hour origin-demand maps to disk (one PNG per hour) and also saves\n",
    "weekday-hour heatmaps. Uses the in-memory `combined` DataFrame if present; otherwise\n",
    "expects you to load it prior to running this script.\n",
    "\n",
    "Outputs are written to: E:/Uni_PGT/visualisation_outputs/station_hour_maps\n",
    "\n",
    "Requirements:\n",
    " - geopandas, matplotlib, seaborn, contextily (optional, for web basemap tiles)\n",
    "   Install with: pip install geopandas matplotlib seaborn contextily\n",
    "\n",
    "Notes on basemap: contextily fetches tiles from the web. If you have no internet,\n",
    "this script will fall back to plotting station points on plain axes.\n",
    "\n",
    "Run this in the same Python session where `combined` exists (not by re-loading the CSV).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/station_hour_maps')\n",
    "HEATMAP_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/heatmaps_weekday_hour')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HEATMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 200\n",
    "POINT_SCALE = 2000  # adjust to scale marker sizes (increase for larger markers)\n",
    "USE_CONTEXTILY = True  # set False if you don't want to fetch basemap tiles\n",
    "TILE_SOURCE = None  # default contextily source (None uses provider's default)\n",
    "# ------------------------\n",
    "\n",
    "print('Using in-memory DataFrame `combined`')\n",
    "try:\n",
    "    combined  # must exist in the environment\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first.')\n",
    "\n",
    "# Ensure datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "\n",
    "# Prepare station aggregated counts by hour\n",
    "agg = combined.groupby(['start_station_id','start_station_latitude','start_station_longitude','hour']).size().reset_index(name='starts')\n",
    "# If lat/lon columns have different names, try alternatives\n",
    "if agg['start_station_latitude'].isna().all() or agg['start_station_longitude'].isna().all():\n",
    "    # try alternative names in combined\n",
    "    lat_col = None\n",
    "    lon_col = None\n",
    "    for c in combined.columns:\n",
    "        if c.lower().endswith('latitude') and 'start' in c.lower():\n",
    "            lat_col = c\n",
    "        if c.lower().endswith('longitude') and 'start' in c.lower():\n",
    "            lon_col = c\n",
    "    if lat_col and lon_col:\n",
    "        agg = combined.groupby(['start_station_id', lat_col, lon_col, 'hour']).size().reset_index(name='starts')\n",
    "        agg = agg.rename(columns={lat_col:'start_station_latitude', lon_col:'start_station_longitude'})\n",
    "\n",
    "# drop rows without coordinates\n",
    "agg = agg.dropna(subset=['start_station_latitude','start_station_longitude'])\n",
    "\n",
    "# Build GeoDataFrame (EPSG:4326)\n",
    "agg['geometry'] = [Point(xy) for xy in zip(agg['start_station_longitude'].astype(float), agg['start_station_latitude'].astype(float))]\n",
    "gdf = gpd.GeoDataFrame(agg, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Project to Web Mercator for contextily (if using basemap)\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_web = gdf.to_crs(epsg=3857)\n",
    "    except Exception as e:\n",
    "        print('Could not reproject to WebMercator, disabling contextily basemap:', e)\n",
    "        USE_CONTEXTILY = False\n",
    "        gdf_web = gdf\n",
    "else:\n",
    "    gdf_web = gdf\n",
    "\n",
    "# Determine map extent (in web mercator if using contextily)\n",
    "minx, miny, maxx, maxy = gdf_web.total_bounds\n",
    "xpad = (maxx - minx) * 0.08 if maxx > minx else 100\n",
    "ypad = (maxy - miny) * 0.08 if maxy > miny else 100\n",
    "extent = (minx - xpad, maxx + xpad, miny - ypad, maxy + ypad)\n",
    "\n",
    "# Save one PNG per hour\n",
    "print('Saving hourly station-origin maps to:', OUT_DIR)\n",
    "for h in range(24):\n",
    "    hour_gdf = gdf_web[gdf_web['hour'] == h]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # plot all stations faintly as background\n",
    "    gdf_web.plot(ax=ax, color='lightgrey', markersize=5, alpha=0.5)\n",
    "\n",
    "    if not hour_gdf.empty:\n",
    "        # marker size scaled by sqrt to reduce dynamic range\n",
    "        sizes = np.sqrt(hour_gdf['starts'].astype(float) + 1) * (POINT_SCALE / max(1, np.sqrt(hour_gdf['starts'].max()*200+ 1)))\n",
    "        hour_gdf.plot(ax=ax, markersize=sizes, column='starts', cmap='Reds', legend=True, alpha=0.9)\n",
    "\n",
    "    if USE_CONTEXTILY:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            if TILE_SOURCE is None:\n",
    "                ctx.add_basemap(ax, crs=gdf_web.crs.to_string())\n",
    "            else:\n",
    "                ctx.add_basemap(ax, source=TILE_SOURCE, crs=gdf_web.crs.to_string())\n",
    "        except Exception as e:\n",
    "            print('contextily failed; continuing without basemap:', e)\n",
    "\n",
    "    ax.set_xlim(extent[0], extent[1])\n",
    "    ax.set_ylim(extent[2], extent[3])\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f'Origin starts — hour {h}')\n",
    "\n",
    "    outpath = OUT_DIR / f'station_starts_hour_{h:02d}.png'\n",
    "    fig.savefig(outpath, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print('Hourly maps saved.')\n",
    "\n",
    "# --- Save weekday-hour heatmap (counts) ---\n",
    "print('Saving weekday-hour heatmap...')\n",
    "combined['weekday'] = pd.Categorical(combined['weekday'], categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], ordered=True)\n",
    "pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(pivot, cmap='YlOrRd')\n",
    "plt.title('Trip starts by weekday and hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Weekday')\n",
    "hm_path = HEATMAP_DIR / 'weekday_hour_heatmap_all_data.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(hm_path, dpi=DPI)\n",
    "plt.close()\n",
    "print('Weekday-hour heatmap saved to', hm_path)\n",
    "\n",
    "print('\\nAll outputs written to:', OUT_DIR.parent)\n"
   ],
   "id": "567fec6fb072f56a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory DataFrame `combined`\n",
      "Saving hourly station-origin maps to: E:\\Uni_PGT\\visualisation_outputs\\station_hour_maps\n",
      "Hourly maps saved.\n",
      "Saving weekday-hour heatmap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_4716\\650905050.py:131: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekday-hour heatmap saved to E:\\Uni_PGT\\visualisation_outputs\\heatmaps_weekday_hour\\weekday_hour_heatmap_all_data.png\n",
      "\n",
      "All outputs written to: E:\\Uni_PGT\\visualisation_outputs\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T10:44:33.294312Z",
     "start_time": "2025-11-02T10:43:36.848547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hourly_maps_and_separate_bars_fixed_extent.py\n",
    "#\n",
    "# Creates per-hour maps (origins left, destinations right) and saves them to disk\n",
    "# (maps and separate bar charts). This version computes a robust map extent using\n",
    "# percentiles (2nd-98th) to ignore coordinate outliers so plots zoom into Edinburgh.\n",
    "#\n",
    "# Run in the same session where `combined` exists (the combined DataFrame you made).\n",
    "# Requires: geopandas, matplotlib, seaborn, contextily (optional for basemap tiles).\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/hourly_maps_v3_fixed_extent')\n",
    "MAP_DIR = OUT_DIR / 'maps'\n",
    "BARS_DIR = OUT_DIR / 'bars'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BARS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 180\n",
    "USE_CONTEXTILY = True     # set False if no internet or you don't want basemap tiles\n",
    "POINT_BASE = 35           # base marker size (smaller => less overlap)\n",
    "TOP_K_BARS = 12\n",
    "CMAP_ORIG = 'Reds'\n",
    "CMAP_DEST = 'Blues'\n",
    "# ---------------------------------\n",
    "\n",
    "# ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DataFrame `combined` not found in memory. Load it first.\")\n",
    "\n",
    "# parse datetimes and hour\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# flexible column detection\n",
    "def find_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "s_lat = find_col(combined, ['start_station_latitude','start_latitude','start_lat','start_station_lat'])\n",
    "s_lon = find_col(combined, ['start_station_longitude','start_longitude','start_lon','start_station_lon'])\n",
    "e_lat = find_col(combined, ['end_station_latitude','end_latitude','end_lat','end_station_lat'])\n",
    "e_lon = find_col(combined, ['end_station_longitude','end_longitude','end_lon','end_station_lon'])\n",
    "\n",
    "if not all([s_lat, s_lon, e_lat, e_lon]):\n",
    "    raise RuntimeError('Could not find necessary start/end latitude/longitude columns in combined dataframe.')\n",
    "\n",
    "# aggregate\n",
    "orig_agg = combined.groupby(['start_station_id', s_lat, s_lon, 'hour']).size().reset_index(name='count')\n",
    "orig_agg = orig_agg.rename(columns={s_lat:'lat', s_lon:'lon', 'start_station_id':'station_id'})\n",
    "\n",
    "dest_agg = combined.groupby(['end_station_id', e_lat, e_lon, 'hour']).size().reset_index(name='count')\n",
    "dest_agg = dest_agg.rename(columns={e_lat:'lat', e_lon:'lon', 'end_station_id':'station_id'})\n",
    "\n",
    "orig_agg = orig_agg.dropna(subset=['lat','lon'])\n",
    "dest_agg = dest_agg.dropna(subset=['lat','lon'])\n",
    "\n",
    "# GeoDataFrames (WGS84)\n",
    "gdf_o = gpd.GeoDataFrame(orig_agg, geometry=[Point(xy) for xy in zip(orig_agg['lon'].astype(float), orig_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "gdf_d = gpd.GeoDataFrame(dest_agg, geometry=[Point(xy) for xy in zip(dest_agg['lon'].astype(float), dest_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "\n",
    "# project to WebMercator for basemap if desired\n",
    "use_ctx = False\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_o_web = gdf_o.to_crs(epsg=3857)\n",
    "        gdf_d_web = gdf_d.to_crs(epsg=3857)\n",
    "        use_ctx = True\n",
    "    except Exception as e:\n",
    "        print('Contextily disabled (reprojection failed):', e)\n",
    "        gdf_o_web = gdf_o\n",
    "        gdf_d_web = gdf_d\n",
    "else:\n",
    "    gdf_o_web = gdf_o\n",
    "    gdf_d_web = gdf_d\n",
    "\n",
    "# ---------------------------\n",
    "# ROBUST EXTENT (2nd - 98th percentile)\n",
    "# ---------------------------\n",
    "# collect x,y arrays (projected)\n",
    "xs = pd.concat([\n",
    "    gdf_o_web.geometry.x.rename('x') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.x.rename('x') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "ys = pd.concat([\n",
    "    gdf_o_web.geometry.y.rename('y') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.y.rename('y') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(xs) == 0 or len(ys) == 0 or not np.isfinite(xs.to_numpy()).any():\n",
    "    raise RuntimeError('No valid projected coordinates found to compute map extent.')\n",
    "\n",
    "low_pct, high_pct = 2, 98\n",
    "minx, maxx = np.percentile(xs, [low_pct, high_pct])\n",
    "miny, maxy = np.percentile(ys, [low_pct, high_pct])\n",
    "\n",
    "# fallback to full min/max if needed\n",
    "full_minx, full_maxx = xs.min(), xs.max()\n",
    "full_miny, full_maxy = ys.min(), ys.max()\n",
    "\n",
    "# add padding (at least 300 m)\n",
    "span_x = maxx - minx\n",
    "span_y = maxy - miny\n",
    "pad_x = max(span_x * 0.06, 300)\n",
    "pad_y = max(span_y * 0.06, 300)\n",
    "extent = (minx - pad_x, maxx + pad_x, miny - pad_y, maxy + pad_y)\n",
    "\n",
    "# safety clamp if extent absurdly large\n",
    "max_allowed_span = 200_000  # 200 km\n",
    "if (extent[1] - extent[0] > max_allowed_span) or (extent[3] - extent[2] > max_allowed_span):\n",
    "    print('WARNING: computed extent is very large. Falling back to full bounds.')\n",
    "    pad_x_f = max((full_maxx - full_minx) * 0.06, 300)\n",
    "    pad_y_f = max((full_maxy - full_miny) * 0.06, 300)\n",
    "    extent = (full_minx - pad_x_f, full_maxx + pad_x_f, full_miny - pad_y_f, full_maxy + pad_y_f)\n",
    "\n",
    "# print sample outliers (stations outside percentile window) to help debugging\n",
    "outlier_mask_o = (gdf_o_web.geometry.x < minx) | (gdf_o_web.geometry.x > maxx) | (gdf_o_web.geometry.y < miny) | (gdf_o_web.geometry.y > maxy)\n",
    "outlier_mask_d = (gdf_d_web.geometry.x < minx) | (gdf_d_web.geometry.x > maxx) | (gdf_d_web.geometry.y < miny) | (gdf_d_web.geometry.y > maxy)\n",
    "outliers_o = gdf_o_web.loc[outlier_mask_o, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "outliers_d = gdf_d_web.loc[outlier_mask_d, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "if not outliers_o.empty or not outliers_d.empty:\n",
    "    print('Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):')\n",
    "    if not outliers_o.empty:\n",
    "        print(' Origin outliers:')\n",
    "        print(outliers_o.to_string(index=False))\n",
    "    if not outliers_d.empty:\n",
    "        print(' Destination outliers:')\n",
    "        print(outliers_d.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# plotting helpers\n",
    "# ---------------------------\n",
    "def sizes_from_counts(series, base=POINT_BASE):\n",
    "    arr = np.sqrt(series.fillna(0).astype(float) + 1.0)\n",
    "    if arr.max() > 0:\n",
    "        scaled = base * (arr / arr.max())\n",
    "    else:\n",
    "        scaled = np.full_like(arr, base * 0.2)\n",
    "    return np.clip(scaled, 2, base * 1.1)\n",
    "\n",
    "# main loop: produce map PNG and a separate bar PNG for each hour\n",
    "for h in range(24):\n",
    "    o_h = gdf_o_web[gdf_o_web['hour'] == h].copy()\n",
    "    d_h = gdf_d_web[gdf_d_web['hour'] == h].copy()\n",
    "\n",
    "    # compute maximum count for color scaling\n",
    "    max_count = int(max(o_h['count'].max() if not o_h.empty else 0,\n",
    "                        d_h['count'].max() if not d_h.empty else 0, 1))\n",
    "\n",
    "    # ---------- MAP FIGURE (orig left, dest right) ----------\n",
    "    fig, (ax_o, ax_d) = plt.subplots(1, 2, figsize=(14, 8), constrained_layout=True)\n",
    "\n",
    "    # set extent and add basemap first (so tiles cover area)\n",
    "    for ax in (ax_o, ax_d):\n",
    "        ax.set_xlim(extent[0], extent[1])\n",
    "        ax.set_ylim(extent[2], extent[3])\n",
    "\n",
    "    if use_ctx:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            ctx.add_basemap(ax_o, crs=gdf_o_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "            ctx.add_basemap(ax_d, crs=gdf_d_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "        except Exception as e:\n",
    "            print('contextily error (continuing without basemap):', e)\n",
    "\n",
    "    # faint background of all stations (for context)\n",
    "    if not gdf_o_web.empty:\n",
    "        ax_o.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "        ax_d.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "\n",
    "    # plot origins (red)\n",
    "    if not o_h.empty:\n",
    "        sizes_o = sizes_from_counts(o_h['count'])\n",
    "        cmap_o = plt.colormaps.get_cmap(CMAP_ORIG)\n",
    "        norm_o = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_o = cmap_o(norm_o(o_h['count'].astype(float)))\n",
    "        ax_o.scatter(o_h.geometry.x, o_h.geometry.y, s=sizes_o, color=colors_o, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_o = cm.ScalarMappable(norm=norm_o, cmap=CMAP_ORIG)\n",
    "        sm_o._A = []\n",
    "        fig.colorbar(sm_o, ax=ax_o, fraction=0.046, pad=0.02).set_label('Origin count (hour)')\n",
    "\n",
    "    ax_o.set_title(f'Origins — hour {h}')\n",
    "    ax_o.axis('off')\n",
    "\n",
    "    # plot destinations (blue)\n",
    "    if not d_h.empty:\n",
    "        sizes_d = sizes_from_counts(d_h['count'])\n",
    "        cmap_d = plt.colormaps.get_cmap(CMAP_DEST)\n",
    "        norm_d = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_d = cmap_d(norm_d(d_h['count'].astype(float)))\n",
    "        ax_d.scatter(d_h.geometry.x, d_h.geometry.y, s=sizes_d, color=colors_d, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_d = cm.ScalarMappable(norm=norm_d, cmap=CMAP_DEST)\n",
    "        sm_d._A = []\n",
    "        fig.colorbar(sm_d, ax=ax_d, fraction=0.046, pad=0.02).set_label('Destination count (hour)')\n",
    "\n",
    "    ax_d.set_title(f'Destinations — hour {h}')\n",
    "    ax_d.axis('off')\n",
    "\n",
    "    map_out = MAP_DIR / f'hour_{h:02d}_maps.png'\n",
    "    fig.savefig(map_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- BARCHART FIGURE (separate) ----------\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5), constrained_layout=True)\n",
    "\n",
    "    if not o_h.empty:\n",
    "        top_o = o_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_o = top_o['station_id'].astype(str).values[::-1]   # reversed for horizontal bars\n",
    "        counts_o = top_o['count'].values[::-1]\n",
    "        ax1.barh(labels_o, counts_o, color='tab:red')\n",
    "        ax1.set_title(f'Top {TOP_K_BARS} origin stations (hour {h})')\n",
    "        ax1.set_xlabel('Starts')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No origin data', ha='center', va='center')\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "    if not d_h.empty:\n",
    "        top_d = d_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_d = top_d['station_id'].astype(str).values[::-1]\n",
    "        counts_d = top_d['count'].values[::-1]\n",
    "        ax2.barh(labels_d, counts_d, color='tab:blue')\n",
    "        ax2.set_title(f'Top {TOP_K_BARS} destination stations (hour {h})')\n",
    "        ax2.set_xlabel('Ends')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No destination data', ha='center', va='center')\n",
    "        ax2.set_axis_off()\n",
    "\n",
    "    bars_out = BARS_DIR / f'hour_{h:02d}_bars.png'\n",
    "    fig2.savefig(bars_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig2)\n",
    "\n",
    "print('Saved maps to', MAP_DIR)\n",
    "print('Saved bar charts to', BARS_DIR)\n"
   ],
   "id": "8bb4164da9370389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):\n",
      " Origin outliers:\n",
      " station_id       lat       lon\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "       1870 55.990530 -3.385597\n",
      " Destination outliers:\n",
      " station_id       lat       lon\n",
      "        280 53.395525 -2.990138\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "Saved maps to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\maps\n",
      "Saved bar charts to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\bars\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T12:19:59.327795Z",
     "start_time": "2025-11-02T12:19:45.679935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "contiguous_hour_clustering_and_gravity.py\n",
    "\n",
    "Performs contiguous clustering of hours (so clusters are consecutive hour-intervals),\n",
    "aggregates OD matrices per cluster, and runs the doubly-constrained gravity model\n",
    "on each cluster using your two impedance functions. No plotting.\n",
    "\n",
    "Usage: run in the same Python session where `combined` exists (you already created it).\n",
    "\n",
    "Outputs (CSV) saved to OUTPUT_DIR:\n",
    " - cluster_membership.csv  (which hours belong to which cluster)\n",
    " - cluster_od_{c}.csv      (aggregated OD counts for cluster c)\n",
    " - cost_matrix_cluster_{c}.csv\n",
    " - predicted_gravity_det1_cluster_{c}.csv\n",
    " - predicted_gravity_det2_cluster_{c}.csv\n",
    " - diagnostics_det1_cluster_{c}.csv\n",
    " - diagnostics_det2_cluster_{c}.csv\n",
    " - metrics_det1_cluster_{c}.csv\n",
    " - metrics_det2_cluster_{c}.csv\n",
    "\n",
    "Notes:\n",
    " - Clustering is performed on vectorized hourly OD patterns (flattened matrices) with\n",
    "   optional scaling + PCA reduction before clustering.\n",
    " - Contiguity is enforced using AgglomerativeClustering with a chain connectivity matrix.\n",
    " - Gravity model uses ALL stations present in the aggregated cluster OD (no top-K truncation).\n",
    " - If many stations appear in a cluster (e.g. >1500) the cost matrix and solver may be slow.\n",
    "\n",
    "Adjust USER CONFIG below to change number of clusters, PCA variance retained, beta grid, etc.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLUSTER_COUNT = 4            # number of contiguous clusters to form\n",
    "PCA_VARIANCE = 0.90          # keep PCA components explaining this fraction of variance\n",
    "BETA_1 = 0.0005\n",
    "BETA_2 = 0.0005\n",
    "ERROR_THRESHOLD = 0.01\n",
    "IMPROVEMENT_THRESHOLD = 1e-6\n",
    "MAX_ITERS = 2000\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# Ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first as `combined`.')\n",
    "\n",
    "# Precompute hourly OD pivot tables (raw counts per hour)\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "if 'hour' not in combined.columns:\n",
    "    combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "hourly_pivots = {}\n",
    "for h in range(24):\n",
    "    sub = combined[combined['hour'] == h]\n",
    "    if sub.empty:\n",
    "        hourly_pivots[h] = pd.DataFrame()\n",
    "        continue\n",
    "    counts = sub.groupby(['start_station_id', 'end_station_id']).size().reset_index(name='count')\n",
    "    pivot = counts.pivot(index='start_station_id', columns='end_station_id', values='count').fillna(0)\n",
    "    hourly_pivots[h] = pivot\n",
    "\n",
    "# Build aligned feature vectors for each hour using union of station ids\n",
    "hours = sorted(hourly_pivots.keys())\n",
    "all_stations = sorted({int(s) for h in hours for s in hourly_pivots[h].index.tolist() + hourly_pivots[h].columns.tolist()})\n",
    "if len(all_stations) == 0:\n",
    "    raise RuntimeError('No station IDs found in hourly pivots--check combined data')\n",
    "\n",
    "def pivot_to_aligned_vector(pivot_df, stations):\n",
    "    mat = pd.DataFrame(0.0, index=stations, columns=stations)\n",
    "    if not pivot_df.empty:\n",
    "        tmp = pivot_df.reindex(index=stations, columns=stations).fillna(0)\n",
    "        mat.iloc[:, :] = tmp.values\n",
    "    return mat.values.flatten()\n",
    "\n",
    "X_list = []\n",
    "for h in hours:\n",
    "    vec = pivot_to_aligned_vector(hourly_pivots[h], all_stations)\n",
    "    X_list.append(vec)\n",
    "X = np.vstack(X_list)  # shape (24, n_features)\n",
    "\n",
    "# Scale + PCA\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=PCA_VARIANCE, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Build chain connectivity (adjacency) for contiguous clustering\n",
    "n_hours = X_pca.shape[0]\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for i in range(n_hours - 1):\n",
    "    rows.extend([i, i+1])\n",
    "    cols.extend([i+1, i])\n",
    "    data.extend([1, 1])\n",
    "connectivity = csr_matrix((data, (rows, cols)), shape=(n_hours, n_hours))\n",
    "\n",
    "# Run contiguous agglomerative clustering\n",
    "agg = AgglomerativeClustering(n_clusters=CLUSTER_COUNT, linkage='ward', connectivity=connectivity)\n",
    "labels = agg.fit_predict(X_pca)\n",
    "\n",
    "hour_cluster_df = pd.DataFrame({'hour': hours, 'cluster': labels}).sort_values('hour')\n",
    "hour_cluster_df.to_csv(OUTPUT_DIR / 'cluster_membership.csv', index=False)\n",
    "print('Hour -> Cluster mapping saved to', OUTPUT_DIR / 'cluster_membership.csv')\n",
    "\n",
    "# Helper: haversine pairwise\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Impedance functions\n",
    "def new_cost1(cost_matrix, beta=BETA_1):\n",
    "    return np.exp(-beta * cost_matrix)\n",
    "\n",
    "def new_cost2(cost_matrix, beta=BETA_2):\n",
    "    return np.exp(-beta * (np.log(cost_matrix + 1.0) ** 2))\n",
    "\n",
    "# Gravity model (doubly-constrained IPF)\n",
    "def gravity_model(O, D, det, error_threshold=ERROR_THRESHOLD, improvement_threshold=IMPROVEMENT_THRESHOLD, max_iters=MAX_ITERS):\n",
    "    O = np.array(O, dtype=float).copy()\n",
    "    D = np.array(D, dtype=float).copy()\n",
    "    sum_O = O.sum(); sum_D = D.sum()\n",
    "    if sum_O <= 0 or sum_D <= 0:\n",
    "        raise ValueError('Origin or Destination totals sum to zero')\n",
    "    if abs(sum_O - sum_D) > 1e-9:\n",
    "        D = D * (sum_O / sum_D)\n",
    "    n = len(O)\n",
    "    Ai = np.ones(n)\n",
    "    Bj = np.ones(n)\n",
    "    prev_error = np.inf\n",
    "    Tij = np.zeros((n, n), dtype=float)\n",
    "    det_mat = np.array(det, dtype=float).copy()\n",
    "    det_mat[det_mat < EPS] = EPS\n",
    "    iteration = 0\n",
    "    while iteration < max_iters:\n",
    "        iteration += 1\n",
    "        denom_i = (det_mat * (Bj * D)[None, :]).sum(axis=1) + EPS\n",
    "        Ai = 1.0 / denom_i\n",
    "        denom_j = (det_mat * (Ai * O)[:, None]).sum(axis=0) + EPS\n",
    "        Bj_new = 1.0 / denom_j\n",
    "        Tij = (Ai * O)[:, None] * (Bj_new * D)[None, :] * det_mat\n",
    "        error = (np.abs(O - Tij.sum(axis=1)).sum() + np.abs(D - Tij.sum(axis=0)).sum()) / (sum_O + EPS)\n",
    "        improvement = abs(prev_error - error)\n",
    "        if error < error_threshold:\n",
    "            stop_reason = 'Error threshold met'\n",
    "            break\n",
    "        if improvement < improvement_threshold:\n",
    "            stop_reason = 'Slow improvement'\n",
    "            break\n",
    "        prev_error = error\n",
    "        Bj = Bj_new\n",
    "    else:\n",
    "        stop_reason = 'max_iters'\n",
    "    diagnostics = {'iterations': iteration, 'error': float(error), 'stop_reason': stop_reason}\n",
    "    return Tij, diagnostics\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(predicted_T, observed_T_df):\n",
    "    obs = observed_T_df.to_numpy().astype(float)\n",
    "    pred = np.array(predicted_T, dtype=float)\n",
    "    if obs.shape != pred.shape:\n",
    "        raise ValueError('Predicted and observed shapes differ')\n",
    "    obs_f = obs.flatten(); pred_f = pred.flatten()\n",
    "    mse = np.mean((obs_f - pred_f) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    ss_tot = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "    ss_res = np.sum((obs_f - pred_f) ** 2)\n",
    "    r2 = float(1.0 - (ss_res / (ss_tot + EPS)))\n",
    "    return {'rmse': rmse, 'r2': r2}\n",
    "\n",
    "# Aggregate OD per cluster and run gravity\n",
    "for c in sorted(hour_cluster_df['cluster'].unique()):\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster'] == c]['hour'].tolist()\n",
    "    print(f'Processing cluster {c}: hours = {hrs}')\n",
    "    # sum OD counts across hours in cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    agg = agg.fillna(0)\n",
    "    if agg.empty:\n",
    "        print(f'Cluster {c}: empty aggregated OD, skipping')\n",
    "        continue\n",
    "    # ensure square by intersection\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c}: too few common stations ({len(common)}), skipping')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "    agg.to_csv(OUTPUT_DIR / f'cluster_od_{c}.csv')\n",
    "\n",
    "    n = len(common)\n",
    "    print(f'Cluster {c}: n_stations={n}')\n",
    "    if n > 1500:\n",
    "        print('WARNING: cluster has many stations (>1500); this may be slow and memory-heavy')\n",
    "\n",
    "    # build coord df (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station lon/lat columns in combined')\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "    if len(coord_df) != n:\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        if missing:\n",
    "            print(f'Cluster {c}: dropping {len(missing)} stations with missing coords (sample): {list(missing)[:10]}')\n",
    "            keep = [s for s in common if s not in missing]\n",
    "            if len(keep) < 2:\n",
    "                print(f'Cluster {c}: too few stations after dropping, skipping')\n",
    "                continue\n",
    "            agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "            coord_df = coord_df.reindex(keep)\n",
    "            common = keep\n",
    "            n = len(common)\n",
    "    # compute cost matrix\n",
    "    lons = coord_df['lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df['lat'].to_numpy(dtype=float)\n",
    "    cost_m = haversine_pairwise(lons, lats)\n",
    "    pd.DataFrame(cost_m, index=common, columns=common).to_csv(OUTPUT_DIR / f'cost_matrix_cluster_{c}.csv')\n",
    "\n",
    "    # deterrence matrices\n",
    "    det1 = new_cost1(cost_m, beta=BETA_1)\n",
    "    det2 = new_cost2(cost_m, beta=BETA_2)\n",
    "\n",
    "    # totals\n",
    "    O = agg.sum(axis=1).to_numpy()\n",
    "    D = agg.sum(axis=0).to_numpy()\n",
    "\n",
    "    # run gravity\n",
    "    Tij1, diag1 = gravity_model(O.copy(), D.copy(), det1)\n",
    "    pred1_df = pd.DataFrame(Tij1, index=agg.index, columns=agg.columns)\n",
    "    pred1_df.to_csv(OUTPUT_DIR / f'predicted_gravity_det1_cluster_{c}.csv')\n",
    "    metrics1 = calculate_metrics(pred1_df, agg)\n",
    "\n",
    "    Tij2, diag2 = gravity_model(O.copy(), D.copy(), det2)\n",
    "    pred2_df = pd.DataFrame(Tij2, index=agg.index, columns=agg.columns)\n",
    "    pred2_df.to_csv(OUTPUT_DIR / f'predicted_gravity_det2_cluster_{c}.csv')\n",
    "    metrics2 = calculate_metrics(pred2_df, agg)\n",
    "\n",
    "    pd.DataFrame([diag1]).to_csv(OUTPUT_DIR / f'diagnostics_det1_cluster_{c}.csv', index=False)\n",
    "    pd.DataFrame([diag2]).to_csv(OUTPUT_DIR / f'diagnostics_det2_cluster_{c}.csv', index=False)\n",
    "    pd.DataFrame([metrics1]).to_csv(OUTPUT_DIR / f'metrics_det1_cluster_{c}.csv', index=False)\n",
    "    pd.DataFrame([metrics2]).to_csv(OUTPUT_DIR / f'metrics_det2_cluster_{c}.csv', index=False)\n",
    "\n",
    "    print(f'Cluster {c}: done. metrics_det1={metrics1}, metrics_det2={metrics2}')\n",
    "\n",
    "print('All clusters processed. Outputs in', OUTPUT_DIR)\n"
   ],
   "id": "e8359de2fc7a9129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour -> Cluster mapping saved to E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity\\cluster_membership.csv\n",
      "Processing cluster 0: hours = [11, 12, 13, 14, 15]\n",
      "Cluster 0: n_stations=191\n",
      "Cluster 0: done. metrics_det1={'rmse': 17.943880744913418, 'r2': 0.5223031333344428}, metrics_det2={'rmse': 21.549098416439413, 'r2': 0.31106562730878384}\n",
      "Processing cluster 1: hours = [16, 17]\n",
      "Cluster 1: n_stations=186\n",
      "Cluster 1: done. metrics_det1={'rmse': 7.517069932381334, 'r2': 0.5754631478890655}, metrics_det2={'rmse': 9.659908253971874, 'r2': 0.29892542471321204}\n",
      "Processing cluster 2: hours = [18, 19, 20, 21, 22, 23]\n",
      "Cluster 2: n_stations=182\n",
      "Cluster 2: done. metrics_det1={'rmse': 8.776010988006757, 'r2': 0.6272312037982062}, metrics_det2={'rmse': 12.086002404670122, 'r2': 0.2930143689265128}\n",
      "Processing cluster 3: hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Cluster 3: n_stations=187\n",
      "Cluster 3: done. metrics_det1={'rmse': 9.258667853183399, 'r2': 0.5308287108153418}, metrics_det2={'rmse': 11.119763836084159, 'r2': 0.32325423565512046}\n",
      "All clusters processed. Outputs in E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T12:40:02.714572Z",
     "start_time": "2025-11-02T12:40:01.350362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- USER / RUN-TIME CONFIG ----------\n",
    "beta_grid = np.logspace(-6, -2, 25)   # grid to search for beta (adjust if desired)\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity')  # match your cluster script\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EPS = 1e-12\n",
    "# --------------------------------------------\n",
    "\n",
    "# Basic haversine (meters) — safe to include even if you have one already\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Normalized RMSE helper\n",
    "def normalized_rmse(pred_df, obs_df):\n",
    "    obs = obs_df.to_numpy(dtype=float)\n",
    "    pred = pred_df.to_numpy(dtype=float)\n",
    "    mse = np.mean((obs - pred) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mean_obs = float(np.mean(obs))\n",
    "    if mean_obs == 0:\n",
    "        return {'rmse': rmse, 'nrmse': float('inf')}\n",
    "    return {'rmse': rmse, 'nrmse': float(rmse / mean_obs)}\n",
    "\n",
    "# Ensure gravity_model & calculate_metrics exist (from your previous script). If not, provide safe fallback.\n",
    "try:\n",
    "    gravity_model  # noqa\n",
    "except NameError:\n",
    "    raise RuntimeError(\"gravity_model is not defined in the session. Run the clustering/gravity script first (which defines gravity_model).\")\n",
    "\n",
    "try:\n",
    "    calculate_metrics  # noqa\n",
    "except NameError:\n",
    "    # fallback basic metrics (should be similar to your earlier calculate_metrics)\n",
    "    def calculate_metrics(pred_df, obs_df):\n",
    "        obs = obs_df.to_numpy(dtype=float)\n",
    "        pred = pred_df.to_numpy(dtype=float)\n",
    "        if obs.shape != pred.shape:\n",
    "            raise ValueError('Predicted and observed shapes differ')\n",
    "        obs_f = obs.flatten()\n",
    "        pred_f = pred.flatten()\n",
    "        mse = np.mean((obs_f - pred_f) ** 2)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        ss_total = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "        ss_residual = np.sum((obs_f - pred_f) ** 2)\n",
    "        r_squared = float(1.0 - (ss_residual / (ss_total + EPS)))\n",
    "        return {'rmse': rmse, 'r2': r_squared}\n",
    "\n",
    "# Core function: runs grid search for det1 on a single aggregated cluster\n",
    "def run_beta_grid_on_cluster(agg_df, coord_df, cost_func, det_func_factory, beta_values, gravity_model_func, metrics_func):\n",
    "    \"\"\"\n",
    "    Returns best = {'beta', 'metrics', 'pred_df', 'diag'} using primary=r2, secondary=-nrmse\n",
    "    \"\"\"\n",
    "    common = [int(x) for x in agg_df.index.tolist()]\n",
    "    lons = coord_df.loc[common, 'lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df.loc[common, 'lat'].to_numpy(dtype=float)\n",
    "    cost_m = cost_func(lons, lats)\n",
    "\n",
    "    O = agg_df.sum(axis=1).to_numpy()\n",
    "    D = agg_df.sum(axis=0).to_numpy()\n",
    "\n",
    "    best = {'beta': None, 'metrics': None, 'pred_df': None, 'diag': None}\n",
    "    best_score = None\n",
    "\n",
    "    for beta in beta_values:\n",
    "        det = det_func_factory(cost_m, beta=beta)\n",
    "        try:\n",
    "            Tij, diag = gravity_model_func(O.copy(), D.copy(), det)\n",
    "        except Exception:\n",
    "            # solver failure for this beta — skip\n",
    "            continue\n",
    "        pred_df = pd.DataFrame(Tij, index=agg_df.index, columns=agg_df.columns)\n",
    "        mets = metrics_func(pred_df, agg_df)    # should include 'r2'\n",
    "        nr = normalized_rmse(pred_df, agg_df)\n",
    "        mets.update(nr)\n",
    "        score = (mets.get('r2', -9999), - (mets.get('nrmse', np.inf) if np.isfinite(mets.get('nrmse', np.inf)) else np.inf))\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best = {'beta': float(beta), 'metrics': mets, 'pred_df': pred_df.copy(), 'diag': diag}\n",
    "    return best\n",
    "\n",
    "# DET1 factory (exponential), no det2 per your request\n",
    "def det1_factory(costm, beta):\n",
    "    return np.exp(-beta * costm)\n",
    "\n",
    "# Containers for results\n",
    "best_betas = {}          # {cluster: {'beta':..., 'metrics':..., 'diag':...}}\n",
    "predicted_matrices = {}  # {cluster: predicted_df}\n",
    "\n",
    "# Main loop across clusters discovered by your contiguous clustering\n",
    "clusters = sorted(hour_cluster_df['cluster'].unique())\n",
    "\n",
    "for c in clusters:\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster']==c]['hour'].tolist()\n",
    "    # aggregate OD across hours in this cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    if agg is None:\n",
    "        print(f'Cluster {c}: empty aggregation, skipping')\n",
    "        continue\n",
    "    agg = agg.fillna(0)\n",
    "\n",
    "    # square matrix by intersection of rows & cols\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c} skipped (too few common stations)')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "\n",
    "    # build coordinate DF for these common stations (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station longitude/latitude columns in `combined`.')\n",
    "\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "\n",
    "    if len(coord_df) != len(common):\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        print(f'Cluster {c}: dropping {len(missing)} stations missing coords (sample): {list(missing)[:6]}')\n",
    "        keep = [s for s in common if s not in missing]\n",
    "        agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "        coord_df = coord_df.reindex(keep)\n",
    "        common = keep\n",
    "        if len(common) < 2:\n",
    "            print(f'Cluster {c}: too few stations after dropping coords, skipping')\n",
    "            continue\n",
    "\n",
    "    # run grid search for det1 only\n",
    "    best1 = run_beta_grid_on_cluster(agg, coord_df, haversine_pairwise, det1_factory, beta_grid, gravity_model, calculate_metrics)\n",
    "\n",
    "    # save best1 outputs\n",
    "    if best1['beta'] is None:\n",
    "        print(f'Cluster {c}: no successful beta found (solver failed for all candidates)')\n",
    "        continue\n",
    "\n",
    "    best_betas[c] = {'beta_det1': best1['beta'], 'metrics_det1': best1['metrics'], 'diag_det1': best1['diag']}\n",
    "    predicted_matrices[c] = best1['pred_df']\n",
    "\n",
    "    # persist to disk\n",
    "    best1['pred_df'].to_csv(OUTPUT_DIR / f'best_pred_det1_clusterr_{c}.csv')\n",
    "    pd.DataFrame([best1['metrics']]).to_csv(OUTPUT_DIR / f'best_metrics_det1_clusterr_{c}.csv', index=False)\n",
    "\n",
    "    print(f\"Cluster {c}: done. best_beta_det1={best1['beta']:.2e}, metrics={best1['metrics']}\")\n",
    "\n",
    "# save summary CSV\n",
    "summary = []\n",
    "for c, info in best_betas.items():\n",
    "    summary.append({'cluster': c, 'best_beta_det1': info['beta_det1'], **{f\"metrics_{k}\": v for k,v in info['metrics_det1'].items()}})\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv', index=False)\n",
    "\n",
    "print('Grid search (det1 only) completed. Summary saved to:', OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv')\n",
    "print('Best betas dict: variable `best_betas` (in memory).')\n",
    "print('Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).')\n"
   ],
   "id": "37462ec51c3d6ba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: done. best_beta_det1=3.16e-04, metrics={'rmse': 16.370597511801055, 'r2': 0.6023978664037832, 'nrmse': 3.3690183047494133}\n",
      "Cluster 1: done. best_beta_det1=4.64e-04, metrics={'rmse': 7.443588960619201, 'r2': 0.5837224590174693, 'nrmse': 3.147300282095059}\n",
      "Cluster 2: done. best_beta_det1=6.81e-04, metrics={'rmse': 8.66234675178408, 'r2': 0.6368246502399453, 'nrmse': 2.9053713970989565}\n",
      "Cluster 3: done. best_beta_det1=4.64e-04, metrics={'rmse': 9.219286201567018, 'r2': 0.534811453308321, 'nrmse': 3.1500856842441305}\n",
      "Grid search (det1 only) completed. Summary saved to: E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity\\beta_gridsearch_summary_det1_only.csv\n",
      "Best betas dict: variable `best_betas` (in memory).\n",
      "Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T18:43:58.715748Z",
     "start_time": "2025-11-02T18:43:48.703620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "station_to_poi_od_from_predicted_fixed.py\n",
    "\n",
    "Fixed version of the POI-OD conversion that uses gravity-model predicted\n",
    "station->station OD matrices (per cluster), and avoids the ValueError you ran\n",
    "into by correctly sizing the POI index for each output matrix.\n",
    "\n",
    "Assumptions:\n",
    " - `predicted_matrices` is a dict in memory mapping cluster -> pandas.DataFrame\n",
    "   with station IDs as index and columns (square) containing predicted Tij counts.\n",
    " - `combined` DataFrame is in memory (for station coords).\n",
    " - POIs CSV is at POI_FILE and contains columns ['lat','lon','category'] and\n",
    "   optionally 'poi_id'.\n",
    "\n",
    "Saves per-cluster POI OD CSVs (only for POIs that receive >0 allocation for\n",
    "stations in that cluster), plus an aggregated POI OD across clusters.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "CATEGORY_WEIGHTS = {\n",
    "    'library': 1.1,\n",
    "    'school': 1.3,\n",
    "    'university': 1.5,\n",
    "    'residential': 1.0,\n",
    "    'commercial': 1.2,\n",
    "    'hospital': 1.4\n",
    "}\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# --- sanity checks: required in-memory objects ---\n",
    "try:\n",
    "    predicted_matrices\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`predicted_matrices` not found in memory. Run gravity grid search first.\")\n",
    "\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`combined` not found in memory. Load your combined trips dataframe.\")\n",
    "\n",
    "# --- load POIs ---\n",
    "pois = pd.read_csv(POI_FILE)\n",
    "pois = pois.dropna(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['category'] = pois['category'].astype(str).fillna('commercial').str.lower().str.strip()\n",
    "pois['weight'] = pois['category'].map(CATEGORY_WEIGHTS).fillna(1.0)\n",
    "\n",
    "# --- build station coords (median of start/end) ---\n",
    "lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "\n",
    "coord_parts = []\n",
    "if start_lon and start_lat:\n",
    "    tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "    tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "    coord_parts.append(tmp)\n",
    "if end_lon and end_lat:\n",
    "    tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "    tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "    coord_parts.append(tmp2)\n",
    "if not coord_parts:\n",
    "    raise RuntimeError('No station lon/lat columns found in `combined`.')\n",
    "\n",
    "stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "stations_df = stations_df.rename_axis('station_id')\n",
    "stations_df.index = stations_df.index.astype(int)\n",
    "# restrict to stations present in any predicted matrix to avoid waste\n",
    "pred_station_ids = set()\n",
    "for df in predicted_matrices.values():\n",
    "    pred_station_ids.update([int(x) for x in df.index.astype(int).tolist()])\n",
    "stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "if stations_df.empty:\n",
    "    raise RuntimeError('No station coordinates available for stations present in predicted_matrices.')\n",
    "station_ids = stations_df.index.astype(int).tolist()\n",
    "\n",
    "# --- helper: haversine (vectorized) ---\n",
    "def haversine_matrix(lonA, latA, lonB, latB):\n",
    "    R = 6371000.0\n",
    "    lonA = np.radians(np.asarray(lonA, dtype=float))\n",
    "    latA = np.radians(np.asarray(latA, dtype=float))\n",
    "    lonB = np.radians(np.asarray(lonB, dtype=float))\n",
    "    latB = np.radians(np.asarray(latB, dtype=float))\n",
    "    dlon = lonA[:, None] - lonB[None, :]\n",
    "    dlat = latA[:, None] - latB[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(latA)[:, None] * np.cos(latB)[None, :] * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# --- precompute nearest POIs for stations using haversine via sklearn's haversine metric ---\n",
    "poi_coords = pois[['lat', 'lon']].to_numpy()\n",
    "station_coords = stations_df[['lat', 'lon']].to_numpy()\n",
    "\n",
    "# use scikit-learn NearestNeighbors with haversine (expects radians)\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "nbrs.fit(np.radians(poi_coords))\n",
    "dists_r, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "# convert radian distances to meters\n",
    "dists_m = dists_r * 6371000.0\n",
    "\n",
    "# --- build allocations: station_id -> dict(poi_index -> weight) ---\n",
    "allocations = {}\n",
    "for i, sid in enumerate(station_ids):\n",
    "    dists = dists_m[i]\n",
    "    idx = idxs[i]\n",
    "    # choose close ones if any inside CLOSE_RADIUS_METERS\n",
    "    mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "    if mask_close.any():\n",
    "        chosen_idx = idx[mask_close]\n",
    "        chosen_dists = dists[mask_close]\n",
    "    else:\n",
    "        chosen_idx = idx\n",
    "        chosen_dists = dists\n",
    "\n",
    "    cat_weights = pois.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "    inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "    raw_scores = cat_weights * inv_dist\n",
    "    if raw_scores.sum() <= 0:\n",
    "        weights = np.ones_like(raw_scores) / len(raw_scores)\n",
    "    else:\n",
    "        weights = raw_scores / raw_scores.sum()\n",
    "\n",
    "    allocations[int(sid)] = {int(p_idx): float(w) for p_idx, w in zip(chosen_idx.tolist(), weights.tolist())}\n",
    "\n",
    "# Save allocations (flat)\n",
    "alloc_rows = []\n",
    "for s, pdict in allocations.items():\n",
    "    for pidx, w in pdict.items():\n",
    "        alloc_rows.append({'station_id': int(s), 'poi_index': int(pidx), 'poi_id': int(pois.at[pidx, 'poi_id']), 'weight': float(w)})\n",
    "alloc_df = pd.DataFrame(alloc_rows)\n",
    "alloc_df.to_csv(OUTPUT_DIR / 'station_poi_allocations.csv', index=False)\n",
    "print('Saved station->POI allocation table to', OUTPUT_DIR / 'station_poi_allocations.csv')\n",
    "\n",
    "# --- Convert predicted station OD -> POI OD per cluster (using only used POIs per cluster) ---\n",
    "poi_od_per_cluster = {}\n",
    "for c, pred_df in predicted_matrices.items():\n",
    "    # ensure pred_df index/cols are ints\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df.index = pred_df.index.astype(int)\n",
    "    pred_df.columns = pred_df.columns.astype(int)\n",
    "\n",
    "    # intersect stations with allocations\n",
    "    common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "    if len(common_stations) < 2:\n",
    "        print(f'Cluster {c}: too few stations with allocations ({len(common_stations)}), skipping')\n",
    "        continue\n",
    "\n",
    "    # build station order and Tij matrix accordingly\n",
    "    Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "    # build station->poi allocation matrix A (nS x nP_used)\n",
    "    # find all POI indices used by these stations\n",
    "    poi_indices_used = sorted({pidx for s in common_stations for pidx in allocations[s].keys()})\n",
    "    if len(poi_indices_used) == 0:\n",
    "        print(f'Cluster {c}: no POIs assigned to these stations, skipping')\n",
    "        continue\n",
    "\n",
    "    nS = len(common_stations)\n",
    "    nP = len(poi_indices_used)\n",
    "    A = np.zeros((nS, nP), dtype=float)\n",
    "    station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "    poi_to_col = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "\n",
    "    for s in common_stations:\n",
    "        r = station_to_row[s]\n",
    "        for pidx, w in allocations[s].items():\n",
    "            if pidx in poi_to_col:\n",
    "                A[r, poi_to_col[pidx]] = w\n",
    "\n",
    "    # compute POI OD: P = A^T * Tij * A\n",
    "    P = A.T.dot(Tij).dot(A)\n",
    "\n",
    "    # build DataFrame: rows/cols labelled by poi_id (not global pois index)\n",
    "    poi_ids = [int(pois.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "    poi_od_df = pd.DataFrame(P, index=poi_ids, columns=poi_ids)\n",
    "    poi_od_per_cluster[c] = poi_od_df\n",
    "    poi_od_df.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}.csv')\n",
    "    print(f'Cluster {c}: saved POI OD with shape {poi_od_df.shape} (n_pois={len(poi_ids)})')\n",
    "\n",
    "# --- aggregate across clusters (align on poi_id union) ---\n",
    "if poi_od_per_cluster:\n",
    "    all_poi_ids = sorted({pid for df in poi_od_per_cluster.values() for pid in df.index.tolist()})\n",
    "    agg_mat = pd.DataFrame(0.0, index=all_poi_ids, columns=all_poi_ids)\n",
    "    for c, df in poi_od_per_cluster.items():\n",
    "        agg_mat = agg_mat.add(df.reindex(index=all_poi_ids, columns=all_poi_ids, fill_value=0), fill_value=0)\n",
    "    agg_mat.to_csv(OUTPUT_DIR / 'poi_od_aggregated_all_clusters.csv')\n",
    "    print('Saved aggregated POI OD for all clusters to', OUTPUT_DIR / 'poi_od_aggregated_all_clusters.csv')\n",
    "else:\n",
    "    print('No POI OD outputs (no clusters had usable data).')\n",
    "\n",
    "print('Done. Outputs in', OUTPUT_DIR)\n"
   ],
   "id": "69ebab7b73aa6ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved station->POI allocation table to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\station_poi_allocations.csv\n",
      "Cluster 0: saved POI OD with shape (369, 369) (n_pois=369)\n",
      "Cluster 1: saved POI OD with shape (357, 357) (n_pois=357)\n",
      "Cluster 2: saved POI OD with shape (355, 355) (n_pois=355)\n",
      "Cluster 3: saved POI OD with shape (363, 363) (n_pois=363)\n",
      "Saved aggregated POI OD for all clusters to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\poi_od_aggregated_all_clusters.csv\n",
      "Done. Outputs in E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T18:49:43.151053Z",
     "start_time": "2025-11-02T18:48:17.332165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# poi_od_sparse_with_baseline.py\n",
    "#\n",
    "# Memory-efficient conversion of predicted station->station OD (per cluster)\n",
    "# into POI->POI OD using sparse matrices and a \"baseline-to-k-nearest\" strategy\n",
    "# for neglected POIs so we avoid creating a dense 33k x 33k matrix.\n",
    "#\n",
    "# Requirements: scipy, scikit-learn, pandas, numpy\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3                # used for station->poi allocation\n",
    "K_BASELINE_NEIGH = 10          # number of POIs to distribute baseline for neglected POI\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "CATEGORY_WEIGHTS = {\n",
    "    'library': 1.1,\n",
    "    'school': 1.3,\n",
    "    'university': 1.5,\n",
    "    'residential': 1.0,\n",
    "    'commercial': 1.2,\n",
    "    'hospital': 1.4\n",
    "}\n",
    "BASELINE_RATIO = 0.001   # baseline magnitude relative to mean observed cell\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# sanity: required objects\n",
    "try:\n",
    "    predicted_matrices\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`predicted_matrices` not in memory. Run gravity step first.\")\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`combined` not in memory. Load combined dataframe first.\")\n",
    "\n",
    "# load POIs\n",
    "pois = pd.read_csv(POI_FILE)\n",
    "pois = pois.dropna(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['category'] = pois.get('category', '').astype(str).fillna('commercial').str.lower().str.strip()\n",
    "pois['weight'] = pois['category'].map(CATEGORY_WEIGHTS).fillna(1.0)\n",
    "\n",
    "# canonical lists and lookups\n",
    "all_poi_ids = pois['poi_id'].astype(int).tolist()\n",
    "poi_index_to_id = {i: int(pois.at[i, 'poi_id']) for i in pois.index}\n",
    "poi_id_to_index = {int(pois.at[i, 'poi_id']): i for i in pois.index}\n",
    "\n",
    "n_all = len(all_poi_ids)\n",
    "print(f\"POIs loaded: {n_all} (will keep results sparse)\")\n",
    "\n",
    "# build station coords as median of start/end\n",
    "lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "coord_parts = []\n",
    "if start_lon and start_lat:\n",
    "    tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "    tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "    coord_parts.append(tmp)\n",
    "if end_lon and end_lat:\n",
    "    tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "    tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "    coord_parts.append(tmp2)\n",
    "if not coord_parts:\n",
    "    raise RuntimeError('No station lon/lat columns found in combined')\n",
    "stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "stations_df.index = stations_df.index.astype(int)\n",
    "\n",
    "# restrict to stations that appear in at least one predicted matrix\n",
    "pred_station_ids = set()\n",
    "for m in predicted_matrices.values():\n",
    "    pred_station_ids.update([int(x) for x in m.index.astype(int).tolist()])\n",
    "stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "station_ids = stations_df.index.astype(int).tolist()\n",
    "print(f\"Using {len(station_ids)} stations for allocation\")\n",
    "\n",
    "# precompute station->nearest POIs (small K)\n",
    "poi_coords = pois[['lat','lon']].to_numpy()\n",
    "station_coords = stations_df[['lat','lon']].to_numpy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "nbrs.fit(np.radians(poi_coords))\n",
    "d_radians, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "dists_m = d_radians * 6371000.0\n",
    "\n",
    "# build allocations mapping station_id -> list of (poi_index, weight)\n",
    "allocations = {}\n",
    "for i, sid in enumerate(station_ids):\n",
    "    dists = dists_m[i]\n",
    "    idx = idxs[i]\n",
    "    mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "    if mask_close.any():\n",
    "        chosen_idx = idx[mask_close]\n",
    "        chosen_dists = dists[mask_close]\n",
    "    else:\n",
    "        chosen_idx = idx\n",
    "        chosen_dists = dists\n",
    "    cat_weights = pois.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "    inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "    raw = cat_weights * inv_dist\n",
    "    if raw.sum() <= 0:\n",
    "        w = np.ones_like(raw) / len(raw)\n",
    "    else:\n",
    "        w = raw / raw.sum()\n",
    "    allocations[int(sid)] = list(zip([int(p) for p in chosen_idx.tolist()], [float(x) for x in w.tolist()]))\n",
    "\n",
    "# Save allocations summary\n",
    "alloc_rows = []\n",
    "for s, lst in allocations.items():\n",
    "    for pidx, w in lst:\n",
    "        alloc_rows.append({'station_id': s, 'poi_index': int(pidx), 'poi_id': int(pois.at[pidx,'poi_id']), 'weight': w})\n",
    "alloc_df = pd.DataFrame(alloc_rows)\n",
    "alloc_df.to_csv(OUTPUT_DIR / 'station_poi_allocations.csv', index=False)\n",
    "print('Saved station->POI allocations (small)')\n",
    "\n",
    "# Build POI neighbor index for baseline distribution (k baseline neighbors on POI network)\n",
    "poi_nbrs = NearestNeighbors(n_neighbors=K_BASELINE_NEIGH, metric='haversine')\n",
    "poi_nbrs.fit(np.radians(poi_coords))\n",
    "poi_d_rad, poi_idxs = poi_nbrs.kneighbors(np.radians(poi_coords))\n",
    "poi_d_m = poi_d_rad * 6371000.0\n",
    "\n",
    "# helper: function to add entries to sparse accumulator lists\n",
    "\n",
    "def append_entries(rows_list, cols_list, data_list, from_poi_ids, to_poi_ids, values_matrix):\n",
    "    # from_poi_ids and to_poi_ids are lists of global poi_id labels (not dataframe indices)\n",
    "    for i, pid in enumerate(from_poi_ids):\n",
    "        for j, qid in enumerate(to_poi_ids):\n",
    "            val = values_matrix[i, j]\n",
    "            if val != 0 and not np.isnan(val):\n",
    "                rows_list.append(poi_id_to_index[pid])\n",
    "                cols_list.append(poi_id_to_index[qid])\n",
    "                data_list.append(float(val))\n",
    "\n",
    "# main conversion loop: produce sparse COO components per cluster\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "poi_od_sparse_per_cluster = {}\n",
    "for c, pred_df in predicted_matrices.items():\n",
    "    print(f'Processing cluster {c}...')\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df.index = pred_df.index.astype(int)\n",
    "    pred_df.columns = pred_df.columns.astype(int)\n",
    "\n",
    "    # stations in this predicted matrix that have allocations\n",
    "    common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "    if len(common_stations) < 2:\n",
    "        print(f' Cluster {c}: too few stations with allocations, skipping')\n",
    "        continue\n",
    "\n",
    "    Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "    # build A_used (nS x nP_used) where nP_used is number of distinct POIs assigned to these stations\n",
    "    poi_indices_used = sorted({pidx for s in common_stations for pidx, _ in allocations[s]})\n",
    "    if len(poi_indices_used) == 0:\n",
    "        print(f' Cluster {c}: no POIs used by these stations, skipping')\n",
    "        continue\n",
    "\n",
    "    station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "    poi_to_col_used = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "    nS = len(common_stations); nP_used = len(poi_indices_used)\n",
    "    A_used = np.zeros((nS, nP_used), dtype=float)\n",
    "    for s in common_stations:\n",
    "        r = station_to_row[s]\n",
    "        for pidx, w in allocations[s]:\n",
    "            if pidx in poi_to_col_used:\n",
    "                A_used[r, poi_to_col_used[pidx]] = w\n",
    "\n",
    "    # compute P_used (nP_used x nP_used) — this is small (few hundreds)\n",
    "    P_used = A_used.T.dot(Tij).dot(A_used)\n",
    "\n",
    "    # Now map P_used into global sparse lists\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    # map used POI local -> global poi_id\n",
    "    poi_ids_used = [int(pois.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "\n",
    "    # append P_used block into sparse lists\n",
    "    for i_local, global_pidx in enumerate(poi_indices_used):\n",
    "        pid = int(pois.at[global_pidx, 'poi_id'])\n",
    "        for j_local, global_qidx in enumerate(poi_indices_used):\n",
    "            qid = int(pois.at[global_qidx, 'poi_id'])\n",
    "            val = P_used[i_local, j_local]\n",
    "            if val != 0 and not np.isnan(val):\n",
    "                rows.append(poi_id_to_index[pid])\n",
    "                cols.append(poi_id_to_index[qid])\n",
    "                data.append(float(val))\n",
    "\n",
    "    # compute baseline per-cell scalar (mean observed cell over used block)\n",
    "    observed_sum = P_used.sum()\n",
    "    if observed_sum <= 0:\n",
    "        mean_cell = 0.0\n",
    "    else:\n",
    "        mean_cell = observed_sum / (n_all * n_all)  # normalized to full universe\n",
    "\n",
    "    mean_weight = pois['weight'].mean()\n",
    "\n",
    "    # identify neglected POIs (those that have no entries in current sparse lists)\n",
    "    used_poi_set = set(poi_ids_used)\n",
    "    all_poi_set = set(all_poi_ids)\n",
    "    neglected = sorted(list(all_poi_set - used_poi_set))\n",
    "    print(f' Cluster {c}: used POIs={len(used_poi_set)}, neglected POIs={len(neglected)}')\n",
    "\n",
    "    # For each neglected POI, distribute baseline to its K_BASELINE_NEIGH nearest POIs (by index in pois)\n",
    "    if len(neglected) > 0 and mean_cell > 0:\n",
    "        # we have precomputed poi_idxs and poi_d_m arrays (POI->neighbors)\n",
    "        for pid in neglected:\n",
    "            pidx = poi_id_to_index[pid]\n",
    "            # neighbors indices (including itself) from poi_idxs array\n",
    "            neigh_local = poi_idxs[pidx, 1:K_BASELINE_NEIGH+1]  # skip self (first neighbor)\n",
    "            neigh_d = poi_d_m[pidx, 1:K_BASELINE_NEIGH+1]\n",
    "            # weights: inverse distance * category weight\n",
    "            neigh_weights = pois.loc[neigh_local, 'weight'].to_numpy(dtype=float)\n",
    "            invd = 1.0 / (neigh_d + EPS)\n",
    "            raw = neigh_weights * invd\n",
    "            if raw.sum() <= 0:\n",
    "                wnorm = np.ones_like(raw) / len(raw)\n",
    "            else:\n",
    "                wnorm = raw / raw.sum()\n",
    "            # baseline total per neglected POI (outflow) distribute across neighbors\n",
    "            baseline_total = mean_cell * BASELINE_RATIO * (pois.at[pidx, 'weight'] / mean_weight)\n",
    "            # distribute baseline_total to outgoing links pid -> neigh\n",
    "            for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                val = baseline_total * wnorm[k_idx]\n",
    "                rows.append(pidx)\n",
    "                cols.append(neigh_idx)\n",
    "                data.append(float(val))\n",
    "            # similarly distribute baseline inflow: neighbors -> pid\n",
    "            for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                val = baseline_total * wnorm[k_idx]\n",
    "                rows.append(neigh_idx)\n",
    "                cols.append(pidx)\n",
    "                data.append(float(val))\n",
    "\n",
    "    # build sparse COO and save\n",
    "    coo = coo_matrix((data, (rows, cols)), shape=(n_all, n_all))\n",
    "    # compress to csr for smaller memory on disk\n",
    "    csr = coo.tocsr()\n",
    "    save_npz(OUTPUT_DIR / f'poi_od_cluster_{c}.npz', csr)\n",
    "\n",
    "    # also save edge-list CSV (only nonzero entries)\n",
    "    coo_nz = coo.tocoo()\n",
    "    edge_df = pd.DataFrame({'from_poi_index': coo_nz.row, 'to_poi_index': coo_nz.col, 'flow': coo_nz.data})\n",
    "    # map poi_index -> poi_id for readability\n",
    "    edge_df['from_poi_id'] = edge_df['from_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_df['to_poi_id'] = edge_df['to_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_df = edge_df[['from_poi_id','to_poi_id','flow']]\n",
    "    edge_df.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}_edgelists.csv', index=False)\n",
    "\n",
    "    poi_od_sparse_per_cluster[c] = {'sparse': csr, 'edge_csv': OUTPUT_DIR / f'poi_od_cluster_{c}_edgelist.csv'}\n",
    "    print(f' Cluster {c}: saved sparse POI OD (nnz={csr.nnz})')\n",
    "\n",
    "# aggregate across clusters (sum sparse matrices)\n",
    "if poi_od_sparse_per_cluster:\n",
    "    first = True\n",
    "    agg_csr = None\n",
    "    for c, info in poi_od_sparse_per_cluster.items():\n",
    "        mat = info['sparse']\n",
    "        if first:\n",
    "            agg_csr = mat.copy()\n",
    "            first = False\n",
    "        else:\n",
    "            agg_csr = agg_csr + mat\n",
    "    # save aggregated\n",
    "    save_npz(OUTPUT_DIR / 'poi_od_aggregated_all_clusters_sparse.npz', agg_csr)\n",
    "    # also save aggregated edge list (may still be big) — write only top flows if desired\n",
    "    coo_agg = agg_csr.tocoo()\n",
    "    edge_agg = pd.DataFrame({'from_idx': coo_agg.row, 'to_idx': coo_agg.col, 'flow': coo_agg.data})\n",
    "    edge_agg['from_poi_id'] = edge_agg['from_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_agg['to_poi_id'] = edge_agg['to_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    \n",
    "    edge_agg[['from_poi_id','to_poi_id','flow']].to_csv(OUTPUT_DIR / 'poi_od_aggregated_all_clusters_edgelists.csv', index=False)\n",
    "    print('Saved aggregated sparse POI OD and edge list')\n",
    "\n",
    "print('Done. Outputs (sparse .npz + edgelists) in', OUTPUT_DIR)\n"
   ],
   "id": "e67a8c942a632705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POIs loaded: 33669 (will keep results sparse)\n",
      "Using 194 stations for allocation\n",
      "Saved station->POI allocations (small)\n",
      "Processing cluster 0...\n",
      " Cluster 0: used POIs=369, neglected POIs=33300\n",
      " Cluster 0: saved sparse POI OD (nnz=494027)\n",
      "Processing cluster 1...\n",
      " Cluster 1: used POIs=357, neglected POIs=33312\n",
      " Cluster 1: saved sparse POI OD (nnz=485399)\n",
      "Processing cluster 2...\n",
      " Cluster 2: used POIs=355, neglected POIs=33314\n",
      " Cluster 2: saved sparse POI OD (nnz=484015)\n",
      "Processing cluster 3...\n",
      " Cluster 3: used POIs=363, neglected POIs=33306\n",
      " Cluster 3: saved sparse POI OD (nnz=489681)\n",
      "Saved aggregated sparse POI OD and edge list\n",
      "Done. Outputs (sparse .npz + edgelists) in E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-02T18:41:54.341072Z",
     "start_time": "2025-11-02T18:41:34.507041Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8509e663bfdfaf01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union of station IDs referenced in predicted_matrices: 194 stations\n",
      "Stations with station->POI allocation: 194 stations\n",
      "Stations in predictions but without allocations (missing coords?): 0 sample: []\n",
      "Stations allocated but not present in predicted_matrices: 0 sample: []\n",
      "Union of POIs present in any per-cluster edgelist: 33669 POIs\n",
      "Union size 33669 > 8000 — too large to produce a dense matrix safely.\n",
      "Option: produce dense CSV for top-N POIs by total flow instead. Using top 1000 by default.\n",
      "Building dense POI matrix for top 1000 POIs (by activity)\n",
      "Saved dense POI matrix CSV subset to E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\\poi_od_dense_subset.csv\n",
      "Saved combined long edgelist to E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\\poi_od_all_clusters_combined_edgelist.csv\n",
      "\n",
      "Diagnostics summary:\n",
      "num_predicted_union_stations      194\n",
      "num_allocated_stations            194\n",
      "num_missing_coord_stations          0\n",
      "num_used_pois_union             33669\n",
      "dense_subset_size                1000\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
