{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:36:05.948102Z",
     "start_time": "2025-10-28T23:36:05.934143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys"
   ],
   "id": "81268aaa4cb40cd2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:36:45.339486Z",
     "start_time": "2025-10-28T23:36:45.086895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- USER CONFIG -----\n",
    "# Directory where your 36 CSV files live\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data')  # <- updated path\n",
    "# Output filename for the combined CSV\n",
    "OUTPUT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "# Pattern to match files (will match filenames containing YYYY_MM or YYYY-MM)\n",
    "GLOB_PATTERN = '*_counts*.csv'\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "def extract_year_month_from_name(fname: str):\n",
    "    \"\"\"Return (year, month) tuple if found in filename, else None.\"\"\"\n",
    "    # look for 4-digit year, separator (_ or -), 2-digit month\n",
    "    m = re.search(r'(\\d{4})[_-](\\d{2})', fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, month = m.group(1), m.group(2)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def main(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(f'No files found matching pattern {GLOB_PATTERN} in {input_dir.resolve()}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for f in files:\n",
    "        ym = extract_year_month_from_name(f.name)\n",
    "        if ym is None:\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        year, month = ym\n",
    "        # read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {f.name}: {e}')\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        # add date columns in two common formats:\n",
    "        # 'year_month' = 'YYYY-MM' and 'month_year' = 'MM/YYYY' (user asked for m/y)\n",
    "        df['year_month'] = f\"{year}-{month}\"\n",
    "        df['month_year'] = f\"{month}/{year}\"\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print('No CSVs successfully read (maybe filename pattern is different). Files skipped:')\n",
    "        print('\\n'.join(skipped))\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Optional: reorder so date columns are near the front\n",
    "    cols = list(combined.columns)\n",
    "    for col in ['year_month', 'month_year']:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # Save the combined CSV\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f'Combined {len(dfs)} files into {output_file} (total rows: {len(combined)})')\n",
    "    if skipped:\n",
    "        print('Skipped files (no YYYY_MM found or read error):')\n",
    "        print('\\n'.join(skipped))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(INPUT_DIR, OUTPUT_FILE)\n"
   ],
   "id": "7dc3b5da354ab4bc",
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'E:\\\\Uni_PGT\\\\counts-data\\\\combined_od_with_date.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPermissionError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 75\u001B[39m\n\u001B[32m     71\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m.join(skipped))\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[34m__name__\u001B[39m == \u001B[33m'\u001B[39m\u001B[33m__main__\u001B[39m\u001B[33m'\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m75\u001B[39m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mINPUT_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mOUTPUT_FILE\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 67\u001B[39m, in \u001B[36mmain\u001B[39m\u001B[34m(input_dir, output_file)\u001B[39m\n\u001B[32m     64\u001B[39m combined = combined[cols]\n\u001B[32m     66\u001B[39m \u001B[38;5;66;03m# Save the combined CSV\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m \u001B[43mcombined\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mCombined \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(dfs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m files into \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m (total rows: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(combined)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m skipped:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Uni_PGT\\Xpress - MMCS\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Uni_PGT\\Xpress - MMCS\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001B[39m, in \u001B[36mNDFrame.to_csv\u001B[39m\u001B[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[39m\n\u001B[32m   3978\u001B[39m df = \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.to_frame()\n\u001B[32m   3980\u001B[39m formatter = DataFrameFormatter(\n\u001B[32m   3981\u001B[39m     frame=df,\n\u001B[32m   3982\u001B[39m     header=header,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3986\u001B[39m     decimal=decimal,\n\u001B[32m   3987\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3989\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3990\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3991\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3992\u001B[39m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[43m=\u001B[49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3993\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3994\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3995\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3996\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3997\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3998\u001B[39m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3999\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4000\u001B[39m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4001\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4002\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4003\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4004\u001B[39m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4005\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   4006\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Uni_PGT\\Xpress - MMCS\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[39m, in \u001B[36mDataFrameRenderer.to_csv\u001B[39m\u001B[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[39m\n\u001B[32m    993\u001B[39m     created_buffer = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    995\u001B[39m csv_formatter = CSVFormatter(\n\u001B[32m    996\u001B[39m     path_or_buf=path_or_buf,\n\u001B[32m    997\u001B[39m     lineterminator=lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1012\u001B[39m     formatter=\u001B[38;5;28mself\u001B[39m.fmt,\n\u001B[32m   1013\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m \u001B[43mcsv_formatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[32m   1017\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Uni_PGT\\Xpress - MMCS\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[39m, in \u001B[36mCSVFormatter.save\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    247\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    248\u001B[39m \u001B[33;03mCreate the writer & save.\u001B[39;00m\n\u001B[32m    249\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m251\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[32m    259\u001B[39m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[32m    260\u001B[39m     \u001B[38;5;28mself\u001B[39m.writer = csvlib.writer(\n\u001B[32m    261\u001B[39m         handles.handle,\n\u001B[32m    262\u001B[39m         lineterminator=\u001B[38;5;28mself\u001B[39m.lineterminator,\n\u001B[32m   (...)\u001B[39m\u001B[32m    267\u001B[39m         quotechar=\u001B[38;5;28mself\u001B[39m.quotechar,\n\u001B[32m    268\u001B[39m     )\n\u001B[32m    270\u001B[39m     \u001B[38;5;28mself\u001B[39m._save()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Uni_PGT\\Xpress - MMCS\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mPermissionError\u001B[39m: [Errno 13] Permission denied: 'E:\\\\Uni_PGT\\\\counts-data\\\\combined_od_with_date.csv'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:07.217429Z",
     "start_time": "2025-10-28T23:37:06.566975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "eb276d40c246370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268488 entries, 0 to 268487\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   month_year        268488 non-null  object \n",
      " 1   year_month        268488 non-null  object \n",
      " 2   start_station_id  268488 non-null  int64  \n",
      " 3   end_station_id    268488 non-null  int64  \n",
      " 4   hour              268488 non-null  float64\n",
      " 5   trip_count        268488 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 12.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       start_station_id  end_station_id           hour     trip_count\n",
       "count     268488.000000   268488.000000  268488.000000  268488.000000\n",
       "mean         930.490595      976.395388      13.844608       1.714162\n",
       "std          662.431144      671.219020       4.854848       1.803095\n",
       "min          171.000000      171.000000       0.000000       1.000000\n",
       "25%          260.000000      262.000000      11.000000       1.000000\n",
       "50%         1024.000000     1025.000000      14.000000       1.000000\n",
       "75%         1729.000000     1737.000000      17.000000       2.000000\n",
       "max         2268.000000     2268.000000      23.000000      84.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.490595</td>\n",
       "      <td>976.395388</td>\n",
       "      <td>13.844608</td>\n",
       "      <td>1.714162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>662.431144</td>\n",
       "      <td>671.219020</td>\n",
       "      <td>4.854848</td>\n",
       "      <td>1.803095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2268.000000</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:10.874919Z",
     "start_time": "2025-10-28T23:37:10.831444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REPORT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\data_quality_report.txt')\n",
    "# Ensure correct data types\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce').astype('Int64')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce').astype('Int64')\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['trip_count'] = pd.to_numeric(df['trip_count'], errors='coerce').astype('Int64')\n",
    "\n",
    "# ----- MISSING VALUES -----\n",
    "missing_summary = df.isna().sum()\n",
    "missing_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(f\"Found {len(missing_rows):,} rows with missing values — will drop them.\")\n",
    "    df = df.dropna()"
   ],
   "id": "c1a1ae76fbf985c3",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:13.399310Z",
     "start_time": "2025-10-28T23:37:13.276633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- DUPLICATES -----\n",
    "num_dupes = df.duplicated().sum()\n",
    "if num_dupes > 0:\n",
    "    print(f\"Dropping {num_dupes:,} duplicate rows.\")\n",
    "    df = df.drop_duplicates()"
   ],
   "id": "80502a5ea7e47868",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:15.884571Z",
     "start_time": "2025-10-28T23:37:15.863410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- VALUE VALIDATION -----\n",
    "# Check valid range for hour (0–23 expected)\n",
    "invalid_hours = df[~df['hour'].between(0, 24)]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours):,} rows with invalid hour values (outside 0–24). Fixing...\")\n",
    "    df = df[df['hour'].between(0, 24)]"
   ],
   "id": "211dc1e4af4c94a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:18.130063Z",
     "start_time": "2025-10-28T23:37:18.097969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check station ID ranges\n",
    "df_s=pd.read_csv(r'E:\\Uni_PGT\\station_data.csv')\n",
    "m=list(df_s['station_id'].unique())\n",
    "w=df['start_station_id'].unique()\n",
    "print(m)\n",
    "print(w)\n",
    "missing_ids = [s for s in w if s not in m]\n",
    "print(missing_ids)\n",
    "print(len(missing_ids))\n"
   ],
   "id": "29fbcd4e747abd69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(2268), np.int64(2265), np.int64(2263), np.int64(2259), np.int64(1824), np.int64(1823), np.int64(1822), np.int64(1821), np.int64(1820), np.int64(1819), np.int64(1818), np.int64(1815), np.int64(1814), np.int64(1813), np.int64(1809), np.int64(1807), np.int64(1798), np.int64(1770), np.int64(1769), np.int64(1768), np.int64(1767), np.int64(1765), np.int64(1763), np.int64(1758), np.int64(1757), np.int64(1756), np.int64(1754), np.int64(1753), np.int64(1749), np.int64(1748), np.int64(1745), np.int64(1744), np.int64(1739), np.int64(1738), np.int64(1737), np.int64(1730), np.int64(1729), np.int64(1728), np.int64(1727), np.int64(1726), np.int64(1725), np.int64(1722), np.int64(1721), np.int64(1720), np.int64(1102), np.int64(1098), np.int64(1097), np.int64(1096), np.int64(1093), np.int64(1092), np.int64(1091), np.int64(1090), np.int64(1052), np.int64(1051), np.int64(1050), np.int64(1039), np.int64(1038), np.int64(1028), np.int64(1025), np.int64(1024), np.int64(1019), np.int64(1017), np.int64(358), np.int64(349), np.int64(296), np.int64(289), np.int64(265), np.int64(264), np.int64(262), np.int64(260), np.int64(259), np.int64(258), np.int64(256), np.int64(254), np.int64(253), np.int64(252), np.int64(251), np.int64(250), np.int64(249), np.int64(248), np.int64(247), np.int64(246), np.int64(225), np.int64(189), np.int64(183)]\n",
      "<IntegerArray>\n",
      "[ 171,  183,  189,  225,  246,  247,  248,  249,  250,  251,\n",
      " ...\n",
      " 1869, 1870, 1871, 1874, 1877, 1860, 2259, 2263, 2265, 2268]\n",
      "Length: 196, dtype: Int64\n",
      "[np.int64(171), np.int64(255), np.int64(257), np.int64(261), np.int64(266), np.int64(273), np.int64(275), np.int64(277), np.int64(284), np.int64(285), np.int64(290), np.int64(297), np.int64(340), np.int64(341), np.int64(342), np.int64(343), np.int64(344), np.int64(345), np.int64(346), np.int64(347), np.int64(350), np.int64(351), np.int64(352), np.int64(353), np.int64(354), np.int64(355), np.int64(356), np.int64(357), np.int64(359), np.int64(365), np.int64(366), np.int64(648), np.int64(820), np.int64(860), np.int64(862), np.int64(863), np.int64(864), np.int64(865), np.int64(866), np.int64(867), np.int64(868), np.int64(869), np.int64(870), np.int64(871), np.int64(872), np.int64(873), np.int64(874), np.int64(875), np.int64(876), np.int64(877), np.int64(878), np.int64(879), np.int64(880), np.int64(881), np.int64(882), np.int64(885), np.int64(887), np.int64(888), np.int64(889), np.int64(883), np.int64(884), np.int64(890), np.int64(891), np.int64(901), np.int64(964), np.int64(965), np.int64(980), np.int64(981), np.int64(982), np.int64(991), np.int64(1018), np.int64(1026), np.int64(299), np.int64(1027), np.int64(1030), np.int64(1031), np.int64(1032), np.int64(1033), np.int64(1040), np.int64(1041), np.int64(1042), np.int64(1055), np.int64(1056), np.int64(1057), np.int64(1094), np.int64(1095), np.int64(1723), np.int64(1724), np.int64(1731), np.int64(1740), np.int64(1743), np.int64(1746), np.int64(1747), np.int64(1752), np.int64(1764), np.int64(1766), np.int64(1799), np.int64(1800), np.int64(1808), np.int64(1857), np.int64(1859), np.int64(1864), np.int64(1865), np.int64(1866), np.int64(1868), np.int64(1869), np.int64(1870), np.int64(1871), np.int64(1874), np.int64(1877), np.int64(1860)]\n",
      "111\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:22.147091Z",
     "start_time": "2025-10-28T23:37:21.980524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- REPORT -----\n",
    "with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('OD Matrix Data Quality Report\\n')\n",
    "    f.write('=' * 40 + '\\n\\n')\n",
    "    f.write(f'Total rows after cleaning: {len(df):,}\\n')\n",
    "    f.write(f'Duplicates removed: {num_dupes}\\n')\n",
    "    f.write(f'Missing rows removed: {len(missing_rows)}\\n')\n",
    "    f.write(f'Invalid hour rows removed: {len(invalid_hours)}\\n\\n')\n",
    "\n",
    "\n",
    "    f.write('Trip count summary (post-clean):\\n')\n",
    "    f.write(str(df['trip_count'].describe()) + '\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Data quality report saved to: {REPORT_FILE}')\n",
    "\n"
   ],
   "id": "c9a8a18a268937df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to: E:\\Uni_PGT\\counts-data\\data_quality_report.txt\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:54.841640Z",
     "start_time": "2025-10-28T23:37:24.618870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "generate_od_heatmaps.py\n",
    "\n",
    "Generates OD heatmap images for each month (36 files) plus one combined OD heatmap\n",
    "from the combined OD CSV created earlier.\n",
    "\n",
    "Notes / behavior:\n",
    " - The script reads 'combined_od_with_date.csv' and expects columns:\n",
    "   ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    " - It will order stations by numeric station id (ascending). If you have a station\n",
    "   reference file and want a specific ordering, set STATION_REF_FILE.\n",
    " - For visualization, the script uses np.log1p on counts to reduce skew. The saved\n",
    "   CSVs keep raw aggregated counts.\n",
    "\n",
    "Run:\n",
    "    python generate_od_heatmaps.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps')\n",
    "STATION_REF_FILE = Path(r'E:\\Uni_PGT\\station_data.csv')  # set to None to order by numeric id\n",
    "LOG_DISPLAY = True   # use log1p for display to reduce skew\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# If no station reference, build station list from data\n",
    "all_stations = np.union1d(df['start_station_id'].unique(), df['end_station_id'].unique()).astype(int)\n",
    "all_stations_sorted = sorted([int(x) for x in all_stations])\n",
    "n_stations = len(all_stations_sorted)\n",
    "stations = sorted([int(x) for x in all_stations])\n",
    "print(f'Total stations used for matrices: {n_stations}')\n",
    "\n",
    "# helper to make pivot and plot\n",
    "\n",
    "def make_pivot(df_subset, stations, aggcol='trip_count'):\n",
    "    # aggregate counts to ensure one cell per pair\n",
    "    agg = df_subset.groupby(['start_station_id', 'end_station_id'])[aggcol].sum().reset_index()\n",
    "    pivot = agg.pivot(index='start_station_id', columns='end_station_id', values=aggcol).reindex(index=stations, columns=stations).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "\n",
    "def plot_matrix(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    # dynamic figsize: cap sizes to avoid enormous images\n",
    "    height, width = arr.shape\n",
    "    figsize = (min(20, max(6, width/10)), min(20, max(6, height/10)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('end_station_id (ordered)')\n",
    "    ax.set_ylabel('start_station_id (ordered)')\n",
    "\n",
    "    # reduce tick labels for readability: show first, middle, last\n",
    "    if width <= 30:\n",
    "        xticks = range(width)\n",
    "        xtick_labels = [str(int(v)) for v in matrix_df.columns]\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xtick_labels, rotation=90, fontsize=6)\n",
    "    else:\n",
    "        ax.set_xticks([0, width//2, width-1])\n",
    "        ax.set_xticklabels([str(int(matrix_df.columns[0])), str(int(matrix_df.columns[width//2])), str(int(matrix_df.columns[-1]))], fontsize=8)\n",
    "\n",
    "    if height <= 30:\n",
    "        yticks = range(height)\n",
    "        ytick_labels = [str(int(v)) for v in matrix_df.index]\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(ytick_labels, fontsize=6)\n",
    "    else:\n",
    "        ax.set_yticks([0, height//2, height-1])\n",
    "        ax.set_yticklabels([str(int(matrix_df.index[0])), str(int(matrix_df.index[height//2])), str(int(matrix_df.index[-1]))], fontsize=8)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved heatmap: {outpath}')\n",
    "\n",
    "\n",
    "# 1) Per-month heatmaps\n",
    "unique_months = sorted(df['year_month'].unique())\n",
    "print(f'Found {len(unique_months)} unique months (expected 36): {unique_months}')\n",
    "\n",
    "for ym in unique_months:\n",
    "    df_month = df[df['year_month'] == ym]\n",
    "    pivot = make_pivot(df_month, stations)\n",
    "    outpath = OUTPUT_DIR / f'heatmap_{ym}.png'\n",
    "    title = f'OD heatmap {ym} (log display={LOG_DISPLAY})'\n",
    "    plot_matrix(pivot, title, outpath)\n",
    "\n",
    "# 2) Combined heatmap for all data\n",
    "pivot_combined = make_pivot(df, stations)\n",
    "# save numeric combined matrix\n",
    "pivot_combined.to_csv(OUTPUT_DIR / 'od_matrix_combined.csv')\n",
    "plot_matrix(pivot_combined, 'OD heatmap combined (log display={})'.format(LOG_DISPLAY), OUTPUT_DIR / 'heatmap_combined.png')\n",
    "\n",
    "print('All done. Generated per-month and combined heatmaps in:')\n",
    "print(OUTPUT_DIR.resolve())"
   ],
   "id": "4d87a987f75f2586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations used for matrices: 198\n",
      "Found 36 unique months (expected 36): ['2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09']\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_combined.png\n",
      "All done. Generated per-month and combined heatmaps in:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:37:58.947044Z",
     "start_time": "2025-10-28T23:37:56.785793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "seasonal_hourly_heatmaps.py\n",
    "\n",
    "Generates seasonal-hourly OD trip heatmaps:\n",
    " - One heatmap per year (season x hour matrix) for each year in the data\n",
    " - One combined heatmap over all years\n",
    " - Saves matrices as CSV and images to OUTPUT_DIR\n",
    "\n",
    "Seasons used (Northern Hemisphere standard):\n",
    " - Winter: Dec, Jan, Feb (DJF)\n",
    " - Spring: Mar, Apr, May (MAM)\n",
    " - Summer: Jun, Jul, Aug (JJA)\n",
    " - Autumn: Sep, Oct, Nov (SON)\n",
    "\n",
    "Expect input columns: ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    "\n",
    "Run:\n",
    "    python seasonal_hourly_heatmaps.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps_seasonal')\n",
    "DPI = 150\n",
    "LOG_DISPLAY = False  # For seasonal-hour heatmaps we keep linear counts by default\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_year_month(ym):\n",
    "    if isinstance(ym, str):\n",
    "        if '-' in ym:\n",
    "            parts = ym.split('-')\n",
    "            return int(parts[0]), int(parts[1])\n",
    "        if '/' in ym:\n",
    "            parts = ym.split('/')\n",
    "            # assume MM/YYYY -> return (YYYY, MM)\n",
    "            return int(parts[1]), int(parts[0])\n",
    "    raise ValueError(f'Unrecognized year_month format: {ym}')\n",
    "\n",
    "parsed = df['year_month'].apply(parse_year_month)\n",
    "df['year'] = parsed.apply(lambda x: x[0])\n",
    "df['month'] = parsed.apply(lambda x: x[1])\n",
    "\n",
    "# season mapping\n",
    "season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "              3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "              6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "              9: 'Autumn', 10: 'Autumn', 11: 'Autumn'}\n",
    "\n",
    "df['season'] = df['month'].map(season_map)\n",
    "\n",
    "# Helper: build season-hour pivot for a given dataframe\n",
    "\n",
    "def season_hour_pivot(df_subset):\n",
    "    # aggregate trip counts by season and hour\n",
    "    agg = df_subset.groupby(['season', 'hour'])['trip_count'].sum().reset_index()\n",
    "    # ensure all seasons and hours 0-23 present\n",
    "    seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    hours = list(range(24))\n",
    "    pivot = agg.pivot(index='season', columns='hour', values='trip_count').reindex(index=seasons, columns=hours).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "# plotting helper\n",
    "\n",
    "def plot_season_hour(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Hour of day')\n",
    "    ax.set_ylabel('Season')\n",
    "\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "    ax.set_xticklabels([str(h) for h in range(0, 24, 2)])\n",
    "\n",
    "    ax.set_yticks(range(len(matrix_df.index)))\n",
    "    ax.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved: {outpath}')\n",
    "\n",
    "# 1) Per-year seasonal-hour heatmaps\n",
    "years = sorted(df['year'].unique())\n",
    "print(f'Found years: {years}')\n",
    "\n",
    "for y in years:\n",
    "    df_year = df[df['year'] == y]\n",
    "    pivot = season_hour_pivot(df_year)\n",
    "    csv_out = OUTPUT_DIR / f'season_hour_matrix_{y}.csv'\n",
    "    img_out = OUTPUT_DIR / f'season_hour_heatmap_{y}.png'\n",
    "    pivot.to_csv(csv_out)\n",
    "    plot_season_hour(pivot, f'Season vs Hour - {y}', img_out)\n",
    "\n",
    "# 2) Combined (all years)\n",
    "pivot_all = season_hour_pivot(df)\n",
    "pivot_all.to_csv(OUTPUT_DIR / 'season_hour_matrix_all_years.csv')\n",
    "plot_season_hour(pivot_all, 'Season vs Hour - All years', OUTPUT_DIR / 'season_hour_heatmap_all_years.png')\n",
    "\n",
    "print('Done. Results saved to:')\n",
    "print(OUTPUT_DIR.resolve())\n"
   ],
   "id": "160179ad6a1012bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found years: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021)]\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2018.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2019.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2020.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2021.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_all_years.png\n",
      "Done. Results saved to:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:38:13.106598Z",
     "start_time": "2025-10-28T23:38:04.590376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "combine_36_csvs_no_date.py\n",
    "\n",
    "Reads all CSV files from cyclehire-cleandata named like 2018_10.csv ... 2021_09.csv\n",
    "(or containing that yyyy_mm pattern in the filename), concatenates them into a single CSV\n",
    "and saves it to cyclehire-cleandata\\combined_all_periods.csv\n",
    "\n",
    "Notes:\n",
    " - This script does NOT add a date column (as requested).\n",
    " - It will align columns by name; missing columns in some files will be filled with NaN.\n",
    " - It prints a short summary of files read and total rows combined.\n",
    "\n",
    "Usage:\n",
    "    python combine_36_csvs_no_date.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata')\n",
    "OUTPUT_FILE = INPUT_DIR / 'combined_all_periods.csv'\n",
    "GLOB_PATTERN = '*_*.csv'  # matches files with yyyy_mm in name like 2018_10.csv\n",
    "FILE_FILTER_REGEX = re.compile(r'(20\\d{2})[_-](0[1-9]|1[0-2])')  # restrict to yyyy_mm patterns\n",
    "# ------------------------\n",
    "\n",
    "# collect candidate files\n",
    "files = sorted(INPUT_DIR.glob(GLOB_PATTERN))\n",
    "selected_files = [f for f in files if FILE_FILTER_REGEX.search(f.name)]\n",
    "\n",
    "if not selected_files:\n",
    "    raise FileNotFoundError(f'No files matching yyyy_mm pattern found in {INPUT_DIR}')\n",
    "\n",
    "print(f'Found {len(selected_files)} files to combine:')\n",
    "for f in selected_files:\n",
    "    print(' -', f.name)\n",
    "\n",
    "# read and concatenate\n",
    "dfs = []\n",
    "for f in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name  # optional: keep which file the row came from\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to read {f.name}: {e}')\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError('No files were successfully read.')\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "combined.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f'Combined {len(dfs)} files into {OUTPUT_FILE} (total rows: {len(combined):,})')\n",
    "\n",
    "# optional quick sanity print\n",
    "print('\\nColumn summary (name : non-null count):')\n",
    "print(combined.notna().sum().sort_values(ascending=False).head(50))\n",
    "\n",
    "print('\\nDone.')\n"
   ],
   "id": "5973531e49f04c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_1496\\2259740896.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 files to combine:\n",
      " - 2018_10.csv\n",
      " - 2018_11.csv\n",
      " - 2018_12.csv\n",
      " - 2019_01.csv\n",
      " - 2019_02.csv\n",
      " - 2019_03.csv\n",
      " - 2019_04.csv\n",
      " - 2019_05.csv\n",
      " - 2019_06.csv\n",
      " - 2019_07.csv\n",
      " - 2019_08.csv\n",
      " - 2019_09.csv\n",
      " - 2019_10.csv\n",
      " - 2019_11.csv\n",
      " - 2019_12.csv\n",
      " - 2020_01.csv\n",
      " - 2020_02.csv\n",
      " - 2020_03.csv\n",
      " - 2020_04.csv\n",
      " - 2020_05.csv\n",
      " - 2020_06.csv\n",
      " - 2020_07.csv\n",
      " - 2020_08.csv\n",
      " - 2020_09.csv\n",
      " - 2020_10.csv\n",
      " - 2020_11.csv\n",
      " - 2020_12.csv\n",
      " - 2021_01.csv\n",
      " - 2021_02.csv\n",
      " - 2021_03.csv\n",
      " - 2021_04.csv\n",
      " - 2021_05.csv\n",
      " - 2021_06.csv\n",
      " - 2021_07.csv\n",
      " - 2021_08.csv\n",
      " - 2021_09.csv\n",
      "Combined 36 files into E:\\Uni_PGT\\cyclehire-cleandata\\combined_all_periods.csv (total rows: 460,655)\n",
      "\n",
      "Column summary (name : non-null count):\n",
      "started_at                   460655\n",
      "ended_at                     460655\n",
      "duration                     460655\n",
      "start_station_id             460655\n",
      "start_station_name           460655\n",
      "start_station_latitude       460655\n",
      "end_station_latitude         460655\n",
      "start_station_longitude      460655\n",
      "end_station_id               460655\n",
      "end_station_name             460655\n",
      "__source_file                460655\n",
      "end_station_longitude        460655\n",
      "start_station_description    456167\n",
      "end_station_description      455560\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:38:13.778370Z",
     "start_time": "2025-10-28T23:38:13.106598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(combined.info())\n",
    "print(combined.describe())"
   ],
   "id": "3e84a0d30fe9b22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460655 entries, 0 to 460654\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   started_at                 460655 non-null  object \n",
      " 1   ended_at                   460655 non-null  object \n",
      " 2   duration                   460655 non-null  int64  \n",
      " 3   start_station_id           460655 non-null  int64  \n",
      " 4   start_station_name         460655 non-null  object \n",
      " 5   start_station_description  456167 non-null  object \n",
      " 6   start_station_latitude     460655 non-null  float64\n",
      " 7   start_station_longitude    460655 non-null  float64\n",
      " 8   end_station_id             460655 non-null  int64  \n",
      " 9   end_station_name           460655 non-null  object \n",
      " 10  end_station_description    455560 non-null  object \n",
      " 11  end_station_latitude       460655 non-null  float64\n",
      " 12  end_station_longitude      460655 non-null  float64\n",
      " 13  __source_file              460655 non-null  object \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 49.2+ MB\n",
      "None\n",
      "           duration  start_station_id  start_station_latitude  \\\n",
      "count  4.606550e+05     460655.000000           460655.000000   \n",
      "mean   1.945961e+03        936.878545               55.950615   \n",
      "std    5.531311e+03        671.956842                0.013497   \n",
      "min    6.100000e+01        171.000000               55.908404   \n",
      "25%    6.280000e+02        261.000000               55.940238   \n",
      "50%    1.166000e+03       1024.000000               55.947600   \n",
      "75%    2.527000e+03       1729.000000               55.958566   \n",
      "max    2.363348e+06       2268.000000               55.992957   \n",
      "\n",
      "       start_station_longitude  end_station_id  end_station_latitude  \\\n",
      "count            460655.000000   460655.000000         460655.000000   \n",
      "mean                 -3.196412      983.399353             55.952535   \n",
      "std                   0.039072      678.334765              0.015748   \n",
      "min                  -3.407156      171.000000             53.395525   \n",
      "25%                  -3.207964      262.000000             55.941791   \n",
      "50%                  -3.192444     1025.000000             55.951501   \n",
      "75%                  -3.180693     1737.000000             55.962487   \n",
      "max                  -3.058307     2268.000000             55.992957   \n",
      "\n",
      "       end_station_longitude  \n",
      "count          460655.000000  \n",
      "mean               -3.195134  \n",
      "std                 0.041796  \n",
      "min                -3.407156  \n",
      "25%                -3.208070  \n",
      "50%                -3.191421  \n",
      "75%                -3.176351  \n",
      "max                -2.990138  \n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:38:23.802640Z",
     "start_time": "2025-10-28T23:38:20.060773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path where your data is stored\n",
    "data_path = r'E:\\Uni_PGT\\cyclehire-cleandata'\n",
    "\n",
    "# Convert started_at column to datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "\n",
    "# Extract weekday and hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Assign seasons based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "combined['season'] = combined['started_at'].dt.month.apply(get_season)\n",
    "\n",
    "# Order weekdays for consistent plotting\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create output folder for heatmaps\n",
    "output_dir = os.path.join(data_path, 'heatmaps_weekday_hour')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to plot and save heatmap\n",
    "def plot_heatmap(data, title, filename):\n",
    "    pivot_table = data.pivot_table(index='weekday', columns='hour', values='duration', aggfunc='count').fillna(0)\n",
    "    pivot_table = pivot_table.reindex(weekday_order)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(pivot_table, cmap='YlGnBu')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Weekday')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Generate heatmap for each season\n",
    "for season, data in combined.groupby('season'):\n",
    "    plot_heatmap(data, f'Trip Count by Hour and Weekday - {season}', f'heatmap_{season}.png')\n",
    "\n",
    "# Generate heatmap for all data combined\n",
    "plot_heatmap(combined, 'Trip Count by Hour and Weekday - All Data', 'heatmap_all_data.png')\n",
    "\n",
    "print(f\"Heatmaps saved in: {output_dir}\")"
   ],
   "id": "ffcd021ad0f256a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps saved in: E:\\Uni_PGT\\cyclehire-cleandata\\heatmaps_weekday_hour\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T23:49:02.481620Z",
     "start_time": "2025-10-28T23:48:59.373430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "duration_and_od_analysis.py\n",
    "\n",
    "Produces:\n",
    " - Histogram of trip durations (linear and log-scaled)\n",
    " - CSV and plot for average duration by hour of day\n",
    " - CSV and plot for average duration by weekday\n",
    " - OD heatmap (average duration per start_station_id x end_station_id)\n",
    "\n",
    "Assumptions:\n",
    " - Combined CSV of the 36 files exists at: cyclehire-cleandata\\combined_all_periods.csv\n",
    " - Columns include: started_at, ended_at, duration, start_station_id, end_station_id\n",
    "\n",
    "Outputs are written to cyclehire-cleandata\\analysis_outputs\n",
    "\n",
    "Run:\n",
    "    python duration_and_od_analysis.py\n",
    "\"\"\"\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs')\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Convert started_at to datetime (coerce errors)\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "# Filter out rows with missing datetime or non-positive duration\n",
    "initial_rows = len(combined)\n",
    "combined = combined[combined['duration'].notna()]\n",
    "combined = combined[combined['duration'] > 0]\n",
    "combined = combined[combined['started_at'].notna()]\n",
    "print(f'Kept {len(combined):,} rows (removed {initial_rows - len(combined):,} invalid rows)')\n",
    "\n",
    "# --- 1) Histograms of duration ---\n",
    "# Linear histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(combined['duration'], bins=100, range=(0, combined['duration'].quantile(0.99)))\n",
    "plt.title('Trip duration distribution (0-99th percentile)')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_linear.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Log-scaled histogram (log1p)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(np.log1p(combined['duration']), bins=100)\n",
    "plt.title('Trip duration distribution (log1p)')\n",
    "plt.xlabel('log1p(Duration)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_log.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Save basic stats\n",
    "desc = combined['duration'].describe()\n",
    "desc.to_csv(OUTPUT_DIR / 'duration_summary_stats.csv')\n",
    "\n",
    "# --- 2) Average duration by hour of day ---\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "avg_by_hour = combined.groupby('hour')['duration'].mean().reindex(range(24)).fillna(0)\n",
    "avg_by_hour.to_csv(OUTPUT_DIR / 'avg_duration_by_hour.csv')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(avg_by_hour.index, avg_by_hour.values, marker='o')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Average duration')\n",
    "plt.title('Average trip duration by hour of day')\n",
    "plt.xticks(range(0,24))\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_hour.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 3) Average duration by weekday ---\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "weekday_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "avg_by_weekday = combined.groupby('weekday')['duration'].mean().reindex(weekday_order)\n",
    "avg_by_weekday.to_csv(OUTPUT_DIR / 'avg_duration_by_weekday.csv')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(avg_by_weekday.index, avg_by_weekday.values)\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Average duration (seconds)')\n",
    "plt.title('Average trip duration by weekday')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_weekday.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 4) OD heatmap: average duration per (start_station_id x end_station_id) ---\n",
    "# To limit memory use, we will restrict to top N stations by activity, but also\n",
    "# save a CSV of aggregated averages for all pairs.\n",
    "\n",
    "# Aggregate per pair\n",
    "pair_agg = combined.groupby(['start_station_id','end_station_id'])['duration'].agg(['mean','count']).reset_index()\n",
    "pair_agg.rename(columns={'mean':'avg_duration','count':'trip_count'}, inplace=True)\n",
    "pair_agg.to_csv(OUTPUT_DIR / 'od_pair_avg_duration_all_pairs.csv', index=False)\n",
    "\n",
    "# Choose top stations by total trips (to make a manageable heatmap)\n",
    "station_activity = pd.concat([combined['start_station_id'], combined['end_station_id']]).value_counts()\n",
    "top_n = 100  # adjust if you want larger/smaller matrix\n",
    "top_stations = station_activity.index[:top_n].astype(int).tolist()\n",
    "print(f'Creating OD heatmap for top {len(top_stations)} stations by activity')\n",
    "\n",
    "# Pivot for top stations\n",
    "subset = pair_agg[pair_agg['start_station_id'].isin(top_stations) & pair_agg['end_station_id'].isin(top_stations)]\n",
    "heat = subset.pivot(index='start_station_id', columns='end_station_id', values='avg_duration').reindex(index=top_stations, columns=top_stations).fillna(0)\n",
    "\n",
    "# Plot heatmap (use log scale for color or linear depending on spread)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(np.log1p(heat.values), aspect='auto')\n",
    "plt.colorbar(label='log1p(avg_duration)')\n",
    "plt.title(f'OD average duration heatmap (top {len(top_stations)} stations)')\n",
    "plt.xlabel('end_station_id (ordered by activity)')\n",
    "plt.ylabel('start_station_id (ordered by activity)')\n",
    "# keep tick labels sparse for readability\n",
    "n = len(top_stations)\n",
    "plt.xticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.yticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'od_avg_duration_heatmap_top{top_n}.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "print('Analysis complete. Outputs saved to:', OUTPUT_DIR.resolve())\n"
   ],
   "id": "a1785be491b730e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_1496\\2767709071.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 460,232 rows (removed 423 invalid rows)\n",
      "Creating OD heatmap for top 100 stations by activity\n",
      "Analysis complete. Outputs saved to: E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
