{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:16.290374Z",
     "start_time": "2025-11-07T12:52:16.283323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys"
   ],
   "id": "81268aaa4cb40cd2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.224052Z",
     "start_time": "2025-11-07T12:52:16.293865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- USER CONFIG -----\n",
    "# Directory where your 36 CSV files live\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data')  # <- updated path\n",
    "# Output filename for the combined CSV\n",
    "OUTPUT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\combined_od_with_datew.csv')\n",
    "# Pattern to match files (will match filenames containing YYYY_MM or YYYY-MM)\n",
    "GLOB_PATTERN = '*_counts*.csv'\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "def extract_year_month_from_name(fname: str):\n",
    "    \n",
    "    \"\"\"Return (year, month) tuple if found in filename, else None.\"\"\"\n",
    "    # look for 4-digit year, separator (_ or -), 2-digit month\n",
    "    m = re.search(r'(\\d{4})[_-](\\d{2})', fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, month = m.group(1), m.group(2)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def main(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(f'No files found matching pattern {GLOB_PATTERN} in {input_dir.resolve()}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for f in files:\n",
    "        ym = extract_year_month_from_name(f.name)\n",
    "        if ym is None:\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        year, month = ym\n",
    "        # read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {f.name}: {e}')\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        # add date columns in two common formats:\n",
    "        # 'year_month' = 'YYYY-MM' and 'month_year' = 'MM/YYYY' (user asked for m/y)\n",
    "        df['year_month'] = f\"{year}-{month}\"\n",
    "        df['month_year'] = f\"{month}/{year}\"\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print('No CSVs successfully read (maybe filename pattern is different). Files skipped:')\n",
    "        print('\\n'.join(skipped))\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Optional: reorder so date columns are near the front\n",
    "    cols = list(combined.columns)\n",
    "    for col in ['year_month', 'month_year']:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # Save the combined CSV\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f'Combined {len(dfs)} files into {output_file} (total rows: {len(combined)})')\n",
    "    if skipped:\n",
    "        print('Skipped files (no YYYY_MM found or read error):')\n",
    "        print('\\n'.join(skipped))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(INPUT_DIR, OUTPUT_FILE)\n"
   ],
   "id": "7dc3b5da354ab4bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\counts-data\\combined_od_with_datew.csv (total rows: 268488)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.696363Z",
     "start_time": "2025-11-07T12:52:19.224052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "eb276d40c246370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268488 entries, 0 to 268487\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   month_year        268488 non-null  object \n",
      " 1   year_month        268488 non-null  object \n",
      " 2   start_station_id  268488 non-null  int64  \n",
      " 3   end_station_id    268488 non-null  int64  \n",
      " 4   hour              268488 non-null  float64\n",
      " 5   trip_count        268488 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 12.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       start_station_id  end_station_id           hour     trip_count\n",
       "count     268488.000000   268488.000000  268488.000000  268488.000000\n",
       "mean         930.490595      976.395388      13.844608       1.714162\n",
       "std          662.431144      671.219020       4.854848       1.803095\n",
       "min          171.000000      171.000000       0.000000       1.000000\n",
       "25%          260.000000      262.000000      11.000000       1.000000\n",
       "50%         1024.000000     1025.000000      14.000000       1.000000\n",
       "75%         1729.000000     1737.000000      17.000000       2.000000\n",
       "max         2268.000000     2268.000000      23.000000      84.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.490595</td>\n",
       "      <td>976.395388</td>\n",
       "      <td>13.844608</td>\n",
       "      <td>1.714162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>662.431144</td>\n",
       "      <td>671.219020</td>\n",
       "      <td>4.854848</td>\n",
       "      <td>1.803095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2268.000000</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.772717Z",
     "start_time": "2025-11-07T12:52:19.696363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REPORT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\data_quality_report.txt')\n",
    "# Ensure correct data types\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce').astype('Int64')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce').astype('Int64')\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['trip_count'] = pd.to_numeric(df['trip_count'], errors='coerce').astype('Int64')\n",
    "\n",
    "# ----- MISSING VALUES -----\n",
    "missing_summary = df.isna().sum()\n",
    "missing_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(f\"Found {len(missing_rows):,} rows with missing values — will drop them.\")\n",
    "    df = df.dropna()"
   ],
   "id": "c1a1ae76fbf985c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.866668Z",
     "start_time": "2025-11-07T12:52:19.772717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- DUPLICATES -----\n",
    "num_dupes = df.duplicated().sum()\n",
    "if num_dupes > 0:\n",
    "    print(f\"Dropping {num_dupes:,} duplicate rows.\")\n",
    "    df = df.drop_duplicates()"
   ],
   "id": "80502a5ea7e47868",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.874416Z",
     "start_time": "2025-11-07T12:52:19.866668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- VALUE VALIDATION -----\n",
    "# Check valid range for hour (0–23 expected)\n",
    "invalid_hours = df[~df['hour'].between(0, 24)]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours):,} rows with invalid hour values (outside 0–24). Fixing...\")\n",
    "    df = df[df['hour'].between(0, 24)]"
   ],
   "id": "211dc1e4af4c94a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:19.949166Z",
     "start_time": "2025-11-07T12:52:19.874416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check station ID ranges\n",
    "df_s=pd.read_csv(r'E:\\Uni_PGT\\station_data.csv')\n",
    "m=list(df_s['station_id'].unique())\n",
    "w=df['start_station_id'].unique()\n",
    "missing_ids = [s for s in w if s not in m]\n",
    "print(missing_ids)\n",
    "print(len(missing_ids))\n"
   ],
   "id": "29fbcd4e747abd69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(171), np.int64(255), np.int64(257), np.int64(261), np.int64(266), np.int64(273), np.int64(275), np.int64(277), np.int64(284), np.int64(285), np.int64(290), np.int64(297), np.int64(340), np.int64(341), np.int64(342), np.int64(343), np.int64(344), np.int64(345), np.int64(346), np.int64(347), np.int64(350), np.int64(351), np.int64(352), np.int64(353), np.int64(354), np.int64(355), np.int64(356), np.int64(357), np.int64(359), np.int64(365), np.int64(366), np.int64(648), np.int64(820), np.int64(860), np.int64(862), np.int64(863), np.int64(864), np.int64(865), np.int64(866), np.int64(867), np.int64(868), np.int64(869), np.int64(870), np.int64(871), np.int64(872), np.int64(873), np.int64(874), np.int64(875), np.int64(876), np.int64(877), np.int64(878), np.int64(879), np.int64(880), np.int64(881), np.int64(882), np.int64(885), np.int64(887), np.int64(888), np.int64(889), np.int64(883), np.int64(884), np.int64(890), np.int64(891), np.int64(901), np.int64(964), np.int64(965), np.int64(980), np.int64(981), np.int64(982), np.int64(991), np.int64(1018), np.int64(1026), np.int64(299), np.int64(1027), np.int64(1030), np.int64(1031), np.int64(1032), np.int64(1033), np.int64(1040), np.int64(1041), np.int64(1042), np.int64(1055), np.int64(1056), np.int64(1057), np.int64(1094), np.int64(1095), np.int64(1723), np.int64(1724), np.int64(1731), np.int64(1740), np.int64(1743), np.int64(1746), np.int64(1747), np.int64(1752), np.int64(1764), np.int64(1766), np.int64(1799), np.int64(1800), np.int64(1808), np.int64(1857), np.int64(1859), np.int64(1864), np.int64(1865), np.int64(1866), np.int64(1868), np.int64(1869), np.int64(1870), np.int64(1871), np.int64(1874), np.int64(1877), np.int64(1860)]\n",
      "111\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:52:20.014958Z",
     "start_time": "2025-11-07T12:52:19.949166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- REPORT -----\n",
    "with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('OD Matrix Data Quality Report\\n')\n",
    "    f.write('=' * 40 + '\\n\\n')\n",
    "    f.write(f'Total rows after cleaning: {len(df):,}\\n')\n",
    "    f.write(f'Duplicates removed: {num_dupes}\\n')\n",
    "    f.write(f'Missing rows removed: {len(missing_rows)}\\n')\n",
    "    f.write(f'Invalid hour rows removed: {len(invalid_hours)}\\n\\n')\n",
    "\n",
    "\n",
    "    f.write('Trip count summary (post-clean):\\n')\n",
    "    f.write(str(df['trip_count'].describe()) + '\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Data quality report saved to: {REPORT_FILE}')\n",
    "\n"
   ],
   "id": "c9a8a18a268937df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to: E:\\Uni_PGT\\counts-data\\data_quality_report.txt\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:14.770065Z",
     "start_time": "2025-11-07T12:52:20.014958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "generate_od_heatmaps.py\n",
    "\n",
    "Generates OD heatmap images for each month (36 files) plus one combined OD heatmap\n",
    "from the combined OD CSV created earlier.\n",
    "\n",
    "Notes / behavior:\n",
    " - The script reads 'combined_od_with_date.csv' and expects columns:\n",
    "   ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    " - It will order stations by numeric station id (ascending). If you have a station\n",
    "   reference file and want a specific ordering, set STATION_REF_FILE.\n",
    " - For visualization, the script uses np.log1p on counts to reduce skew. The saved\n",
    "   CSVs keep raw aggregated counts.\n",
    "\n",
    "Run:\n",
    "    python generate_od_heatmaps.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps')\n",
    "STATION_REF_FILE = Path(r'E:\\Uni_PGT\\station_data.csv')  # set to None to order by numeric id\n",
    "LOG_DISPLAY = True   # use log1p for display to reduce skew\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# If no station reference, build station list from data\n",
    "all_stations = np.union1d(df['start_station_id'].unique(), df['end_station_id'].unique()).astype(int)\n",
    "all_stations_sorted = sorted([int(x) for x in all_stations])\n",
    "n_stations = len(all_stations_sorted)\n",
    "stations = sorted([int(x) for x in all_stations])\n",
    "print(f'Total stations used for matrices: {n_stations}')\n",
    "\n",
    "# helper to make pivot and plot\n",
    "\n",
    "def make_pivot(df_subset, stations, aggcol='trip_count'):\n",
    "    # aggregate counts to ensure one cell per pair\n",
    "    agg = df_subset.groupby(['start_station_id', 'end_station_id'])[aggcol].sum().reset_index()\n",
    "    pivot = agg.pivot(index='start_station_id', columns='end_station_id', values=aggcol).reindex(index=stations, columns=stations).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "\n",
    "def plot_matrix(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    # dynamic figsize: cap sizes to avoid enormous images\n",
    "    height, width = arr.shape\n",
    "    figsize = (min(20, max(6, width/10)), min(20, max(6, height/10)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('end_station_id (ordered)')\n",
    "    ax.set_ylabel('start_station_id (ordered)')\n",
    "\n",
    "    # reduce tick labels for readability: show first, middle, last\n",
    "    if width <= 30:\n",
    "        xticks = range(width)\n",
    "        xtick_labels = [str(int(v)) for v in matrix_df.columns]\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xtick_labels, rotation=90, fontsize=6)\n",
    "    else:\n",
    "        ax.set_xticks([0, width//2, width-1])\n",
    "        ax.set_xticklabels([str(int(matrix_df.columns[0])), str(int(matrix_df.columns[width//2])), str(int(matrix_df.columns[-1]))], fontsize=8)\n",
    "\n",
    "    if height <= 30:\n",
    "        yticks = range(height)\n",
    "        ytick_labels = [str(int(v)) for v in matrix_df.index]\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(ytick_labels, fontsize=6)\n",
    "    else:\n",
    "        ax.set_yticks([0, height//2, height-1])\n",
    "        ax.set_yticklabels([str(int(matrix_df.index[0])), str(int(matrix_df.index[height//2])), str(int(matrix_df.index[-1]))], fontsize=8)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved heatmap: {outpath}')\n",
    "\n",
    "\n",
    "# 1) Per-month heatmaps\n",
    "unique_months = sorted(df['year_month'].unique())\n",
    "print(f'Found {len(unique_months)} unique months (expected 36): {unique_months}')\n",
    "\n",
    "for ym in unique_months:\n",
    "    df_month = df[df['year_month'] == ym]\n",
    "    pivot = make_pivot(df_month, stations)\n",
    "    outpath = OUTPUT_DIR / f'heatmap_{ym}.png'\n",
    "    title = f'OD heatmap {ym} (log display={LOG_DISPLAY})'\n",
    "    plot_matrix(pivot, title, outpath)\n",
    "\n",
    "# 2) Combined heatmap for all data\n",
    "pivot_combined = make_pivot(df, stations)\n",
    "# save numeric combined matrix\n",
    "pivot_combined.to_csv(OUTPUT_DIR / 'od_matrix_combined.csv')\n",
    "plot_matrix(pivot_combined, 'OD heatmap combined (log display={})'.format(LOG_DISPLAY), OUTPUT_DIR / 'heatmap_combined.png')\n",
    "\n",
    "print('All done. Generated per-month and combined heatmaps in:')\n",
    "print(OUTPUT_DIR.resolve())"
   ],
   "id": "4d87a987f75f2586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations used for matrices: 198\n",
      "Found 36 unique months (expected 36): ['2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09']\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_combined.png\n",
      "All done. Generated per-month and combined heatmaps in:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:18.018827Z",
     "start_time": "2025-11-07T12:53:14.888488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "seasonal_hourly_heatmaps.py\n",
    "\n",
    "Generates seasonal-hourly OD trip heatmaps:\n",
    " - One heatmap per year (season x hour matrix) for each year in the data\n",
    " - One combined heatmap over all years\n",
    " - Saves matrices as CSV and images to OUTPUT_DIR\n",
    "\n",
    "Seasons used (Northern Hemisphere standard):\n",
    " - Winter: Dec, Jan, Feb (DJF)\n",
    " - Spring: Mar, Apr, May (MAM)\n",
    " - Summer: Jun, Jul, Aug (JJA)\n",
    " - Autumn: Sep, Oct, Nov (SON)\n",
    "\n",
    "Expect input columns: ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    "\n",
    "Run:\n",
    "    python seasonal_hourly_heatmaps.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps_seasonal')\n",
    "DPI = 150\n",
    "LOG_DISPLAY = False  # For seasonal-hour heatmaps we keep linear counts by default\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_year_month(ym):\n",
    "    if isinstance(ym, str):\n",
    "        if '-' in ym:\n",
    "            parts = ym.split('-')\n",
    "            return int(parts[0]), int(parts[1])\n",
    "        if '/' in ym:\n",
    "            parts = ym.split('/')\n",
    "            # assume MM/YYYY -> return (YYYY, MM)\n",
    "            return int(parts[1]), int(parts[0])\n",
    "    raise ValueError(f'Unrecognized year_month format: {ym}')\n",
    "\n",
    "parsed = df['year_month'].apply(parse_year_month)\n",
    "df['year'] = parsed.apply(lambda x: x[0])\n",
    "df['month'] = parsed.apply(lambda x: x[1])\n",
    "\n",
    "# season mapping\n",
    "season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "              3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "              6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "              9: 'Autumn', 10: 'Autumn', 11: 'Autumn'}\n",
    "\n",
    "df['season'] = df['month'].map(season_map)\n",
    "\n",
    "# Helper: build season-hour pivot for a given dataframe\n",
    "\n",
    "def season_hour_pivot(df_subset):\n",
    "    # aggregate trip counts by season and hour\n",
    "    agg = df_subset.groupby(['season', 'hour'])['trip_count'].sum().reset_index()\n",
    "    # ensure all seasons and hours 0-23 present\n",
    "    seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    hours = list(range(24))\n",
    "    pivot = agg.pivot(index='season', columns='hour', values='trip_count').reindex(index=seasons, columns=hours).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "# plotting helper\n",
    "\n",
    "def plot_season_hour(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Hour of day')\n",
    "    ax.set_ylabel('Season')\n",
    "\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "    ax.set_xticklabels([str(h) for h in range(0, 24, 2)])\n",
    "\n",
    "    ax.set_yticks(range(len(matrix_df.index)))\n",
    "    ax.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved: {outpath}')\n",
    "\n",
    "# 1) Per-year seasonal-hour heatmaps\n",
    "years = sorted(df['year'].unique())\n",
    "print(f'Found years: {years}')\n",
    "\n",
    "for y in years:\n",
    "    df_year = df[df['year'] == y]\n",
    "    pivot = season_hour_pivot(df_year)\n",
    "    csv_out = OUTPUT_DIR / f'season_hour_matrix_{y}.csv'\n",
    "    img_out = OUTPUT_DIR / f'season_hour_heatmap_{y}.png'\n",
    "    pivot.to_csv(csv_out)\n",
    "    plot_season_hour(pivot, f'Season vs Hour - {y}', img_out)\n",
    "\n",
    "# 2) Combined (all years)\n",
    "pivot_all = season_hour_pivot(df)\n",
    "pivot_all.to_csv(OUTPUT_DIR / 'season_hour_matrix_all_years.csv')\n",
    "plot_season_hour(pivot_all, 'Season vs Hour - All years', OUTPUT_DIR / 'season_hour_heatmap_all_years.png')\n",
    "\n",
    "print('Done. Results saved to:')\n",
    "print(OUTPUT_DIR.resolve())\n"
   ],
   "id": "160179ad6a1012bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found years: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021)]\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2018.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2019.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2020.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2021.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_all_years.png\n",
      "Done. Results saved to:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:32.658283Z",
     "start_time": "2025-11-07T12:53:18.026627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "combine_36_csvs_no_date.py\n",
    "\n",
    "Reads all CSV files from cyclehire-cleandata named like 2018_10.csv ... 2021_09.csv\n",
    "(or containing that yyyy_mm pattern in the filename), concatenates them into a single CSV\n",
    "and saves it to cyclehire-cleandata\\combined_all_periods.csv\n",
    "\n",
    "Notes:\n",
    " - This script does NOT add a date column (as requested).\n",
    " - It will align columns by name; missing columns in some files will be filled with NaN.\n",
    " - It prints a short summary of files read and total rows combined.\n",
    "\n",
    "Usage:\n",
    "    python combine_36_csvs_no_date.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata')\n",
    "OUTPUT_FILE = INPUT_DIR / 'combined_all_periods.csv'\n",
    "GLOB_PATTERN = '*_*.csv'  # matches files with yyyy_mm in name like 2018_10.csv\n",
    "FILE_FILTER_REGEX = re.compile(r'(20\\d{2})[_-](0[1-9]|1[0-2])')  # restrict to yyyy_mm patterns\n",
    "# ------------------------\n",
    "\n",
    "# collect candidate files\n",
    "files = sorted(INPUT_DIR.glob(GLOB_PATTERN))\n",
    "selected_files = [f for f in files if FILE_FILTER_REGEX.search(f.name)]\n",
    "\n",
    "if not selected_files:\n",
    "    raise FileNotFoundError(f'No files matching yyyy_mm pattern found in {INPUT_DIR}')\n",
    "\n",
    "print(f'Found {len(selected_files)} files to combine:')\n",
    "for f in selected_files:\n",
    "    print(' -', f.name)\n",
    "\n",
    "# read and concatenate\n",
    "dfs = []\n",
    "for f in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name  # optional: keep which file the row came from\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to read {f.name}: {e}')\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError('No files were successfully read.')\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "combined.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f'Combined {len(dfs)} files into {OUTPUT_FILE} (total rows: {len(combined):,})')\n",
    "\n",
    "# optional quick sanity print\n",
    "print('\\nColumn summary (name : non-null count):')\n",
    "print(combined.notna().sum().sort_values(ascending=False).head(50))\n",
    "\n",
    "print('\\nDone.')\n"
   ],
   "id": "5973531e49f04c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 files to combine:\n",
      " - 2018_10.csv\n",
      " - 2018_11.csv\n",
      " - 2018_12.csv\n",
      " - 2019_01.csv\n",
      " - 2019_02.csv\n",
      " - 2019_03.csv\n",
      " - 2019_04.csv\n",
      " - 2019_05.csv\n",
      " - 2019_06.csv\n",
      " - 2019_07.csv\n",
      " - 2019_08.csv\n",
      " - 2019_09.csv\n",
      " - 2019_10.csv\n",
      " - 2019_11.csv\n",
      " - 2019_12.csv\n",
      " - 2020_01.csv\n",
      " - 2020_02.csv\n",
      " - 2020_03.csv\n",
      " - 2020_04.csv\n",
      " - 2020_05.csv\n",
      " - 2020_06.csv\n",
      " - 2020_07.csv\n",
      " - 2020_08.csv\n",
      " - 2020_09.csv\n",
      " - 2020_10.csv\n",
      " - 2020_11.csv\n",
      " - 2020_12.csv\n",
      " - 2021_01.csv\n",
      " - 2021_02.csv\n",
      " - 2021_03.csv\n",
      " - 2021_04.csv\n",
      " - 2021_05.csv\n",
      " - 2021_06.csv\n",
      " - 2021_07.csv\n",
      " - 2021_08.csv\n",
      " - 2021_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_17544\\2259740896.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\cyclehire-cleandata\\combined_all_periods.csv (total rows: 460,655)\n",
      "\n",
      "Column summary (name : non-null count):\n",
      "started_at                   460655\n",
      "ended_at                     460655\n",
      "duration                     460655\n",
      "start_station_id             460655\n",
      "start_station_name           460655\n",
      "start_station_latitude       460655\n",
      "end_station_latitude         460655\n",
      "start_station_longitude      460655\n",
      "end_station_id               460655\n",
      "end_station_name             460655\n",
      "__source_file                460655\n",
      "end_station_longitude        460655\n",
      "start_station_description    456167\n",
      "end_station_description      455560\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:33.130078Z",
     "start_time": "2025-11-07T12:53:32.658283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(combined.info())\n",
    "print(combined.describe())"
   ],
   "id": "3e84a0d30fe9b22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460655 entries, 0 to 460654\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   started_at                 460655 non-null  object \n",
      " 1   ended_at                   460655 non-null  object \n",
      " 2   duration                   460655 non-null  int64  \n",
      " 3   start_station_id           460655 non-null  int64  \n",
      " 4   start_station_name         460655 non-null  object \n",
      " 5   start_station_description  456167 non-null  object \n",
      " 6   start_station_latitude     460655 non-null  float64\n",
      " 7   start_station_longitude    460655 non-null  float64\n",
      " 8   end_station_id             460655 non-null  int64  \n",
      " 9   end_station_name           460655 non-null  object \n",
      " 10  end_station_description    455560 non-null  object \n",
      " 11  end_station_latitude       460655 non-null  float64\n",
      " 12  end_station_longitude      460655 non-null  float64\n",
      " 13  __source_file              460655 non-null  object \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 49.2+ MB\n",
      "None\n",
      "           duration  start_station_id  start_station_latitude  \\\n",
      "count  4.606550e+05     460655.000000           460655.000000   \n",
      "mean   1.945961e+03        936.878545               55.950615   \n",
      "std    5.531311e+03        671.956842                0.013497   \n",
      "min    6.100000e+01        171.000000               55.908404   \n",
      "25%    6.280000e+02        261.000000               55.940238   \n",
      "50%    1.166000e+03       1024.000000               55.947600   \n",
      "75%    2.527000e+03       1729.000000               55.958566   \n",
      "max    2.363348e+06       2268.000000               55.992957   \n",
      "\n",
      "       start_station_longitude  end_station_id  end_station_latitude  \\\n",
      "count            460655.000000   460655.000000         460655.000000   \n",
      "mean                 -3.196412      983.399353             55.952535   \n",
      "std                   0.039072      678.334765              0.015748   \n",
      "min                  -3.407156      171.000000             53.395525   \n",
      "25%                  -3.207964      262.000000             55.941791   \n",
      "50%                  -3.192444     1025.000000             55.951501   \n",
      "75%                  -3.180693     1737.000000             55.962487   \n",
      "max                  -3.058307     2268.000000             55.992957   \n",
      "\n",
      "       end_station_longitude  \n",
      "count          460655.000000  \n",
      "mean               -3.195134  \n",
      "std                 0.041796  \n",
      "min                -3.407156  \n",
      "25%                -3.208070  \n",
      "50%                -3.191421  \n",
      "75%                -3.176351  \n",
      "max                -2.990138  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:37.148649Z",
     "start_time": "2025-11-07T12:53:33.130078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path where your data is stored\n",
    "data_path = r'E:\\Uni_PGT\\cyclehire-cleandata'\n",
    "\n",
    "# Convert started_at column to datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "\n",
    "# Extract weekday and hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Assign seasons based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "combined['season'] = combined['started_at'].dt.month.apply(get_season)\n",
    "\n",
    "# Order weekdays for consistent plotting\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create output folder for heatmaps\n",
    "output_dir = os.path.join(data_path, 'heatmaps_weekday_hour')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to plot and save heatmap\n",
    "def plot_heatmap(data, title, filename):\n",
    "    pivot_table = data.pivot_table(index='weekday', columns='hour', values='duration', aggfunc='count').fillna(0)\n",
    "    pivot_table = pivot_table.reindex(weekday_order)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(pivot_table, cmap='YlGnBu')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Weekday')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Generate heatmap for each season\n",
    "for season, data in combined.groupby('season'):\n",
    "    plot_heatmap(data, f'Trip Count by Hour and Weekday - {season}', f'heatmap_{season}.png')\n",
    "\n",
    "# Generate heatmap for all data combined\n",
    "plot_heatmap(combined, 'Trip Count by Hour and Weekday - All Data', 'heatmap_all_data.png')\n",
    "\n",
    "print(f\"Heatmaps saved in: {output_dir}\")"
   ],
   "id": "ffcd021ad0f256a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps saved in: E:\\Uni_PGT\\cyclehire-cleandata\\heatmaps_weekday_hour\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:53:39.902155Z",
     "start_time": "2025-11-07T12:53:37.150155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "duration_and_od_analysis.py\n",
    "\n",
    "Produces:\n",
    " - Histogram of trip durations (linear and log-scaled)\n",
    " - CSV and plot for average duration by hour of day\n",
    " - CSV and plot for average duration by weekday\n",
    " - OD heatmap (average duration per start_station_id x end_station_id)\n",
    "\n",
    "Assumptions:\n",
    " - Combined CSV of the 36 files exists at: cyclehire-cleandata\\combined_all_periods.csv\n",
    " - Columns include: started_at, ended_at, duration, start_station_id, end_station_id\n",
    "\n",
    "Outputs are written to cyclehire-cleandata\\analysis_outputs\n",
    "\n",
    "Run:\n",
    "    python duration_and_od_analysis.py\n",
    "\"\"\"\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs')\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Convert started_at to datetime (coerce errors)\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "# Filter out rows with missing datetime or non-positive duration\n",
    "initial_rows = len(combined)\n",
    "combined = combined[combined['duration'].notna()]\n",
    "combined = combined[combined['duration'] > 0]\n",
    "combined = combined[combined['started_at'].notna()]\n",
    "print(f'Kept {len(combined):,} rows (removed {initial_rows - len(combined):,} invalid rows)')\n",
    "\n",
    "# --- 1) Histograms of duration ---\n",
    "# Linear histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(combined['duration'], bins=100, range=(0, combined['duration'].quantile(0.99)))\n",
    "plt.title('Trip duration distribution (0-99th percentile)')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_linear.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Log-scaled histogram (log1p)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(np.log1p(combined['duration']), bins=100)\n",
    "plt.title('Trip duration distribution (log1p)')\n",
    "plt.xlabel('log1p(Duration)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_log.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Save basic stats\n",
    "desc = combined['duration'].describe()\n",
    "desc.to_csv(OUTPUT_DIR / 'duration_summary_stats.csv')\n",
    "\n",
    "# --- 2) Average duration by hour of day ---\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "avg_by_hour = combined.groupby('hour')['duration'].mean().reindex(range(24)).fillna(0)\n",
    "avg_by_hour.to_csv(OUTPUT_DIR / 'avg_duration_by_hour.csv')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(avg_by_hour.index, avg_by_hour.values, marker='o')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Average duration')\n",
    "plt.title('Average trip duration by hour of day')\n",
    "plt.xticks(range(0,24))\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_hour.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 3) Average duration by weekday ---\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "weekday_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "avg_by_weekday = combined.groupby('weekday')['duration'].mean().reindex(weekday_order)\n",
    "avg_by_weekday.to_csv(OUTPUT_DIR / 'avg_duration_by_weekday.csv')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(avg_by_weekday.index, avg_by_weekday.values)\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Average duration (seconds)')\n",
    "plt.title('Average trip duration by weekday')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_weekday.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 4) OD heatmap: average duration per (start_station_id x end_station_id) ---\n",
    "# To limit memory use, we will restrict to top N stations by activity, but also\n",
    "# save a CSV of aggregated averages for all pairs.\n",
    "\n",
    "# Aggregate per pair\n",
    "pair_agg = combined.groupby(['start_station_id','end_station_id'])['duration'].agg(['mean','count']).reset_index()\n",
    "pair_agg.rename(columns={'mean':'avg_duration','count':'trip_count'}, inplace=True)\n",
    "pair_agg.to_csv(OUTPUT_DIR / 'od_pair_avg_duration_all_pairs.csv', index=False)\n",
    "\n",
    "# Choose top stations by total trips (to make a manageable heatmap)\n",
    "station_activity = pd.concat([combined['start_station_id'], combined['end_station_id']]).value_counts()\n",
    "top_n = 100  # adjust if you want larger/smaller matrix\n",
    "top_stations = station_activity.index[:top_n].astype(int).tolist()\n",
    "print(f'Creating OD heatmap for top {len(top_stations)} stations by activity')\n",
    "\n",
    "# Pivot for top stations\n",
    "subset = pair_agg[pair_agg['start_station_id'].isin(top_stations) & pair_agg['end_station_id'].isin(top_stations)]\n",
    "heat = subset.pivot(index='start_station_id', columns='end_station_id', values='avg_duration').reindex(index=top_stations, columns=top_stations).fillna(0)\n",
    "\n",
    "# Plot heatmap (use log scale for color or linear depending on spread)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(np.log1p(heat.values), aspect='auto')\n",
    "plt.colorbar(label='log1p(avg_duration)')\n",
    "plt.title(f'OD average duration heatmap (top {len(top_stations)} stations)')\n",
    "plt.xlabel('end_station_id (ordered by activity)')\n",
    "plt.ylabel('start_station_id (ordered by activity)')\n",
    "# keep tick labels sparse for readability\n",
    "n = len(top_stations)\n",
    "plt.xticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.yticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'od_avg_duration_heatmap_top{top_n}.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "print('Analysis complete. Outputs saved to:', OUTPUT_DIR.resolve())\n"
   ],
   "id": "a1785be491b730e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_17544\\3145085024.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 460,232 rows (removed 423 invalid rows)\n",
      "Creating OD heatmap for top 100 stations by activity\n",
      "Analysis complete. Outputs saved to: E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:54:12.864246Z",
     "start_time": "2025-11-07T12:53:39.906125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "save_hourly_maps_with_basemap.py\n",
    "\n",
    "Saves per-hour origin-demand maps to disk (one PNG per hour) and also saves\n",
    "weekday-hour heatmaps. Uses the in-memory `combined` DataFrame if present; otherwise\n",
    "expects you to load it prior to running this script.\n",
    "\n",
    "Outputs are written to: E:/Uni_PGT/visualisation_outputs/station_hour_maps\n",
    "\n",
    "Requirements:\n",
    " - geopandas, matplotlib, seaborn, contextily (optional, for web basemap tiles)\n",
    "   Install with: pip install geopandas matplotlib seaborn contextily\n",
    "\n",
    "Notes on basemap: contextily fetches tiles from the web. If you have no internet,\n",
    "this script will fall back to plotting station points on plain axes.\n",
    "\n",
    "Run this in the same Python session where `combined` exists (not by re-loading the CSV).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/station_hour_maps')\n",
    "HEATMAP_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/heatmaps_weekday_hour')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HEATMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 200\n",
    "POINT_SCALE = 2000  # adjust to scale marker sizes (increase for larger markers)\n",
    "USE_CONTEXTILY = True  # set False if you don't want to fetch basemap tiles\n",
    "TILE_SOURCE = None  # default contextily source (None uses provider's default)\n",
    "# ------------------------\n",
    "\n",
    "print('Using in-memory DataFrame `combined`')\n",
    "try:\n",
    "    combined  # must exist in the environment\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first.')\n",
    "\n",
    "# Ensure datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "\n",
    "# Prepare station aggregated counts by hour\n",
    "agg = combined.groupby(['start_station_id','start_station_latitude','start_station_longitude','hour']).size().reset_index(name='starts')\n",
    "# If lat/lon columns have different names, try alternatives\n",
    "if agg['start_station_latitude'].isna().all() or agg['start_station_longitude'].isna().all():\n",
    "    # try alternative names in combined\n",
    "    lat_col = None\n",
    "    lon_col = None\n",
    "    for c in combined.columns:\n",
    "        if c.lower().endswith('latitude') and 'start' in c.lower():\n",
    "            lat_col = c\n",
    "        if c.lower().endswith('longitude') and 'start' in c.lower():\n",
    "            lon_col = c\n",
    "    if lat_col and lon_col:\n",
    "        agg = combined.groupby(['start_station_id', lat_col, lon_col, 'hour']).size().reset_index(name='starts')\n",
    "        agg = agg.rename(columns={lat_col:'start_station_latitude', lon_col:'start_station_longitude'})\n",
    "\n",
    "# drop rows without coordinates\n",
    "agg = agg.dropna(subset=['start_station_latitude','start_station_longitude'])\n",
    "\n",
    "# Build GeoDataFrame (EPSG:4326)\n",
    "agg['geometry'] = [Point(xy) for xy in zip(agg['start_station_longitude'].astype(float), agg['start_station_latitude'].astype(float))]\n",
    "gdf = gpd.GeoDataFrame(agg, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Project to Web Mercator for contextily (if using basemap)\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_web = gdf.to_crs(epsg=3857)\n",
    "    except Exception as e:\n",
    "        print('Could not reproject to WebMercator, disabling contextily basemap:', e)\n",
    "        USE_CONTEXTILY = False\n",
    "        gdf_web = gdf\n",
    "else:\n",
    "    gdf_web = gdf\n",
    "\n",
    "# Determine map extent (in web mercator if using contextily)\n",
    "minx, miny, maxx, maxy = gdf_web.total_bounds\n",
    "xpad = (maxx - minx) * 0.08 if maxx > minx else 100\n",
    "ypad = (maxy - miny) * 0.08 if maxy > miny else 100\n",
    "extent = (minx - xpad, maxx + xpad, miny - ypad, maxy + ypad)\n",
    "\n",
    "# Save one PNG per hour\n",
    "print('Saving hourly station-origin maps to:', OUT_DIR)\n",
    "for h in range(24):\n",
    "    hour_gdf = gdf_web[gdf_web['hour'] == h]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # plot all stations faintly as background\n",
    "    gdf_web.plot(ax=ax, color='lightgrey', markersize=5, alpha=0.5)\n",
    "\n",
    "    if not hour_gdf.empty:\n",
    "        # marker size scaled by sqrt to reduce dynamic range\n",
    "        sizes = np.sqrt(hour_gdf['starts'].astype(float) + 1) * (POINT_SCALE / max(1, np.sqrt(hour_gdf['starts'].max()*200+ 1)))\n",
    "        hour_gdf.plot(ax=ax, markersize=sizes, column='starts', cmap='Reds', legend=True, alpha=0.9)\n",
    "\n",
    "    if USE_CONTEXTILY:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            if TILE_SOURCE is None:\n",
    "                ctx.add_basemap(ax, crs=gdf_web.crs.to_string())\n",
    "            else:\n",
    "                ctx.add_basemap(ax, source=TILE_SOURCE, crs=gdf_web.crs.to_string())\n",
    "        except Exception as e:\n",
    "            print('contextily failed; continuing without basemap:', e)\n",
    "\n",
    "    ax.set_xlim(extent[0], extent[1])\n",
    "    ax.set_ylim(extent[2], extent[3])\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f'Origin starts — hour {h}')\n",
    "\n",
    "    outpath = OUT_DIR / f'station_starts_hour_{h:02d}.png'\n",
    "    fig.savefig(outpath, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print('Hourly maps saved.')\n",
    "\n",
    "# --- Save weekday-hour heatmap (counts) ---\n",
    "print('Saving weekday-hour heatmap...')\n",
    "combined['weekday'] = pd.Categorical(combined['weekday'], categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], ordered=True)\n",
    "pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(pivot, cmap='YlOrRd')\n",
    "plt.title('Trip starts by weekday and hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Weekday')\n",
    "hm_path = HEATMAP_DIR / 'weekday_hour_heatmap_all_data.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(hm_path, dpi=DPI)\n",
    "plt.close()\n",
    "print('Weekday-hour heatmap saved to', hm_path)\n",
    "\n",
    "print('\\nAll outputs written to:', OUT_DIR.parent)\n"
   ],
   "id": "567fec6fb072f56a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory DataFrame `combined`\n",
      "Saving hourly station-origin maps to: E:\\Uni_PGT\\visualisation_outputs\\station_hour_maps\n",
      "Hourly maps saved.\n",
      "Saving weekday-hour heatmap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_17544\\650905050.py:131: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekday-hour heatmap saved to E:\\Uni_PGT\\visualisation_outputs\\heatmaps_weekday_hour\\weekday_hour_heatmap_all_data.png\n",
      "\n",
      "All outputs written to: E:\\Uni_PGT\\visualisation_outputs\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:54:51.338483Z",
     "start_time": "2025-11-07T12:54:12.864246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hourly_maps_and_separate_bars_fixed_extent.py\n",
    "#\n",
    "# Creates per-hour maps (origins left, destinations right) and saves them to disk\n",
    "# (maps and separate bar charts). This version computes a robust map extent using\n",
    "# percentiles (2nd-98th) to ignore coordinate outliers so plots zoom into Edinburgh.\n",
    "#\n",
    "# Run in the same session where `combined` exists (the combined DataFrame you made).\n",
    "# Requires: geopandas, matplotlib, seaborn, contextily (optional for basemap tiles).\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/hourly_maps_v3_fixed_extent')\n",
    "MAP_DIR = OUT_DIR / 'maps'\n",
    "BARS_DIR = OUT_DIR / 'bars'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BARS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 180\n",
    "USE_CONTEXTILY = True     # set False if no internet or you don't want basemap tiles\n",
    "POINT_BASE = 35           # base marker size (smaller => less overlap)\n",
    "TOP_K_BARS = 12\n",
    "CMAP_ORIG = 'Reds'\n",
    "CMAP_DEST = 'Blues'\n",
    "# ---------------------------------\n",
    "\n",
    "# ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DataFrame `combined` not found in memory. Load it first.\")\n",
    "\n",
    "# parse datetimes and hour\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# flexible column detection\n",
    "def find_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "s_lat = find_col(combined, ['start_station_latitude','start_latitude','start_lat','start_station_lat'])\n",
    "s_lon = find_col(combined, ['start_station_longitude','start_longitude','start_lon','start_station_lon'])\n",
    "e_lat = find_col(combined, ['end_station_latitude','end_latitude','end_lat','end_station_lat'])\n",
    "e_lon = find_col(combined, ['end_station_longitude','end_longitude','end_lon','end_station_lon'])\n",
    "\n",
    "if not all([s_lat, s_lon, e_lat, e_lon]):\n",
    "    raise RuntimeError('Could not find necessary start/end latitude/longitude columns in combined dataframe.')\n",
    "\n",
    "# aggregate\n",
    "orig_agg = combined.groupby(['start_station_id', s_lat, s_lon, 'hour']).size().reset_index(name='count')\n",
    "orig_agg = orig_agg.rename(columns={s_lat:'lat', s_lon:'lon', 'start_station_id':'station_id'})\n",
    "\n",
    "dest_agg = combined.groupby(['end_station_id', e_lat, e_lon, 'hour']).size().reset_index(name='count')\n",
    "dest_agg = dest_agg.rename(columns={e_lat:'lat', e_lon:'lon', 'end_station_id':'station_id'})\n",
    "\n",
    "orig_agg = orig_agg.dropna(subset=['lat','lon'])\n",
    "dest_agg = dest_agg.dropna(subset=['lat','lon'])\n",
    "\n",
    "# GeoDataFrames (WGS84)\n",
    "gdf_o = gpd.GeoDataFrame(orig_agg, geometry=[Point(xy) for xy in zip(orig_agg['lon'].astype(float), orig_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "gdf_d = gpd.GeoDataFrame(dest_agg, geometry=[Point(xy) for xy in zip(dest_agg['lon'].astype(float), dest_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "\n",
    "# project to WebMercator for basemap if desired\n",
    "use_ctx = False\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_o_web = gdf_o.to_crs(epsg=3857)\n",
    "        gdf_d_web = gdf_d.to_crs(epsg=3857)\n",
    "        use_ctx = True\n",
    "    except Exception as e:\n",
    "        print('Contextily disabled (reprojection failed):', e)\n",
    "        gdf_o_web = gdf_o\n",
    "        gdf_d_web = gdf_d\n",
    "else:\n",
    "    gdf_o_web = gdf_o\n",
    "    gdf_d_web = gdf_d\n",
    "\n",
    "# ---------------------------\n",
    "# ROBUST EXTENT (2nd - 98th percentile)\n",
    "# ---------------------------\n",
    "# collect x,y arrays (projected)\n",
    "xs = pd.concat([\n",
    "    gdf_o_web.geometry.x.rename('x') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.x.rename('x') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "ys = pd.concat([\n",
    "    gdf_o_web.geometry.y.rename('y') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.y.rename('y') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(xs) == 0 or len(ys) == 0 or not np.isfinite(xs.to_numpy()).any():\n",
    "    raise RuntimeError('No valid projected coordinates found to compute map extent.')\n",
    "\n",
    "low_pct, high_pct = 2, 98\n",
    "minx, maxx = np.percentile(xs, [low_pct, high_pct])\n",
    "miny, maxy = np.percentile(ys, [low_pct, high_pct])\n",
    "\n",
    "# fallback to full min/max if needed\n",
    "full_minx, full_maxx = xs.min(), xs.max()\n",
    "full_miny, full_maxy = ys.min(), ys.max()\n",
    "\n",
    "# add padding (at least 300 m)\n",
    "span_x = maxx - minx\n",
    "span_y = maxy - miny\n",
    "pad_x = max(span_x * 0.06, 300)\n",
    "pad_y = max(span_y * 0.06, 300)\n",
    "extent = (minx - pad_x, maxx + pad_x, miny - pad_y, maxy + pad_y)\n",
    "\n",
    "# safety clamp if extent absurdly large\n",
    "max_allowed_span = 200_000  # 200 km\n",
    "if (extent[1] - extent[0] > max_allowed_span) or (extent[3] - extent[2] > max_allowed_span):\n",
    "    print('WARNING: computed extent is very large. Falling back to full bounds.')\n",
    "    pad_x_f = max((full_maxx - full_minx) * 0.06, 300)\n",
    "    pad_y_f = max((full_maxy - full_miny) * 0.06, 300)\n",
    "    extent = (full_minx - pad_x_f, full_maxx + pad_x_f, full_miny - pad_y_f, full_maxy + pad_y_f)\n",
    "\n",
    "# print sample outliers (stations outside percentile window) to help debugging\n",
    "outlier_mask_o = (gdf_o_web.geometry.x < minx) | (gdf_o_web.geometry.x > maxx) | (gdf_o_web.geometry.y < miny) | (gdf_o_web.geometry.y > maxy)\n",
    "outlier_mask_d = (gdf_d_web.geometry.x < minx) | (gdf_d_web.geometry.x > maxx) | (gdf_d_web.geometry.y < miny) | (gdf_d_web.geometry.y > maxy)\n",
    "outliers_o = gdf_o_web.loc[outlier_mask_o, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "outliers_d = gdf_d_web.loc[outlier_mask_d, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "if not outliers_o.empty or not outliers_d.empty:\n",
    "    print('Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):')\n",
    "    if not outliers_o.empty:\n",
    "        print(' Origin outliers:')\n",
    "        print(outliers_o.to_string(index=False))\n",
    "    if not outliers_d.empty:\n",
    "        print(' Destination outliers:')\n",
    "        print(outliers_d.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# plotting helpers\n",
    "# ---------------------------\n",
    "def sizes_from_counts(series, base=POINT_BASE):\n",
    "    arr = np.sqrt(series.fillna(0).astype(float) + 1.0)\n",
    "    if arr.max() > 0:\n",
    "        scaled = base * (arr / arr.max())\n",
    "    else:\n",
    "        scaled = np.full_like(arr, base * 0.2)\n",
    "    return np.clip(scaled, 2, base * 1.1)\n",
    "\n",
    "# main loop: produce map PNG and a separate bar PNG for each hour\n",
    "for h in range(24):\n",
    "    o_h = gdf_o_web[gdf_o_web['hour'] == h].copy()\n",
    "    d_h = gdf_d_web[gdf_d_web['hour'] == h].copy()\n",
    "\n",
    "    # compute maximum count for color scaling\n",
    "    max_count = int(max(o_h['count'].max() if not o_h.empty else 0,\n",
    "                        d_h['count'].max() if not d_h.empty else 0, 1))\n",
    "\n",
    "    # ---------- MAP FIGURE (orig left, dest right) ----------\n",
    "    fig, (ax_o, ax_d) = plt.subplots(1, 2, figsize=(14, 8), constrained_layout=True)\n",
    "\n",
    "    # set extent and add basemap first (so tiles cover area)\n",
    "    for ax in (ax_o, ax_d):\n",
    "        ax.set_xlim(extent[0], extent[1])\n",
    "        ax.set_ylim(extent[2], extent[3])\n",
    "\n",
    "    if use_ctx:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            ctx.add_basemap(ax_o, crs=gdf_o_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "            ctx.add_basemap(ax_d, crs=gdf_d_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "        except Exception as e:\n",
    "            print('contextily error (continuing without basemap):', e)\n",
    "\n",
    "    # faint background of all stations (for context)\n",
    "    if not gdf_o_web.empty:\n",
    "        ax_o.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "        ax_d.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "\n",
    "    # plot origins (red)\n",
    "    if not o_h.empty:\n",
    "        sizes_o = sizes_from_counts(o_h['count'])\n",
    "        cmap_o = plt.colormaps.get_cmap(CMAP_ORIG)\n",
    "        norm_o = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_o = cmap_o(norm_o(o_h['count'].astype(float)))\n",
    "        ax_o.scatter(o_h.geometry.x, o_h.geometry.y, s=sizes_o, color=colors_o, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_o = cm.ScalarMappable(norm=norm_o, cmap=CMAP_ORIG)\n",
    "        sm_o._A = []\n",
    "        fig.colorbar(sm_o, ax=ax_o, fraction=0.046, pad=0.02).set_label('Origin count (hour)')\n",
    "\n",
    "    ax_o.set_title(f'Origins — hour {h}')\n",
    "    ax_o.axis('off')\n",
    "\n",
    "    # plot destinations (blue)\n",
    "    if not d_h.empty:\n",
    "        sizes_d = sizes_from_counts(d_h['count'])\n",
    "        cmap_d = plt.colormaps.get_cmap(CMAP_DEST)\n",
    "        norm_d = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_d = cmap_d(norm_d(d_h['count'].astype(float)))\n",
    "        ax_d.scatter(d_h.geometry.x, d_h.geometry.y, s=sizes_d, color=colors_d, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_d = cm.ScalarMappable(norm=norm_d, cmap=CMAP_DEST)\n",
    "        sm_d._A = []\n",
    "        fig.colorbar(sm_d, ax=ax_d, fraction=0.046, pad=0.02).set_label('Destination count (hour)')\n",
    "\n",
    "    ax_d.set_title(f'Destinations — hour {h}')\n",
    "    ax_d.axis('off')\n",
    "\n",
    "    map_out = MAP_DIR / f'hour_{h:02d}_maps.png'\n",
    "    fig.savefig(map_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- BARCHART FIGURE (separate) ----------\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5), constrained_layout=True)\n",
    "\n",
    "    if not o_h.empty:\n",
    "        top_o = o_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_o = top_o['station_id'].astype(str).values[::-1]   # reversed for horizontal bars\n",
    "        counts_o = top_o['count'].values[::-1]\n",
    "        ax1.barh(labels_o, counts_o, color='tab:red')\n",
    "        ax1.set_title(f'Top {TOP_K_BARS} origin stations (hour {h})')\n",
    "        ax1.set_xlabel('Starts')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No origin data', ha='center', va='center')\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "    if not d_h.empty:\n",
    "        top_d = d_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_d = top_d['station_id'].astype(str).values[::-1]\n",
    "        counts_d = top_d['count'].values[::-1]\n",
    "        ax2.barh(labels_d, counts_d, color='tab:blue')\n",
    "        ax2.set_title(f'Top {TOP_K_BARS} destination stations (hour {h})')\n",
    "        ax2.set_xlabel('Ends')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No destination data', ha='center', va='center')\n",
    "        ax2.set_axis_off()\n",
    "\n",
    "    bars_out = BARS_DIR / f'hour_{h:02d}_bars.png'\n",
    "    fig2.savefig(bars_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig2)\n",
    "\n",
    "print('Saved maps to', MAP_DIR)\n",
    "print('Saved bar charts to', BARS_DIR)\n"
   ],
   "id": "8bb4164da9370389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):\n",
      " Origin outliers:\n",
      " station_id       lat       lon\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "       1870 55.990530 -3.385597\n",
      " Destination outliers:\n",
      " station_id       lat       lon\n",
      "        280 53.395525 -2.990138\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "Saved maps to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\maps\n",
      "Saved bar charts to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\bars\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:02.832976Z",
     "start_time": "2025-11-07T12:54:51.342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# contiguous_hour_clustering_and_gravity_avg_per_day.py\n",
    "\"\"\"\n",
    "Same clustering + gravity script you provided, but changes to produce\n",
    "OD aggregated per cluster *averaged per calendar day*.\n",
    "\n",
    "Key change: after summing hourly OD matrices for a cluster we divide\n",
    "by `n_days` (the number of unique calendar dates present in the\n",
    "`combined` DataFrame). This turns cluster totals (total trips across\n",
    "all days in the dataset occurring during those cluster hours) into\n",
    "an average number of trips per calendar day for that cluster.\n",
    "\n",
    "Notes / caveats:\n",
    " - `n_days` is computed globally from `combined['started_at'].dt.date.nunique()`.\n",
    "   This is the simplest and most defensible choice: it gives average trips\n",
    "   per calendar day across the whole observation window. If you prefer to\n",
    "   compute `n_days` per-hour or per-cluster (e.g. count of days that actually\n",
    "   contained at least one trip in the cluster hours), see the comment below\n",
    "   and I can provide that variant.\n",
    " - The averaged `agg` (OD) is then used to compute O and D (origin/destination\n",
    "   totals) and run the gravity model. The gravity model will therefore predict\n",
    "   average trips per calendar day for that cluster period.\n",
    "\n",
    "Run this in the same session where `combined` (the concatenated trip DataFrame)\n",
    "exists in memory.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity_avg_per_day')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLUSTER_COUNT = 4            # number of contiguous clusters to form\n",
    "PCA_VARIANCE = 0.90          # keep PCA components explaining this fraction of variance\n",
    "BETA_1 = 0.0005\n",
    "ERROR_THRESHOLD = 0.01\n",
    "IMPROVEMENT_THRESHOLD = 1e-6\n",
    "MAX_ITERS = 2000\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# Ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first as `combined`.')\n",
    "\n",
    "# Prepare datetime and hour\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "if 'hour' not in combined.columns:\n",
    "    combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Compute number of calendar days present in the dataset (global)\n",
    "# This is used to convert cluster totals -> average trips per calendar day\n",
    "n_days = combined['started_at'].dt.date.nunique()\n",
    "print(f\"Dataset spans {n_days} calendar days (unique dates) — cluster OD will be averaged per calendar day.\")\n",
    "\n",
    "# Optional alternative (commented): compute n_days_per_cluster by counting unique dates\n",
    "# with at least one trip in cluster hours. This can yield slightly different averages\n",
    "# that only count days where cluster hours had any activity. If you prefer that,\n",
    "# uncomment and use the per-cluster approach shown later in a comment.\n",
    "\n",
    "# Precompute hourly OD pivot tables (raw counts per hour)\n",
    "hourly_pivots = {}\n",
    "for h in range(24):\n",
    "    sub = combined[combined['hour'] == h]\n",
    "    if sub.empty:\n",
    "        hourly_pivots[h] = pd.DataFrame()\n",
    "        continue\n",
    "    counts = sub.groupby(['start_station_id', 'end_station_id']).size().reset_index(name='count')\n",
    "    pivot = counts.pivot(index='start_station_id', columns='end_station_id', values='count').fillna(0)\n",
    "    hourly_pivots[h] = pivot\n",
    "\n",
    "# Build aligned feature vectors for each hour using union of station ids\n",
    "hours = sorted(hourly_pivots.keys())\n",
    "all_stations = sorted({int(s) for h in hours for s in (list(hourly_pivots[h].index) + list(hourly_pivots[h].columns))})\n",
    "if len(all_stations) == 0:\n",
    "    raise RuntimeError('No station IDs found in hourly pivots--check combined data')\n",
    "\n",
    "def pivot_to_aligned_vector(pivot_df, stations):\n",
    "    mat = pd.DataFrame(0.0, index=stations, columns=stations)\n",
    "    if not pivot_df.empty:\n",
    "        tmp = pivot_df.reindex(index=stations, columns=stations).fillna(0)\n",
    "        mat.iloc[:, :] = tmp.values\n",
    "    return mat.values.flatten()\n",
    "\n",
    "X_list = []\n",
    "for h in hours:\n",
    "    vec = pivot_to_aligned_vector(hourly_pivots[h], all_stations)\n",
    "    X_list.append(vec)\n",
    "X = np.vstack(X_list)  # shape (24, n_features)\n",
    "\n",
    "# Scale + PCA\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=PCA_VARIANCE, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Build chain connectivity (adjacency) for contiguous clustering\n",
    "n_hours = X_pca.shape[0]\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for i in range(n_hours - 1):\n",
    "    rows.extend([i, i+1])\n",
    "    cols.extend([i+1, i])\n",
    "    data.extend([1, 1])\n",
    "connectivity = csr_matrix((data, (rows, cols)), shape=(n_hours, n_hours))\n",
    "\n",
    "# Run contiguous agglomerative clustering\n",
    "agg = AgglomerativeClustering(n_clusters=CLUSTER_COUNT, linkage='ward', connectivity=connectivity)\n",
    "labels = agg.fit_predict(X_pca)\n",
    "\n",
    "hour_cluster_df = pd.DataFrame({'hour': hours, 'cluster': labels}).sort_values('hour')\n",
    "hour_cluster_df.to_csv(OUTPUT_DIR / 'cluster_membership.csv', index=False)\n",
    "print('Hour -> Cluster mapping saved to', OUTPUT_DIR / 'cluster_membership.csv')\n",
    "\n",
    "# Helper: haversine pairwise\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Impedance function (single det1 used here)\n",
    "def new_cost1(cost_matrix, beta=BETA_1):\n",
    "    return np.exp(-beta * cost_matrix)\n",
    "\n",
    "# Gravity model (doubly-constrained IPF)\n",
    "def gravity_model(O, D, det, error_threshold=ERROR_THRESHOLD, improvement_threshold=IMPROVEMENT_THRESHOLD, max_iters=MAX_ITERS):\n",
    "    O = np.array(O, dtype=float).copy()\n",
    "    D = np.array(D, dtype=float).copy()\n",
    "    sum_O = O.sum(); sum_D = D.sum()\n",
    "    if sum_O <= 0 or sum_D <= 0:\n",
    "        raise ValueError('Origin or Destination totals sum to zero')\n",
    "    if abs(sum_O - sum_D) > 1e-9:\n",
    "        D = D * (sum_O / sum_D)\n",
    "    n = len(O)\n",
    "    Ai = np.ones(n)\n",
    "    Bj = np.ones(n)\n",
    "    prev_error = np.inf\n",
    "    Tij = np.zeros((n, n), dtype=float)\n",
    "    det_mat = np.array(det, dtype=float).copy()\n",
    "    det_mat[det_mat < EPS] = EPS\n",
    "    iteration = 0\n",
    "    while iteration < max_iters:\n",
    "        iteration += 1\n",
    "        denom_i = (det_mat * (Bj * D)[None, :]).sum(axis=1) + EPS\n",
    "        Ai = 1.0 / denom_i\n",
    "        denom_j = (det_mat * (Ai * O)[:, None]).sum(axis=0) + EPS\n",
    "        Bj_new = 1.0 / denom_j\n",
    "        Tij = (Ai * O)[:, None] * (Bj_new * D)[None, :] * det_mat\n",
    "        error = (np.abs(O - Tij.sum(axis=1)).sum() + np.abs(D - Tij.sum(axis=0)).sum()) / (sum_O + EPS)\n",
    "        improvement = abs(prev_error - error)\n",
    "        if error < error_threshold:\n",
    "            stop_reason = 'Error threshold met'\n",
    "            break\n",
    "        if improvement < improvement_threshold:\n",
    "            stop_reason = 'Slow improvement'\n",
    "            break\n",
    "        prev_error = error\n",
    "        Bj = Bj_new\n",
    "    else:\n",
    "        stop_reason = 'max_iters'\n",
    "    diagnostics = {'iterations': iteration, 'error': float(error), 'stop_reason': stop_reason}\n",
    "    return Tij, diagnostics\n",
    "\n",
    "# Metrics\n",
    "def calculate_metrics(predicted_T, observed_T_df):\n",
    "    obs = observed_T_df.to_numpy().astype(float)\n",
    "    pred = np.array(predicted_T, dtype=float)\n",
    "    if obs.shape != pred.shape:\n",
    "        raise ValueError('Predicted and observed shapes differ')\n",
    "    obs_f = obs.flatten(); pred_f = pred.flatten()\n",
    "    mse = np.mean((obs_f - pred_f) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    ss_tot = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "    ss_res = np.sum((obs_f - pred_f) ** 2)\n",
    "    r2 = float(1.0 - (ss_res / (ss_tot + EPS)))\n",
    "    return {'rmse': rmse, 'r2': r2}\n",
    "\n",
    "# Aggregate OD per cluster and run gravity — now averaging per calendar day\n",
    "for c in sorted(hour_cluster_df['cluster'].unique()):\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster'] == c]['hour'].tolist()\n",
    "    print(f'Processing cluster {c}: hours = {hrs}')\n",
    "    # sum OD counts across hours in cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    agg = agg.fillna(0)\n",
    "\n",
    "    # ----- NEW: convert cluster totals -> average per calendar day -----\n",
    "    # Divide the aggregated matrix by the number of calendar days in the dataset\n",
    "    # so that values represent \"average trips per calendar day during the cluster hours\".\n",
    "    if n_days > 0:\n",
    "        agg = agg / float(n_days)\n",
    "    else:\n",
    "        print('Warning: n_days==0, skipping division (no date information)')\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    if agg.empty:\n",
    "        print(f'Cluster {c}: empty aggregated OD, skipping')\n",
    "        continue\n",
    "\n",
    "    # ensure square by intersection\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c}: too few common stations ({len(common)}), skipping')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "\n",
    "    # Save the averaged cluster OD (this CSV now contains average trips per calendar day for the cluster hours)\n",
    "    agg.to_csv(OUTPUT_DIR / f'cluster_od_avgperday_{c}.csv')\n",
    "\n",
    "    n = len(common)\n",
    "    print(f'Cluster {c}: n_stations={n} (averaged over {n_days} days)')\n",
    "    if n > 1500:\n",
    "        print('WARNING: cluster has many stations (>1500); this may be slow and memory-heavy')\n",
    "\n",
    "    # build coord df (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station lon/lat columns in combined')\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "    if len(coord_df) != n:\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        if missing:\n",
    "            print(f'Cluster {c}: dropping {len(missing)} stations with missing coords (sample): {list(missing)[:10]}')\n",
    "            keep = [s for s in common if s not in missing]\n",
    "            if len(keep) < 2:\n",
    "                print(f'Cluster {c}: too few stations after dropping, skipping')\n",
    "                continue\n",
    "            agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "            coord_df = coord_df.reindex(keep)\n",
    "            common = keep\n",
    "            n = len(common)\n",
    "\n",
    "    # compute cost matrix\n",
    "    lons = coord_df['lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df['lat'].to_numpy(dtype=float)\n",
    "    cost_m = haversine_pairwise(lons, lats)\n",
    "    pd.DataFrame(cost_m, index=common, columns=common).to_csv(OUTPUT_DIR / f'cost_matrix_cluster_{c}.csv')\n",
    "\n",
    "    # deterrence matrix\n",
    "    det1 = new_cost1(cost_m, beta=BETA_1)\n",
    "\n",
    "    # totals (these totals are now average trips per calendar day for the cluster hours)\n",
    "    O = agg.sum(axis=1).to_numpy()\n",
    "    D = agg.sum(axis=0).to_numpy()\n",
    "\n",
    "    # run gravity\n",
    "    Tij1, diag1 = gravity_model(O.copy(), D.copy(), det1)\n",
    "    pred1_df = pd.DataFrame(Tij1, index=agg.index, columns=agg.columns)\n",
    "    pred1_df.to_csv(OUTPUT_DIR / f'predicted_gravity_det1_cluster_{c}_avgperday.csv')\n",
    "    metrics1 = calculate_metrics(pred1_df, agg)\n",
    "\n",
    "    pd.DataFrame([diag1]).to_csv(OUTPUT_DIR / f'diagnostics_det1_cluster_{c}.csv', index=False)\n",
    "    pd.DataFrame([metrics1]).to_csv(OUTPUT_DIR / f'metrics_det1_cluster_{c}.csv', index=False)\n",
    "\n",
    "    print(f'Cluster {c}: done. metrics_det1={metrics1}')\n",
    "\n",
    "print('All clusters processed. Outputs in', OUTPUT_DIR)\n"
   ],
   "id": "e8359de2fc7a9129",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset spans 1083 calendar days (unique dates) — cluster OD will be averaged per calendar day.\n",
      "Hour -> Cluster mapping saved to E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity_avg_per_day\\cluster_membership.csv\n",
      "Processing cluster 0: hours = [11, 12, 13, 14, 15]\n",
      "Cluster 0: n_stations=191 (averaged over 1083 days)\n",
      "Cluster 0: done. metrics_det1={'rmse': 0.0165686802815451, 'r2': 0.5223031333344694}\n",
      "Processing cluster 1: hours = [16, 17]\n",
      "Cluster 1: n_stations=186 (averaged over 1083 days)\n",
      "Cluster 1: done. metrics_det1={'rmse': 0.006940969466649425, 'r2': 0.5754631478891745}\n",
      "Processing cluster 2: hours = [18, 19, 20, 21, 22, 23]\n",
      "Cluster 2: n_stations=182 (averaged over 1083 days)\n",
      "Cluster 2: done. metrics_det1={'rmse': 0.008103426581723673, 'r2': 0.6272312037982717}\n",
      "Processing cluster 3: hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Cluster 3: n_stations=187 (averaged over 1083 days)\n",
      "Cluster 3: done. metrics_det1={'rmse': 0.00854909312389969, 'r2': 0.5308287108154313}\n",
      "All clusters processed. Outputs in E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity_avg_per_day\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:04.180312Z",
     "start_time": "2025-11-07T12:55:02.832976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# beta_grid_det1_avg_per_day.py\n",
    "# Updated beta-grid search script that expects cluster-aggregated OD to be\n",
    "# averaged per calendar day before running gravity model.\n",
    "# This version computes n_days from the `combined` DataFrame and divides the\n",
    "# aggregated cluster OD by n_days (so O/D represent average trips per calendar day).\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- USER / RUN-TIME CONFIG ----------\n",
    "beta_grid = np.logspace(-6, -2, 25)   # grid to search for beta (adjust if desired)\n",
    "OUTPUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/clustered_gravity')  # match your cluster script\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EPS = 1e-12\n",
    "# --------------------------------------------\n",
    "\n",
    "# Basic haversine (meters)\n",
    "def haversine_pairwise(lons, lats):\n",
    "    R = 6371000.0\n",
    "    lon = np.radians(lons)\n",
    "    lat = np.radians(lats)\n",
    "    dlon = lon[:, None] - lon[None, :]\n",
    "    dlat = lat[:, None] - lat[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat[:, None]) * np.cos(lat[None, :]) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# Normalized RMSE helper\n",
    "def normalized_rmse(pred_df, obs_df):\n",
    "    obs = obs_df.to_numpy(dtype=float)\n",
    "    pred = pred_df.to_numpy(dtype=float)\n",
    "    mse = np.mean((obs - pred) ** 2)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mean_obs = float(np.mean(obs))\n",
    "    if mean_obs == 0:\n",
    "        return {'rmse': rmse, 'nrmse': float('inf')}\n",
    "    return {'rmse': rmse, 'nrmse': float(rmse / mean_obs)}\n",
    "\n",
    "# Ensure gravity_model & calculate_metrics exist (from your previous script). If not, provide safe fallback.\n",
    "try:\n",
    "    gravity_model  # noqa\n",
    "except NameError:\n",
    "    raise RuntimeError(\"gravity_model is not defined in the session. Run the clustering/gravity script first (which defines gravity_model).\")\n",
    "\n",
    "try:\n",
    "    calculate_metrics  # noqa\n",
    "except NameError:\n",
    "    # fallback basic metrics (should be similar to your earlier calculate_metrics)\n",
    "    def calculate_metrics(pred_df, obs_df):\n",
    "        obs = obs_df.to_numpy(dtype=float)\n",
    "        pred = pred_df.to_numpy(dtype=float)\n",
    "        if obs.shape != pred.shape:\n",
    "            raise ValueError('Predicted and observed shapes differ')\n",
    "        obs_f = obs.flatten()\n",
    "        pred_f = pred.flatten()\n",
    "        mse = np.mean((obs_f - pred_f) ** 2)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        ss_total = np.sum((obs_f - obs_f.mean()) ** 2)\n",
    "        ss_residual = np.sum((obs_f - pred_f) ** 2)\n",
    "        r_squared = float(1.0 - (ss_residual / (ss_total + EPS)))\n",
    "        return {'rmse': rmse, 'r2': r_squared}\n",
    "\n",
    "# Core function: runs grid search for det1 on a single aggregated cluster\n",
    "def run_beta_grid_on_cluster(agg_df, coord_df, cost_func, det_func_factory, beta_values, gravity_model_func, metrics_func):\n",
    "    \"\"\"\n",
    "    Returns best = {'beta', 'metrics', 'pred_df', 'diag'} using primary=r2, secondary=-nrmse\n",
    "    \"\"\"\n",
    "    common = [int(x) for x in agg_df.index.tolist()]\n",
    "    lons = coord_df.loc[common, 'lon'].to_numpy(dtype=float)\n",
    "    lats = coord_df.loc[common, 'lat'].to_numpy(dtype=float)\n",
    "    cost_m = cost_func(lons, lats)\n",
    "\n",
    "    O = agg_df.sum(axis=1).to_numpy()\n",
    "    D = agg_df.sum(axis=0).to_numpy()\n",
    "\n",
    "    best = {'beta': None, 'metrics': None, 'pred_df': None, 'diag': None}\n",
    "    best_score = None\n",
    "\n",
    "    for beta in beta_values:\n",
    "        det = det_func_factory(cost_m, beta=beta)\n",
    "        try:\n",
    "            Tij, diag = gravity_model_func(O.copy(), D.copy(), det)\n",
    "        except Exception:\n",
    "            # solver failure for this beta — skip\n",
    "            continue\n",
    "        pred_df = pd.DataFrame(Tij, index=agg_df.index, columns=agg_df.columns)\n",
    "        mets = metrics_func(pred_df, agg_df)    # should include 'r2'\n",
    "        nr = normalized_rmse(pred_df, agg_df)\n",
    "        mets.update(nr)\n",
    "        score = (mets.get('r2', -9999), - (mets.get('nrmse', np.inf) if np.isfinite(mets.get('nrmse', np.inf)) else np.inf))\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best = {'beta': float(beta), 'metrics': mets, 'pred_df': pred_df.copy(), 'diag': diag}\n",
    "    return best\n",
    "\n",
    "# DET1 factory (exponential), no det2 per your request\n",
    "def det1_factory(costm, beta):\n",
    "    return np.exp(-beta * costm)\n",
    "\n",
    "# Containers for results\n",
    "best_betas = {}          # {cluster: {'beta':..., 'metrics':..., 'diag':...}}\n",
    "predicted_matrices = {}  # {cluster: predicted_df}\n",
    "\n",
    "# Compute number of calendar days in the combined dataset (used to convert cluster totals -> avg per calendar day)\n",
    "try:\n",
    "    n_days = int(combined['started_at'].dt.date.nunique())\n",
    "except Exception:\n",
    "    # fallback: if combined not present or started_at not parsed, set to 1 (no scaling)\n",
    "    n_days = 1\n",
    "\n",
    "clusters = sorted(hour_cluster_df['cluster'].unique())\n",
    "\n",
    "for c in clusters:\n",
    "    hrs = hour_cluster_df[hour_cluster_df['cluster']==c]['hour'].tolist()\n",
    "    # aggregate OD across hours in this cluster\n",
    "    agg = None\n",
    "    for h in hrs:\n",
    "        p = hourly_pivots[h]\n",
    "        if agg is None:\n",
    "            agg = p.copy()\n",
    "        else:\n",
    "            agg = agg.add(p, fill_value=0)\n",
    "    if agg is None:\n",
    "        print(f'Cluster {c}: empty aggregation, skipping')\n",
    "        continue\n",
    "    agg = agg.fillna(0)\n",
    "\n",
    "    # --- NEW: convert aggregated cluster totals to average trips per calendar day ---\n",
    "    if n_days > 1:\n",
    "        agg = agg / float(n_days)\n",
    "    # save the averaged cluster OD for diagnostic purposes\n",
    "    agg.to_csv(OUTPUT_DIR / f'cluster_od_avg_per_day_{c}.csv')\n",
    "\n",
    "    # square matrix by intersection of rows & cols\n",
    "    rows = [int(x) for x in agg.index.tolist()]\n",
    "    cols = [int(x) for x in agg.columns.tolist()]\n",
    "    common = sorted(list(set(rows) & set(cols)))\n",
    "    if len(common) < 2:\n",
    "        print(f'Cluster {c} skipped (too few common stations)')\n",
    "        continue\n",
    "    agg = agg.reindex(index=common, columns=common).fillna(0)\n",
    "\n",
    "    # build coordinate DF for these common stations (median of start/end coords)\n",
    "    lon_cols = [col for col in combined.columns if 'longitude' in col.lower()]\n",
    "    lat_cols = [col for col in combined.columns if 'latitude' in col.lower()]\n",
    "    start_lon = next((col for col in lon_cols if col.lower().startswith('start')), None)\n",
    "    start_lat = next((col for col in lat_cols if col.lower().startswith('start')), None)\n",
    "    end_lon = next((col for col in lon_cols if col.lower().startswith('end')), None)\n",
    "    end_lat = next((col for col in lat_cols if col.lower().startswith('end')), None)\n",
    "    coord_parts = []\n",
    "    if start_lon and start_lat:\n",
    "        tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "        tmp = tmp.rename(columns={start_lon:'lon', start_lat:'lat'})\n",
    "        coord_parts.append(tmp)\n",
    "    if end_lon and end_lat:\n",
    "        tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "        tmp2 = tmp2.rename(columns={end_lon:'lon', end_lat:'lat'})\n",
    "        coord_parts.append(tmp2)\n",
    "    if not coord_parts:\n",
    "        raise RuntimeError('Could not find station longitude/latitude columns in `combined`.')\n",
    "\n",
    "    coord_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "    coord_df = coord_df.reindex(common).dropna()\n",
    "\n",
    "    if len(coord_df) != len(common):\n",
    "        missing = set(common) - set(coord_df.index.astype(int).tolist())\n",
    "        print(f'Cluster {c}: dropping {len(missing)} stations missing coords (sample): {list(missing)[:6]}')\n",
    "        keep = [s for s in common if s not in missing]\n",
    "        agg = agg.reindex(index=keep, columns=keep).fillna(0)\n",
    "        coord_df = coord_df.reindex(keep)\n",
    "        common = keep\n",
    "        if len(common) < 2:\n",
    "            print(f'Cluster {c}: too few stations after dropping coords, skipping')\n",
    "            continue\n",
    "\n",
    "    # run grid search for det1 only\n",
    "    best1 = run_beta_grid_on_cluster(agg, coord_df, haversine_pairwise, det1_factory, beta_grid, gravity_model, calculate_metrics)\n",
    "\n",
    "    # save best1 outputs\n",
    "    if best1['beta'] is None:\n",
    "        print(f'Cluster {c}: no successful beta found (solver failed for all candidates)')\n",
    "        continue\n",
    "\n",
    "    best_betas[c] = {'beta_det1': best1['beta'], 'metrics_det1': best1['metrics'], 'diag_det1': best1['diag']}\n",
    "    predicted_matrices[c] = best1['pred_df']\n",
    "\n",
    "    # persist to disk\n",
    "    best1['pred_df'].to_csv(OUTPUT_DIR / f'best_pred_det1_clusterr_{c}.csv')\n",
    "    pd.DataFrame([best1['metrics']]).to_csv(OUTPUT_DIR / f'best_metrics_det1_clusterr_{c}.csv', index=False)\n",
    "\n",
    "    print(f\"Cluster {c}: done. best_beta_det1={best1['beta']:.2e}, metrics={best1['metrics']}\")\n",
    "\n",
    "# save summary CSV\n",
    "summary = []\n",
    "for c, info in best_betas.items():\n",
    "    summary.append({'cluster': c, 'best_beta_det1': info['beta_det1'], **{f\"metrics_{k}\": v for k,v in info['metrics_det1'].items()}})\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv', index=False)\n",
    "\n",
    "print('Grid search (det1 only) completed. Summary saved to:', OUTPUT_DIR / 'beta_gridsearch_summary_det1_only.csv')\n",
    "print('Best betas dict: variable `best_betas` (in memory).')\n",
    "print('Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).')"
   ],
   "id": "37462ec51c3d6ba2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: done. best_beta_det1=3.16e-04, metrics={'rmse': 0.01511597184838509, 'r2': 0.6023978664038023, 'nrmse': 3.3690183047494133}\n",
      "Cluster 1: done. best_beta_det1=4.64e-04, metrics={'rmse': 0.0068731200005717336, 'r2': 0.583722459017577, 'nrmse': 3.1473002820950526}\n",
      "Cluster 2: done. best_beta_det1=6.81e-04, metrics={'rmse': 0.007998473455017589, 'r2': 0.6368246502400101, 'nrmse': 2.905371397098946}\n",
      "Cluster 3: done. best_beta_det1=4.64e-04, metrics={'rmse': 0.0085127296413361, 'r2': 0.5348114533084086, 'nrmse': 3.1500856842441234}\n",
      "Grid search (det1 only) completed. Summary saved to: E:\\Uni_PGT\\visualisation_outputs\\clustered_gravity\\beta_gridsearch_summary_det1_only.csv\n",
      "Best betas dict: variable `best_betas` (in memory).\n",
      "Predicted OD DataFrames per cluster: variable `predicted_matrices` (in memory).\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:04.348829Z",
     "start_time": "2025-11-07T12:55:04.182874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "station_to_poi_od_from_predicted_fixed.py\n",
    "\n",
    "Fixed version of the POI-OD conversion that uses gravity-model predicted\n",
    "station->station OD matrices (per cluster), and avoids the ValueError you ran\n",
    "into by correctly sizing the POI index for each output matrix.\n",
    "\n",
    "Assumptions:\n",
    " - `predicted_matrices` is a dict in memory mapping cluster -> pandas.DataFrame\n",
    "   with station IDs as index and columns (square) containing predicted Tij counts.\n",
    " - `combined` DataFrame is in memory (for station coords).\n",
    " - POIs CSV is at POI_FILE and contains columns ['lat','lon','category'] and\n",
    "   optionally 'poi_id'.\n",
    "\n",
    "Saves per-cluster POI OD CSVs (only for POIs that receive >0 allocation for\n",
    "stations in that cluster), plus an aggregated POI OD across clusters.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "CATEGORY_WEIGHTS = {\n",
    "    'library': 1.1,\n",
    "    'school': 1.3,\n",
    "    'university': 1.5,\n",
    "    'residential': 1.0,\n",
    "    'commercial': 1.2,\n",
    "    'hospital': 1.4\n",
    "}\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# --- sanity checks: required in-memory objects ---\n",
    "try:\n",
    "    predicted_matrices\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`predicted_matrices` not found in memory. Run gravity grid search first.\")\n",
    "\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`combined` not found in memory. Load your combined trips dataframe.\")\n",
    "\n",
    "# --- load POIs ---\n",
    "pois = pd.read_csv(POI_FILE)\n",
    "pois = pois.dropna(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['category'] = pois['category'].astype(str).fillna('commercial').str.lower().str.strip()\n",
    "pois['weight'] = pois['category'].map(CATEGORY_WEIGHTS).fillna(1.0)"
   ],
   "id": "8d08ba7ec5dac7e0",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:06.116048Z",
     "start_time": "2025-11-07T12:55:04.348829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- load POIs ---\n",
    "pois = pd.read_csv(POI_FILE)\n",
    "pois = pois.dropna(subset=['lat', 'lon']).reset_index(drop=True)\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['category'] = pois['category'].astype(str).fillna('commercial').str.lower().str.strip()\n",
    "pois['weight'] = pois['category'].map(CATEGORY_WEIGHTS).fillna(1.0)\n",
    "\n",
    "# --- MEMORY-EFFICIENT DBSCAN CLUSTERING ---\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "print(\"DBSCAN clustering with memory optimization...\")\n",
    "\n",
    "original_pois = pois.copy()\n",
    "clustered_pois_list = []\n",
    "current_poi_id = 0\n",
    "\n",
    "# Process in smaller batches by category to reduce memory usage\n",
    "for category in pois['category'].unique():\n",
    "    category_pois = pois[pois['category'] == category].copy().reset_index(drop=True)\n",
    "    \n",
    "    if len(category_pois) <= 1:\n",
    "        # Single POI in this category - keep as is\n",
    "        for _, poi in category_pois.iterrows():\n",
    "            clustered_pois_list.append({\n",
    "                'poi_id': current_poi_id,\n",
    "                'lat': poi['lat'],\n",
    "                'lon': poi['lon'],\n",
    "                'category': category,\n",
    "                'weight': poi['weight'],\n",
    "                'original_poi_count': 1,\n",
    "                'original_poi_ids': [poi['poi_id']]\n",
    "            })\n",
    "            current_poi_id += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {category}: {len(category_pois)} POIs\")\n",
    "    \n",
    "    # Convert to radians for haversine distance\n",
    "    coords_rad = np.radians(category_pois[['lat', 'lon']].values)\n",
    "    \n",
    "    # Use DBSCAN with optimized parameters\n",
    "    # eps in radians: 200 meters / Earth radius\n",
    "    eps_rad = 30 / 6371000\n",
    "    \n",
    "    # Use algorithm='ball_tree' for better memory efficiency with haversine\n",
    "    dbscan = DBSCAN(\n",
    "        eps=eps_rad,\n",
    "        min_samples=1,  # Every point forms a cluster\n",
    "        metric='haversine',\n",
    "        algorithm='ball_tree',  # More memory efficient for haversine\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Fit and get labels\n",
    "    labels = dbscan.fit_predict(coords_rad)\n",
    "    \n",
    "    # Create clusters from labels\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(i)\n",
    "    \n",
    "    # Create clustered POIs\n",
    "    for label, indices in clusters.items():\n",
    "        cluster_data = category_pois.iloc[indices]\n",
    "        \n",
    "        if len(cluster_data) == 1:\n",
    "            poi = cluster_data.iloc[0]\n",
    "            clustered_pois_list.append({\n",
    "                'poi_id': current_poi_id,\n",
    "                'lat': poi['lat'], 'lon': poi['lon'],\n",
    "                'category': category, 'weight': poi['weight'],\n",
    "                'original_poi_count': 1,\n",
    "                'original_poi_ids': [poi['poi_id']]\n",
    "            })\n",
    "        else:\n",
    "            clustered_pois_list.append({\n",
    "                'poi_id': current_poi_id,\n",
    "                'lat': cluster_data['lat'].mean(),\n",
    "                'lon': cluster_data['lon'].mean(),\n",
    "                'category': category,\n",
    "                'weight': cluster_data['weight'].sum(),\n",
    "                'original_poi_count': len(cluster_data),\n",
    "                'original_poi_ids': cluster_data['poi_id'].tolist()\n",
    "            })\n",
    "        current_poi_id += 1\n",
    "\n",
    "pois = pd.DataFrame(clustered_pois_list)\n",
    "print(f\"Reduced from {len(original_pois)} to {len(pois)} POI clusters\")\n",
    "pois.to_csv(r'E:\\Uni_PGT\\visualisation_outputs\\reduced_pois.csv', index=False)\n",
    "print('Saved reduced pois to', r'E:\\Uni_PGT\\visualisation_outputs\\reduced_pois.csv')\n",
    "# --- build station coords (median of start/end) ---\n",
    "print(original_pois.head())\n",
    "print(pois.head())"
   ],
   "id": "9f4a85be77f4d896",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN clustering with memory optimization...\n",
      "  Processing library: 49 POIs\n",
      "  Processing school: 188 POIs\n",
      "  Processing university: 28 POIs\n",
      "  Processing residential: 32837 POIs\n",
      "  Processing commercial: 551 POIs\n",
      "  Processing hospital: 16 POIs\n",
      "Reduced from 33669 to 5926 POI clusters\n",
      "Saved reduced pois to E:\\Uni_PGT\\visualisation_outputs\\reduced_pois.csv\n",
      "                                name                       geometry  \\\n",
      "0              Wester Hailes Library  POINT (-3.2851455 55.9162285)   \n",
      "1          Pirniehall Primary School  POINT (-3.2510739 55.9737056)   \n",
      "2               Corstorphine Library  POINT (-3.2810362 55.9407099)   \n",
      "3  Fettes College Preparatory School  POINT (-3.2259783 55.9666246)   \n",
      "4                           Haywired  POINT (-3.1233362 55.9344588)   \n",
      "\n",
      "         lat       lon category  poi_id  weight  \n",
      "0  55.916229 -3.285146  library       0     1.1  \n",
      "1  55.973706 -3.251074   school       1     1.3  \n",
      "2  55.940710 -3.281036  library       2     1.1  \n",
      "3  55.966625 -3.225978   school       3     1.3  \n",
      "4  55.934459 -3.123336   school       4     1.3  \n",
      "   poi_id        lat       lon category  weight  original_poi_count  \\\n",
      "0       0  55.916229 -3.285146  library     1.1                   1   \n",
      "1       1  55.940710 -3.281036  library     1.1                   1   \n",
      "2       2  55.942745 -3.056334  library     1.1                   1   \n",
      "3       3  55.947205 -3.186909  library     1.1                   1   \n",
      "4       4  55.933194 -3.136747  library     1.1                   1   \n",
      "\n",
      "  original_poi_ids  \n",
      "0              [0]  \n",
      "1              [2]  \n",
      "2              [5]  \n",
      "3              [7]  \n",
      "4             [10]  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:07.294944Z",
     "start_time": "2025-11-07T12:55:06.116048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# --- build station coords (median of start/end) ---\n",
    "lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "\n",
    "coord_parts = []\n",
    "if start_lon and start_lat:\n",
    "    tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "    tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "    coord_parts.append(tmp)\n",
    "if end_lon and end_lat:\n",
    "    tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "    tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "    coord_parts.append(tmp2)\n",
    "if not coord_parts:\n",
    "    raise RuntimeError('No station lon/lat columns found in `combined`.')\n",
    "\n",
    "stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "stations_df = stations_df.rename_axis('station_id')\n",
    "stations_df.index = stations_df.index.astype(int)\n",
    "# restrict to stations present in any predicted matrix to avoid waste\n",
    "pred_station_ids = set()\n",
    "for df in predicted_matrices.values():\n",
    "    pred_station_ids.update([int(x) for x in df.index.astype(int).tolist()])\n",
    "stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "if stations_df.empty:\n",
    "    raise RuntimeError('No station coordinates available for stations present in predicted_matrices.')\n",
    "station_ids = stations_df.index.astype(int).tolist()\n",
    "\n",
    "# --- helper: haversine (vectorized) ---\n",
    "def haversine_matrix(lonA, latA, lonB, latB):\n",
    "    R = 6371000.0\n",
    "    lonA = np.radians(np.asarray(lonA, dtype=float))\n",
    "    latA = np.radians(np.asarray(latA, dtype=float))\n",
    "    lonB = np.radians(np.asarray(lonB, dtype=float))\n",
    "    latB = np.radians(np.asarray(latB, dtype=float))\n",
    "    dlon = lonA[:, None] - lonB[None, :]\n",
    "    dlat = latA[:, None] - latB[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(latA)[:, None] * np.cos(latB)[None, :] * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# --- precompute nearest POIs for stations using haversine via sklearn's haversine metric ---\n",
    "poi_coords = pois[['lat', 'lon']].to_numpy()\n",
    "station_coords = stations_df[['lat', 'lon']].to_numpy()\n",
    "\n",
    "# use scikit-learn NearestNeighbors with haversine (expects radians)\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "nbrs.fit(np.radians(poi_coords))\n",
    "dists_r, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "# convert radian distances to meters\n",
    "dists_m = dists_r * 6371000.0\n",
    "\n",
    "# --- build allocations: station_id -> dict(poi_index -> weight) ---\n",
    "allocations = {}\n",
    "for i, sid in enumerate(station_ids):\n",
    "    dists = dists_m[i]\n",
    "    idx = idxs[i]\n",
    "    # choose close ones if any inside CLOSE_RADIUS_METERS\n",
    "    mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "    if mask_close.any():\n",
    "        chosen_idx = idx[mask_close]\n",
    "        chosen_dists = dists[mask_close]\n",
    "    else:\n",
    "        chosen_idx = idx\n",
    "        chosen_dists = dists\n",
    "\n",
    "    cat_weights = pois.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "    inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "    raw_scores = cat_weights * inv_dist\n",
    "    if raw_scores.sum() <= 0:\n",
    "        weights = np.ones_like(raw_scores) / len(raw_scores)\n",
    "    else:\n",
    "        weights = raw_scores / raw_scores.sum()\n",
    "\n",
    "    allocations[int(sid)] = {int(p_idx): float(w) for p_idx, w in zip(chosen_idx.tolist(), weights.tolist())}\n",
    "\n",
    "# Save allocations (flat)\n",
    "alloc_rows = []\n",
    "for s, pdict in allocations.items():\n",
    "    for pidx, w in pdict.items():\n",
    "        alloc_rows.append({'station_id': int(s), 'poi_index': int(pidx), 'poi_id': int(pois.at[pidx, 'poi_id']), 'weight': float(w)})\n",
    "alloc_df = pd.DataFrame(alloc_rows)\n",
    "alloc_df.to_csv(OUTPUT_DIR / 'station_poi_allocations.csv', index=False)\n",
    "print('Saved station->POI allocation table to', OUTPUT_DIR / 'station_poi_allocations.csv')\n",
    "\n",
    "# --- Convert predicted station OD -> POI OD per cluster (using only used POIs per cluster) ---\n",
    "poi_od_per_cluster = {}\n",
    "for c, pred_df in predicted_matrices.items():\n",
    "    # ensure pred_df index/cols are ints\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df.index = pred_df.index.astype(int)\n",
    "    pred_df.columns = pred_df.columns.astype(int)\n",
    "\n",
    "    # intersect stations with allocations\n",
    "    common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "    if len(common_stations) < 2:\n",
    "        print(f'Cluster {c}: too few stations with allocations ({len(common_stations)}), skipping')\n",
    "        continue\n",
    "\n",
    "    # build station order and Tij matrix accordingly\n",
    "    Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "    # build station->poi allocation matrix A (nS x nP_used)\n",
    "    # find all POI indices used by these stations\n",
    "    poi_indices_used = sorted({pidx for s in common_stations for pidx in allocations[s].keys()})\n",
    "    if len(poi_indices_used) == 0:\n",
    "        print(f'Cluster {c}: no POIs assigned to these stations, skipping')\n",
    "        continue\n",
    "\n",
    "    nS = len(common_stations)\n",
    "    nP = len(poi_indices_used)\n",
    "    A = np.zeros((nS, nP), dtype=float)\n",
    "    station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "    poi_to_col = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "\n",
    "    for s in common_stations:\n",
    "        r = station_to_row[s]\n",
    "        for pidx, w in allocations[s].items():\n",
    "            if pidx in poi_to_col:\n",
    "                A[r, poi_to_col[pidx]] = w\n",
    "\n",
    "    # compute POI OD: P = A^T * Tij * A\n",
    "    P = A.T.dot(Tij).dot(A)\n",
    "\n",
    "    # build DataFrame: rows/cols labelled by poi_id (not global pois index)\n",
    "    poi_ids = [int(pois.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "    poi_od_df = pd.DataFrame(P, index=poi_ids, columns=poi_ids)\n",
    "    poi_od_per_cluster[c] = poi_od_df\n",
    "    poi_od_df.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}F.csv')\n",
    "    print(f'Cluster {c}: saved POI OD with shape {poi_od_df.shape} (n_pois={len(poi_ids)})')\n",
    "\n",
    "# --- aggregate across clusters (align on poi_id union) ---\n",
    "if poi_od_per_cluster:\n",
    "    all_poi_ids = sorted({pid for df in poi_od_per_cluster.values() for pid in df.index.tolist()})\n",
    "    agg_mat = pd.DataFrame(0.0, index=all_poi_ids, columns=all_poi_ids)\n",
    "    for c, df in poi_od_per_cluster.items():\n",
    "        agg_mat = agg_mat.add(df.reindex(index=all_poi_ids, columns=all_poi_ids, fill_value=0), fill_value=0)\n",
    "    agg_mat.to_csv(OUTPUT_DIR / 'poi_od_aggregated_all_clusters_F.csv')\n",
    "    print('Saved aggregated POI OD for all clusters to', OUTPUT_DIR / 'poi_od_aggregated_all_clusters.csv')\n",
    "else:\n",
    "    print('No POI OD outputs (no clusters had usable data).')\n",
    "\n",
    "print('Done. Outputs in', OUTPUT_DIR)\n"
   ],
   "id": "69ebab7b73aa6ade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved station->POI allocation table to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\station_poi_allocations.csv\n",
      "Cluster 0: saved POI OD with shape (341, 341) (n_pois=341)\n",
      "Cluster 1: saved POI OD with shape (330, 330) (n_pois=330)\n",
      "Cluster 2: saved POI OD with shape (329, 329) (n_pois=329)\n",
      "Cluster 3: saved POI OD with shape (335, 335) (n_pois=335)\n",
      "Saved aggregated POI OD for all clusters to E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\\poi_od_aggregated_all_clusters.csv\n",
      "Done. Outputs in E:\\Uni_PGT\\visualisation_outputs\\poi_od_fixed\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:25.823267Z",
     "start_time": "2025-11-07T12:55:07.294944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# poi_od_sparse_with_baseline.py\n",
    "#\n",
    "# Memory-efficient conversion of predicted station->station OD (per cluster)\n",
    "# into POI->POI OD using sparse matrices and a \"baseline-to-k-nearest\" strategy\n",
    "# for neglected POIs so we avoid creating a dense 33k x 33k matrix.\n",
    "#\n",
    "# Requirements: scipy, scikit-learn, pandas, numpy\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "POI_FILE = r'E:\\Uni_PGT\\Express\\edinburgh_pois.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_NEIGHBORS = 3                # used for station->poi allocation\n",
    "K_BASELINE_NEIGH = 10          # number of POIs to distribute baseline for neglected POI\n",
    "CLOSE_RADIUS_METERS = 500\n",
    "CATEGORY_WEIGHTS = {\n",
    "    'library': 1.1,\n",
    "    'school': 1.3,\n",
    "    'university': 1.5,\n",
    "    'residential': 1.0,\n",
    "    'commercial': 1.2,\n",
    "    'hospital': 1.4\n",
    "}\n",
    "BASELINE_RATIO = 0.1   # baseline magnitude relative to mean observed cell\n",
    "EPS = 1e-12\n",
    "# ---------------------------------\n",
    "\n",
    "# sanity: required objects\n",
    "try:\n",
    "    predicted_matrices\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`predicted_matrices` not in memory. Run gravity step first.\")\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"`combined` not in memory. Load combined dataframe first.\")\n",
    "\n",
    "# load POIs\n",
    "\n",
    "# canonical lists and lookups\n",
    "all_poi_ids = pois['poi_id'].astype(int).tolist()\n",
    "poi_index_to_id = {i: int(pois.at[i, 'poi_id']) for i in pois.index}\n",
    "poi_id_to_index = {int(pois.at[i, 'poi_id']): i for i in pois.index}\n",
    "\n",
    "n_all = len(all_poi_ids)\n",
    "print(f\"POIs loaded: {n_all} (will keep results sparse)\")\n",
    "\n",
    "# build station coords as median of start/end\n",
    "lon_cols = [c for c in combined.columns if 'longitude' in c.lower()]\n",
    "lat_cols = [c for c in combined.columns if 'latitude' in c.lower()]\n",
    "start_lon = next((c for c in lon_cols if c.lower().startswith('start')), None)\n",
    "start_lat = next((c for c in lat_cols if c.lower().startswith('start')), None)\n",
    "end_lon = next((c for c in lon_cols if c.lower().startswith('end')), None)\n",
    "end_lat = next((c for c in lat_cols if c.lower().startswith('end')), None)\n",
    "coord_parts = []\n",
    "if start_lon and start_lat:\n",
    "    tmp = combined.groupby('start_station_id')[[start_lon, start_lat]].median()\n",
    "    tmp = tmp.rename(columns={start_lon: 'lon', start_lat: 'lat'})\n",
    "    coord_parts.append(tmp)\n",
    "if end_lon and end_lat:\n",
    "    tmp2 = combined.groupby('end_station_id')[[end_lon, end_lat]].median()\n",
    "    tmp2 = tmp2.rename(columns={end_lon: 'lon', end_lat: 'lat'})\n",
    "    coord_parts.append(tmp2)\n",
    "if not coord_parts:\n",
    "    raise RuntimeError('No station lon/lat columns found in combined')\n",
    "stations_df = pd.concat(coord_parts).groupby(level=0).first()\n",
    "stations_df.index = stations_df.index.astype(int)\n",
    "\n",
    "# restrict to stations that appear in at least one predicted matrix\n",
    "pred_station_ids = set()\n",
    "for m in predicted_matrices.values():\n",
    "    pred_station_ids.update([int(x) for x in m.index.astype(int).tolist()])\n",
    "stations_df = stations_df.reindex(sorted(pred_station_ids)).dropna()\n",
    "station_ids = stations_df.index.astype(int).tolist()\n",
    "print(f\"Using {len(station_ids)} stations for allocation\")\n",
    "\n",
    "# precompute station->nearest POIs (small K)\n",
    "poi_coords = pois[['lat','lon']].to_numpy()\n",
    "station_coords = stations_df[['lat','lon']].to_numpy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=K_NEIGHBORS, metric='haversine')\n",
    "nbrs.fit(np.radians(poi_coords))\n",
    "d_radians, idxs = nbrs.kneighbors(np.radians(station_coords))\n",
    "dists_m = d_radians * 6371000.0\n",
    "\n",
    "# build allocations mapping station_id -> list of (poi_index, weight)\n",
    "allocations = {}\n",
    "for i, sid in enumerate(station_ids):\n",
    "    dists = dists_m[i]\n",
    "    idx = idxs[i]\n",
    "    mask_close = dists <= CLOSE_RADIUS_METERS\n",
    "    if mask_close.any():\n",
    "        chosen_idx = idx[mask_close]\n",
    "        chosen_dists = dists[mask_close]\n",
    "    else:\n",
    "        chosen_idx = idx\n",
    "        chosen_dists = dists\n",
    "    cat_weights = pois.loc[chosen_idx, 'weight'].to_numpy(dtype=float)\n",
    "    inv_dist = 1.0 / (chosen_dists + EPS)\n",
    "    raw = cat_weights * inv_dist\n",
    "    if raw.sum() <= 0:\n",
    "        w = np.ones_like(raw) / len(raw)\n",
    "    else:\n",
    "        w = raw / raw.sum()\n",
    "    allocations[int(sid)] = list(zip([int(p) for p in chosen_idx.tolist()], [float(x) for x in w.tolist()]))\n",
    "\n",
    "# Save allocations summary\n",
    "alloc_rows = []\n",
    "for s, lst in allocations.items():\n",
    "    for pidx, w in lst:\n",
    "        alloc_rows.append({'station_id': s, 'poi_index': int(pidx), 'poi_id': int(pois.at[pidx,'poi_id']), 'weight': w})\n",
    "alloc_df = pd.DataFrame(alloc_rows)\n",
    "alloc_df.to_csv(OUTPUT_DIR / 'station_poi_allocations.csv', index=False)\n",
    "print('Saved station->POI allocations (small)')\n",
    "\n",
    "# Build POI neighbor index for baseline distribution (k baseline neighbors on POI network)\n",
    "poi_nbrs = NearestNeighbors(n_neighbors=K_BASELINE_NEIGH, metric='haversine')\n",
    "poi_nbrs.fit(np.radians(poi_coords))\n",
    "poi_d_rad, poi_idxs = poi_nbrs.kneighbors(np.radians(poi_coords))\n",
    "poi_d_m = poi_d_rad * 6371000.0\n",
    "\n",
    "# helper: function to add entries to sparse accumulator lists\n",
    "\n",
    "def append_entries(rows_list, cols_list, data_list, from_poi_ids, to_poi_ids, values_matrix):\n",
    "    # from_poi_ids and to_poi_ids are lists of global poi_id labels (not dataframe indices)\n",
    "    for i, pid in enumerate(from_poi_ids):\n",
    "        for j, qid in enumerate(to_poi_ids):\n",
    "            val = values_matrix[i, j]\n",
    "            if val != 0 and not np.isnan(val):\n",
    "                rows_list.append(poi_id_to_index[pid])\n",
    "                cols_list.append(poi_id_to_index[qid])\n",
    "                data_list.append(float(val))\n",
    "\n",
    "# main conversion loop: produce sparse COO components per cluster\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "poi_od_sparse_per_cluster = {}\n",
    "for c, pred_df in predicted_matrices.items():\n",
    "    print(f'Processing cluster {c}...')\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df.index = pred_df.index.astype(int)\n",
    "    pred_df.columns = pred_df.columns.astype(int)\n",
    "\n",
    "    # stations in this predicted matrix that have allocations\n",
    "    common_stations = sorted(list(set(pred_df.index.tolist()) & set(allocations.keys())))\n",
    "    if len(common_stations) < 2:\n",
    "        print(f' Cluster {c}: too few stations with allocations, skipping')\n",
    "        continue\n",
    "\n",
    "    Tij = pred_df.reindex(index=common_stations, columns=common_stations).fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "    # build A_used (nS x nP_used) where nP_used is number of distinct POIs assigned to these stations\n",
    "    poi_indices_used = sorted({pidx for s in common_stations for pidx, _ in allocations[s]})\n",
    "    if len(poi_indices_used) == 0:\n",
    "        print(f' Cluster {c}: no POIs used by these stations, skipping')\n",
    "        continue\n",
    "\n",
    "    station_to_row = {s:i for i,s in enumerate(common_stations)}\n",
    "    poi_to_col_used = {p:i for i,p in enumerate(poi_indices_used)}\n",
    "    nS = len(common_stations); nP_used = len(poi_indices_used)\n",
    "    A_used = np.zeros((nS, nP_used), dtype=float)\n",
    "    for s in common_stations:\n",
    "        r = station_to_row[s]\n",
    "        for pidx, w in allocations[s]:\n",
    "            if pidx in poi_to_col_used:\n",
    "                A_used[r, poi_to_col_used[pidx]] = w\n",
    "\n",
    "    # compute P_used (nP_used x nP_used) — this is small (few hundreds)\n",
    "    P_used = A_used.T.dot(Tij).dot(A_used)\n",
    "\n",
    "    # Now map P_used into global sparse lists\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    # map used POI local -> global poi_id\n",
    "    poi_ids_used = [int(pois.at[pidx, 'poi_id']) for pidx in poi_indices_used]\n",
    "\n",
    "    # append P_used block into sparse lists\n",
    "    for i_local, global_pidx in enumerate(poi_indices_used):\n",
    "        pid = int(pois.at[global_pidx, 'poi_id'])\n",
    "        for j_local, global_qidx in enumerate(poi_indices_used):\n",
    "            qid = int(pois.at[global_qidx, 'poi_id'])\n",
    "            val = P_used[i_local, j_local]\n",
    "            if val != 0 and not np.isnan(val):\n",
    "                rows.append(poi_id_to_index[pid])\n",
    "                cols.append(poi_id_to_index[qid])\n",
    "                data.append(float(val))\n",
    "\n",
    "    # compute baseline per-cell scalar (mean observed cell over used block)\n",
    "    observed_sum = P_used.sum()\n",
    "    if observed_sum <= 0:\n",
    "        mean_cell = 0.0\n",
    "    else:\n",
    "        mean_cell = observed_sum / (n_all)  # normalized to full universe\n",
    "\n",
    "    mean_weight = pois['weight'].mean()\n",
    "\n",
    "    # identify neglected POIs (those that have no entries in current sparse lists)\n",
    "    used_poi_set = set(poi_ids_used)\n",
    "    all_poi_set = set(all_poi_ids)\n",
    "    neglected = sorted(list(all_poi_set - used_poi_set))\n",
    "    print(f' Cluster {c}: used POIs={len(used_poi_set)}, neglected POIs={len(neglected)}')\n",
    "\n",
    "    # For each neglected POI, distribute baseline to its K_BASELINE_NEIGH nearest POIs (by index in pois)\n",
    "    if len(neglected) > 0 and mean_cell > 0:\n",
    "        # we have precomputed poi_idxs and poi_d_m arrays (POI->neighbors)\n",
    "        for pid in neglected:\n",
    "            pidx = poi_id_to_index[pid]\n",
    "            # neighbors indices (including itself) from poi_idxs array\n",
    "            neigh_local = poi_idxs[pidx, 1:K_BASELINE_NEIGH+1]  # skip self (first neighbor)\n",
    "            neigh_d = poi_d_m[pidx, 1:K_BASELINE_NEIGH+1]\n",
    "            # weights: inverse distance * category weight\n",
    "            neigh_weights = pois.loc[neigh_local, 'weight'].to_numpy(dtype=float)\n",
    "            invd = 1.0 / (neigh_d + EPS)\n",
    "            raw = neigh_weights * invd\n",
    "            if raw.sum() <= 0:\n",
    "                wnorm = np.ones_like(raw) / len(raw)\n",
    "            else:\n",
    "                wnorm = raw / raw.sum()\n",
    "            # baseline total per neglected POI (outflow) distribute across neighbors\n",
    "            baseline_total = mean_cell * BASELINE_RATIO * (pois.at[pidx, 'weight'] / mean_weight)\n",
    "            # distribute baseline_total to outgoing links pid -> neigh\n",
    "            for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                val = baseline_total * wnorm[k_idx]\n",
    "                rows.append(pidx)\n",
    "                cols.append(neigh_idx)\n",
    "                data.append(float(val))\n",
    "            # similarly distribute baseline inflow: neighbors -> pid\n",
    "            for k_idx, neigh_idx in enumerate(neigh_local):\n",
    "                qid = int(pois.at[int(neigh_idx), 'poi_id'])\n",
    "                val = baseline_total * wnorm[k_idx]\n",
    "                rows.append(neigh_idx)\n",
    "                cols.append(pidx)\n",
    "                data.append(float(val))\n",
    "\n",
    "    # build sparse COO and save\n",
    "    coo = coo_matrix((data, (rows, cols)), shape=(n_all, n_all))\n",
    "    # compress to csr for smaller memory on disk\n",
    "    csr = coo.tocsr()\n",
    "    save_npz(OUTPUT_DIR / f'poi_od_cluster_{c}.npz', csr)\n",
    "\n",
    "    # also save edge-list CSV (only nonzero entries)\n",
    "    coo_nz = coo.tocoo()\n",
    "    edge_df = pd.DataFrame({'from_poi_index': coo_nz.row, 'to_poi_index': coo_nz.col, 'flow': coo_nz.data})\n",
    "    # map poi_index -> poi_id for readability\n",
    "    edge_df['from_poi_id'] = edge_df['from_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_df['to_poi_id'] = edge_df['to_poi_index'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_df = edge_df[['from_poi_id','to_poi_id','flow']]\n",
    "    edge_df.to_csv(OUTPUT_DIR / f'poi_od_cluster_{c}_edgelists.csv', index=False)\n",
    "\n",
    "    poi_od_sparse_per_cluster[c] = {'sparse': csr, 'edge_csv': OUTPUT_DIR / f'poi_od_cluster_{c}_edgelist.csv'}\n",
    "    print(f' Cluster {c}: saved sparse POI OD (nnz={csr.nnz})')\n",
    "\n",
    "# aggregate across clusters (sum sparse matrices)\n",
    "if poi_od_sparse_per_cluster:\n",
    "    first = True\n",
    "    agg_csr = None\n",
    "    for c, info in poi_od_sparse_per_cluster.items():\n",
    "        mat = info['sparse']\n",
    "        if first:\n",
    "            agg_csr = mat.copy()\n",
    "            first = False\n",
    "        else:\n",
    "            agg_csr = agg_csr + mat\n",
    "    # save aggregated\n",
    "    save_npz(OUTPUT_DIR / 'poi_od_aggregated_all_clusters_sparse.npz', agg_csr)\n",
    "    # also save aggregated edge list (may still be big) — write only top flows if desired\n",
    "    coo_agg = agg_csr.tocoo()\n",
    "    edge_agg = pd.DataFrame({'from_idx': coo_agg.row, 'to_idx': coo_agg.col, 'flow': coo_agg.data})\n",
    "    edge_agg['from_poi_id'] = edge_agg['from_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    edge_agg['to_poi_id'] = edge_agg['to_idx'].map(lambda x: int(pois.at[int(x),'poi_id']))\n",
    "    \n",
    "    edge_agg[['from_poi_id','to_poi_id','flow']].to_csv(OUTPUT_DIR / 'poi_od_aggregated_all_clusters_edgelistsF.csv', index=False)\n",
    "    print('Saved aggregated sparse POI OD and edge list')\n",
    "\n",
    "print('Done. Outputs (sparse .npz + edgelists) in', OUTPUT_DIR)\n"
   ],
   "id": "e67a8c942a632705",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POIs loaded: 5926 (will keep results sparse)\n",
      "Using 194 stations for allocation\n",
      "Saved station->POI allocations (small)\n",
      "Processing cluster 0...\n",
      " Cluster 0: used POIs=341, neglected POIs=5585\n",
      " Cluster 0: saved sparse POI OD (nnz=177457)\n",
      "Processing cluster 1...\n",
      " Cluster 1: used POIs=330, neglected POIs=5596\n",
      " Cluster 1: saved sparse POI OD (nnz=170174)\n",
      "Processing cluster 2...\n",
      " Cluster 2: used POIs=329, neglected POIs=5597\n",
      " Cluster 2: saved sparse POI OD (nnz=169533)\n",
      "Processing cluster 3...\n",
      " Cluster 3: used POIs=335, neglected POIs=5591\n",
      " Cluster 3: saved sparse POI OD (nnz=173471)\n",
      "Saved aggregated sparse POI OD and edge list\n",
      "Done. Outputs (sparse .npz + edgelists) in E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:55:25.829915Z",
     "start_time": "2025-11-07T12:55:25.823267Z"
    }
   },
   "cell_type": "code",
   "source": "print(baseline_total)",
   "id": "9c6e8dfb740fc567",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003907142268726043\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:59:38.210092Z",
     "start_time": "2025-11-07T12:59:37.356960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# kmeans_candidates_no_snap.py\n",
    "# Demand-weighted KMeans (NO snapping to POIs) to produce P candidate stations\n",
    "# Inputs:\n",
    "#  - POI_FILE: CSV with columns ['poi_id'(optional),'lat','lon',...]\n",
    "#  - POI_EDGELIST: combined POI edgelist CSV with columns [from_poi_id,to_poi_id,flow]\n",
    "# Outputs saved to OUTPUT_DIR:\n",
    "#  - candidate_stations_P{P}_kmeans_no_snap.csv (centroid lon/lat + demand stats)\n",
    "#  - candidate_eval_P{P}_kmeans_no_snap.csv\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# --------- USER CONFIG ----------\n",
    "\n",
    "POI_EDGELIST = r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\\poi_od_aggregated_all_clusters_edgelistsF.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\candidate_stations')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "P = 100                          # number of candidate centroids to produce\n",
    "METHOD_NAME = 'kmeans_no_snap'\n",
    "KMEANS_RANDOM_STATE = 42\n",
    "KM_INIT = 'k-means++'           # 'k-means++' or 'random'\n",
    "COVERAGE_RADIUS_METERS = 800.0  # evaluation radius for coverage (walk radius)\n",
    "# --------------------------------\n",
    "\n",
    "# Utility: small haversine helpers (meters)\n",
    "def haversine_matrix(lonsA, latsA, lonsB, latsB):\n",
    "    R = 6371000.0\n",
    "    lonA = np.radians(np.asarray(lonsA, dtype=float))\n",
    "    latA = np.radians(np.asarray(latsA, dtype=float))\n",
    "    lonB = np.radians(np.asarray(lonsB, dtype=float))\n",
    "    latB = np.radians(np.asarray(latsB, dtype=float))\n",
    "    dlon = lonA[:, None] - lonB[None, :]\n",
    "    dlat = latA[:, None] - latB[None, :]\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(latA)[:, None] * np.cos(latB)[None, :] * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "    return R * c\n",
    "\n",
    "# --- load POIs and compute demand per POI from edgelist ---\n",
    "\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['poi_id'] = pois['poi_id'].astype(int)\n",
    "\n",
    "print('Loading POI edgelist to compute demand per POI ...')\n",
    "edges = pd.read_csv(POI_EDGELIST)\n",
    "# detect flow column\n",
    "cols_lower = {c.lower(): c for c in edges.columns}\n",
    "flow_col = None\n",
    "for candidate in ('flow', 'demand', 'count', 'weight'):\n",
    "    if candidate in cols_lower:\n",
    "        flow_col = cols_lower[candidate]\n",
    "        break\n",
    "if flow_col is None:\n",
    "    numeric_cols = [c for c in edges.columns if np.issubdtype(edges[c].dtype, np.number)]\n",
    "    if numeric_cols:\n",
    "        flow_col = numeric_cols[-1]\n",
    "    else:\n",
    "        raise RuntimeError('Could not detect numeric flow column in edgelist.')\n",
    "print('Flow column detected:', flow_col)\n",
    "\n",
    "inflow = edges.groupby('to_poi_id')[flow_col].sum()\n",
    "outflow = edges.groupby('from_poi_id')[flow_col].sum()\n",
    "all_ids = sorted(set(inflow.index.tolist()) | set(outflow.index.tolist()) | set(pois['poi_id'].tolist()))\n",
    "\n",
    "# demand = inflow + outflow (symmetric importance for being origin/destination)\n",
    "demand_series = pd.Series(0.0, index=all_ids, dtype=float)\n",
    "if not inflow.empty:\n",
    "    demand_series.loc[inflow.index] += inflow\n",
    "if not outflow.empty:\n",
    "    demand_series.loc[outflow.index] += outflow\n",
    "\n",
    "poi_demand = pois.set_index('poi_id').reindex(all_ids).copy()\n",
    "poi_demand['demand'] = demand_series.reindex(poi_demand.index).fillna(0.0)\n",
    "\n",
    "poi_ids = poi_demand.index.astype(int).tolist()\n",
    "lats = poi_demand['lat'].to_numpy(dtype=float)\n",
    "lons = poi_demand['lon'].to_numpy(dtype=float)\n",
    "weights = poi_demand['demand'].to_numpy(dtype=float)\n",
    "\n",
    "total_demand = weights.sum()\n",
    "print(f'POIs loaded: {len(poi_ids)} POIs, total demand = {total_demand:.2f}')\n",
    "\n",
    "# avoid zero-weight issues\n",
    "weights_safe = weights.copy()\n",
    "weights_safe[weights_safe <= 0] = 1e-6\n",
    "\n",
    "# Project lon/lat to a simple local metric space (equirectangular approx) for KMeans\n",
    "mean_lat = np.mean(lats)\n",
    "xm = (lons - np.mean(lons)) * (111320 * np.cos(np.radians(mean_lat)))\n",
    "ym = (lats - np.mean(lats)) * 110540\n",
    "X_m = np.column_stack([xm, ym])\n",
    "\n",
    "# Run weighted KMeans (sample_weight)\n",
    "print('Running demand-weighted KMeans (no snapping) ...')\n",
    "km = KMeans(n_clusters=P, init=KM_INIT, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "km.fit(X_m, sample_weight=weights_safe)\n",
    "centers_m = km.cluster_centers_\n",
    "\n",
    "# Map centers back to lon/lat approx\n",
    "centers_lon = (centers_m[:,0] / (111320 * np.cos(np.radians(mean_lat)))) + np.mean(lons)\n",
    "centers_lat = (centers_m[:,1] / 110540) + np.mean(lats)\n",
    "\n",
    "candidates = pd.DataFrame({'centroid_lon': centers_lon, 'centroid_lat': centers_lat})\n",
    "candidates['method'] = METHOD_NAME\n",
    "candidates['cluster_label_kmeans'] = np.arange(len(candidates))\n",
    "\n",
    "# Evaluate coverage: distance from each POI to nearest centroid\n",
    "D = haversine_matrix(lons, lats, candidates['centroid_lon'].to_numpy(), candidates['centroid_lat'].to_numpy())\n",
    "min_dist = D.min(axis=1)\n",
    "within = (min_dist <= COVERAGE_RADIUS_METERS)\n",
    "covered_demand = weights[within].sum()\n",
    "coverage_fraction = covered_demand / (weights.sum() + 1e-12)\n",
    "avg_weighted_dist = np.sum(min_dist * weights) / (weights.sum() + 1e-12)\n",
    "median_dist = np.median(min_dist)\n",
    "\n",
    "eval_metrics = {\n",
    "    'n_candidates': len(candidates),\n",
    "    'coverage_fraction_demand': float(coverage_fraction),\n",
    "    'avg_weighted_distance_m': float(avg_weighted_dist),\n",
    "    'median_distance_m': float(median_dist),\n",
    "    'num_pois_total': len(poi_ids),\n",
    "    'total_demand': float(weights.sum())\n",
    "}\n",
    "\n",
    "# Save outputs\n",
    "cand_out = OUTPUT_DIR / f'candidate_stations_P{P}_{METHOD_NAME}.csv'\n",
    "candidates.to_csv(cand_out, index=False)\n",
    "pd.Series(eval_metrics).to_frame('value').to_csv(OUTPUT_DIR / f'candidate_eval_P{P}_{METHOD_NAME}.csv')\n",
    "\n",
    "print('Saved candidates to:', cand_out)\n",
    "print('Saved evaluation to:', OUTPUT_DIR / f'candidate_eval_P{P}_{METHOD_NAME}.csv')\n",
    "print('Done.')\n"
   ],
   "id": "45e28ff106e4d686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POI edgelist to compute demand per POI ...\n",
      "Flow column detected: flow\n",
      "POIs loaded: 5926 POIs, total demand = 1010.52\n",
      "Running demand-weighted KMeans (no snapping) ...\n",
      "Saved candidates to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_stations_P100_kmeans_no_snap.csv\n",
      "Saved evaluation to: E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidate_eval_P100_kmeans_no_snap.csv\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:59:58.725327Z",
     "start_time": "2025-11-07T12:59:48.286152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# map_candidates_no_snap_kmeans.py\n",
    "\"\"\"\n",
    "Create demand-weighted KMeans (NO SNAP) candidate locations (P centroids) and\n",
    "plot them on an interactive folium map (save HTML). This version DOES NOT snap\n",
    "centroids to POIs — it uses centroid lon/lat directly. It also handles the\n",
    "\"candidate file needs lat/lon or poi_id\" situation by checking the candidate\n",
    "CSV and joining to POIs when only poi_id is provided.\n",
    "\n",
    "Usage:\n",
    " - Edit the FILE PATHS in USER CONFIG.\n",
    " - Run this script in the same environment where pandas, numpy, sklearn and folium are installed.\n",
    "\n",
    "Outputs:\n",
    " - candidate CSV (with centroid_lon/centroid_lat)\n",
    " - interactive map HTML saved to OUTPUT_DIR\n",
    "\n",
    "Note: if you already produced candidates with a separate script, set\n",
    "CANDIDATE_FILE to that CSV. If it lacks centroid_lon/centroid_lat but has\n",
    "poi_id, the script will join to the POI file to get coordinates.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import folium\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ---------------- USER CONFIG ----------------\n",
    "POI_EDGELIST = r'E:\\Uni_PGT\\visualisation_outputs\\poi_od_sparse_baseline\\poi_od_aggregated_all_clusters_edgelistsF.csv'\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\visualisation_outputs\\candidate_stations')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# If you already have a candidate file from another step, set this; else set to None and script will create centroids\n",
    "CANDIDATE_FILE = None  # r'E:\\path\\to\\existing_candidates.csv'  # set to file if already produced\n",
    "\n",
    "P = 100\n",
    "KM_INIT = 'k-means++'\n",
    "KMEANS_RANDOM_STATE = 42\n",
    "# map settings\n",
    "MAP_CENTER = (55.95, -3.20)   # roughly Edinburgh\n",
    "MAP_ZOOM = 12\n",
    "\n",
    "EPS = 1e-9\n",
    "# ----------------------------------------------\n",
    "\n",
    "# --- helper: load POIs and compute demand per POI from edgelist ---\n",
    "\n",
    "if 'poi_id' not in pois.columns:\n",
    "    pois['poi_id'] = pois.index.astype(int)\n",
    "pois['poi_id'] = pois['poi_id'].astype(int)\n",
    "\n",
    "# load edgelist to compute demand per POI\n",
    "print('Loading POI edgelist (this may be large) ...')\n",
    "edges = pd.read_csv(POI_EDGELIST)\n",
    "# attempt to infer flow column\n",
    "cols_lower = {c.lower(): c for c in edges.columns}\n",
    "flow_col = None\n",
    "for cand in ('flow', 'demand', 'count', 'weight'):\n",
    "    if cand in cols_lower:\n",
    "        flow_col = cols_lower[cand]\n",
    "        break\n",
    "if flow_col is None:\n",
    "    numeric_cols = [c for c in edges.columns if np.issubdtype(edges[c].dtype, np.number)]\n",
    "    if numeric_cols:\n",
    "        flow_col = numeric_cols[-1]\n",
    "\n",
    "if flow_col is None:\n",
    "    raise RuntimeError('Could not detect numeric flow column in edgelist. Edit script to provide flow column name.')\n",
    "\n",
    "print('Flow column detected:', flow_col)\n",
    "\n",
    "inflow = edges.groupby('to_poi_id')[flow_col].sum()\n",
    "outflow = edges.groupby('from_poi_id')[flow_col].sum()\n",
    "all_ids = sorted(set(inflow.index.tolist()) | set(outflow.index.tolist()) | set(pois['poi_id'].tolist()))\n",
    "\n",
    "# demand = inflow + outflow\n",
    "demand_series = pd.Series(0.0, index=all_ids, dtype=float)\n",
    "demand_series.loc[inflow.index] += inflow\n",
    "demand_series.loc[outflow.index] += outflow\n",
    "\n",
    "poi_demand = pois.set_index('poi_id').reindex(all_ids).copy()\n",
    "poi_demand['demand'] = demand_series.reindex(poi_demand.index).fillna(0.0)\n",
    "\n",
    "poi_ids = poi_demand.index.astype(int).tolist()\n",
    "lats = poi_demand['lat'].to_numpy(dtype=float)\n",
    "lons = poi_demand['lon'].to_numpy(dtype=float)\n",
    "weights = poi_demand['demand'].to_numpy(dtype=float)\n",
    "\n",
    "# avoid zero sample_weight for KMeans\n",
    "weights_safe = weights.copy()\n",
    "weights_safe[weights_safe <= 0] = 1e-6\n",
    "\n",
    "# --- create candidates with NO SNAP (centroids in lon/lat space) ---\n",
    "if CANDIDATE_FILE is None:\n",
    "    print(f'Running weighted KMeans (P={P}) to create candidate centroids (NO SNAP) ...')\n",
    "    # project lon/lat to approximate meters for better geometric clustering\n",
    "    mean_lat = np.mean(lats)\n",
    "    xm = (lons - np.mean(lons)) * (111320 * np.cos(np.radians(mean_lat)))\n",
    "    ym = (lats - np.mean(lats)) * 110540\n",
    "    X_m = np.column_stack([xm, ym])\n",
    "\n",
    "    km = KMeans(n_clusters=P, init=KM_INIT, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    km.fit(X_m, sample_weight=weights_safe)\n",
    "    centers_m = km.cluster_centers_\n",
    "\n",
    "    # map centers back to lon/lat approx\n",
    "    centers_lon = (centers_m[:,0] / (111320 * np.cos(np.radians(mean_lat)))) + np.mean(lons)\n",
    "    centers_lat = (centers_m[:,1] / 110540) + np.mean(lats)\n",
    "\n",
    "    candidates = pd.DataFrame({\n",
    "        'candidate_id': range(len(centers_lon)),\n",
    "        'centroid_lon': centers_lon,\n",
    "        'centroid_lat': centers_lat\n",
    "    })\n",
    "    candidate_out = OUTPUT_DIR / f'candidates_kmeans_nosnap_P{P}.csv'\n",
    "    candidates.to_csv(candidate_out, index=False)\n",
    "    print('Saved candidate centroids to', candidate_out)\n",
    "else:\n",
    "    print('Loading existing candidate file:', CANDIDATE_FILE)\n",
    "    candidates = pd.read_csv(CANDIDATE_FILE)\n",
    "    candidate_out = Path(CANDIDATE_FILE)\n",
    "\n",
    "# --- Ensure we have lon/lat for plotting ---\n",
    "if 'centroid_lon' in candidates.columns and 'centroid_lat' in candidates.columns:\n",
    "    centroids_lon = candidates['centroid_lon'].to_numpy(dtype=float)\n",
    "    centroids_lat = candidates['centroid_lat'].to_numpy(dtype=float)\n",
    "elif 'lon' in candidates.columns and 'lat' in candidates.columns:\n",
    "    centroids_lon = candidates['lon'].to_numpy(dtype=float)\n",
    "    centroids_lat = candidates['lat'].to_numpy(dtype=float)\n",
    "elif 'poi_id' in candidates.columns:\n",
    "    # join to POIs to get lat/lon\n",
    "    print('Candidate file has poi_id but no lon/lat. Joining to POI file to obtain coordinates...')\n",
    "    join_df = candidates.merge(pois[['poi_id','lat','lon']], left_on='poi_id', right_on='poi_id', how='left')\n",
    "    if join_df['lat'].isna().any() or join_df['lon'].isna().any():\n",
    "        missing = join_df[join_df['lat'].isna() | join_df['lon'].isna()]\n",
    "        raise RuntimeError(f'Some candidate poi_id rows could not be matched to POIs (sample):\\n{missing.head().to_string()}')\n",
    "    centroids_lon = join_df['lon'].to_numpy(dtype=float)\n",
    "    centroids_lat = join_df['lat'].to_numpy(dtype=float)\n",
    "    # update candidates with centroid_* columns for consistency\n",
    "    candidates['centroid_lon'] = centroids_lon\n",
    "    candidates['centroid_lat'] = centroids_lat\n",
    "    candidates.to_csv(candidate_out, index=False)\n",
    "    print('Augmented candidate file with lon/lat and saved to', candidate_out)\n",
    "else:\n",
    "    raise RuntimeError('Candidate file needs centroid_lon/centroid_lat or poi_id to join to POIs. Edit candidate CSV.')\n",
    "\n",
    "# --- Build map with folium ---\n",
    "print('Building interactive map (folium) ...')\n",
    "map_center = MAP_CENTER\n",
    "m = folium.Map(location=map_center, zoom_start=MAP_ZOOM, tiles='cartodbpositron')\n",
    "\n",
    "# add POI layer (optional: show major POIs as light dots)\n",
    "# we will sample only POIs with demand > 0 to avoid overcrowding\n",
    "poi_sample = poi_demand.copy()\n",
    "poi_sample['demand_norm'] = poi_sample['demand'] / (poi_sample['demand'].max() + EPS)\n",
    "poi_layer = folium.FeatureGroup(name='POIs (demand)')\n",
    "for pid, row in poi_sample.iterrows():\n",
    "    if row['demand'] > 0:\n",
    "        folium.CircleMarker(location=(row['lat'], row['lon']), radius=2 + 6*row['demand_norm'],\n",
    "                            popup=f\"POI {int(pid)}: {row['demand']:.2f}\", fill=True, fill_opacity=0.6).add_to(poi_layer)\n",
    "# add layer but keep initially off for performance\n",
    "poi_layer.add_to(m)\n",
    "\n",
    "# add candidate centroids\n",
    "cand_layer = folium.FeatureGroup(name=f'Candidates (P={len(centroids_lon)})')\n",
    "for cid, lon, lat in zip(candidates.get('candidate_id', range(len(centroids_lon))), centroids_lon, centroids_lat):\n",
    "    folium.CircleMarker(location=(lat, lon), radius=6, color='red', fill=True, fill_color='red',\n",
    "                        popup=f'candidate {cid}').add_to(cand_layer)\n",
    "cand_layer.add_to(m)\n",
    "\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "map_out = OUTPUT_DIR / f'candidates_map_kmeans_nosnap_P{P}.html'\n",
    "m.save(str(map_out))\n",
    "print('Saved interactive map to', map_out)\n",
    "\n",
    "print('Done.')\n"
   ],
   "id": "8509e663bfdfaf01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POI edgelist (this may be large) ...\n",
      "Flow column detected: flow\n",
      "Running weighted KMeans (P=100) to create candidate centroids (NO SNAP) ...\n",
      "Saved candidate centroids to E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidates_kmeans_nosnap_P100.csv\n",
      "Building interactive map (folium) ...\n",
      "Saved interactive map to E:\\Uni_PGT\\visualisation_outputs\\candidate_stations\\candidates_map_kmeans_nosnap_P100.html\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 34
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
