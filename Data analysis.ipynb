{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:27.544554Z",
     "start_time": "2025-10-30T13:31:27.540494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import sys"
   ],
   "id": "81268aaa4cb40cd2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:34.107915Z",
     "start_time": "2025-10-30T13:31:30.831939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- USER CONFIG -----\n",
    "# Directory where your 36 CSV files live\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data')  # <- updated path\n",
    "# Output filename for the combined CSV\n",
    "OUTPUT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "# Pattern to match files (will match filenames containing YYYY_MM or YYYY-MM)\n",
    "GLOB_PATTERN = '*_counts*.csv'\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "def extract_year_month_from_name(fname: str):\n",
    "    \"\"\"Return (year, month) tuple if found in filename, else None.\"\"\"\n",
    "    # look for 4-digit year, separator (_ or -), 2-digit month\n",
    "    m = re.search(r'(\\d{4})[_-](\\d{2})', fname)\n",
    "    if not m:\n",
    "        return None\n",
    "    year, month = m.group(1), m.group(2)\n",
    "    return year, month\n",
    "\n",
    "\n",
    "def main(input_dir: Path, output_file: Path):\n",
    "    files = sorted(input_dir.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(f'No files found matching pattern {GLOB_PATTERN} in {input_dir.resolve()}')\n",
    "        sys.exit(1)\n",
    "\n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for f in files:\n",
    "        ym = extract_year_month_from_name(f.name)\n",
    "        if ym is None:\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        year, month = ym\n",
    "        # read CSV\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to read {f.name}: {e}')\n",
    "            skipped.append(f.name)\n",
    "            continue\n",
    "\n",
    "        # add date columns in two common formats:\n",
    "        # 'year_month' = 'YYYY-MM' and 'month_year' = 'MM/YYYY' (user asked for m/y)\n",
    "        df['year_month'] = f\"{year}-{month}\"\n",
    "        df['month_year'] = f\"{month}/{year}\"\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        print('No CSVs successfully read (maybe filename pattern is different). Files skipped:')\n",
    "        print('\\n'.join(skipped))\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "\n",
    "    # Optional: reorder so date columns are near the front\n",
    "    cols = list(combined.columns)\n",
    "    for col in ['year_month', 'month_year']:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    combined = combined[cols]\n",
    "\n",
    "    # Save the combined CSV\n",
    "    combined.to_csv(output_file, index=False)\n",
    "    print(f'Combined {len(dfs)} files into {output_file} (total rows: {len(combined)})')\n",
    "    if skipped:\n",
    "        print('Skipped files (no YYYY_MM found or read error):')\n",
    "        print('\\n'.join(skipped))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(INPUT_DIR, OUTPUT_FILE)\n"
   ],
   "id": "7dc3b5da354ab4bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 36 files into E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv (total rows: 268488)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:37.079385Z",
     "start_time": "2025-10-30T13:31:36.626032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df=pd.read_csv(r'E:\\Uni_PGT\\counts-data\\combined_od_with_date.csv')\n",
    "df.info()\n",
    "df.describe()"
   ],
   "id": "eb276d40c246370d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 268488 entries, 0 to 268487\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   month_year        268488 non-null  object \n",
      " 1   year_month        268488 non-null  object \n",
      " 2   start_station_id  268488 non-null  int64  \n",
      " 3   end_station_id    268488 non-null  int64  \n",
      " 4   hour              268488 non-null  float64\n",
      " 5   trip_count        268488 non-null  int64  \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 12.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       start_station_id  end_station_id           hour     trip_count\n",
       "count     268488.000000   268488.000000  268488.000000  268488.000000\n",
       "mean         930.490595      976.395388      13.844608       1.714162\n",
       "std          662.431144      671.219020       4.854848       1.803095\n",
       "min          171.000000      171.000000       0.000000       1.000000\n",
       "25%          260.000000      262.000000      11.000000       1.000000\n",
       "50%         1024.000000     1025.000000      14.000000       1.000000\n",
       "75%         1729.000000     1737.000000      17.000000       2.000000\n",
       "max         2268.000000     2268.000000      23.000000      84.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>hour</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "      <td>268488.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.490595</td>\n",
       "      <td>976.395388</td>\n",
       "      <td>13.844608</td>\n",
       "      <td>1.714162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>662.431144</td>\n",
       "      <td>671.219020</td>\n",
       "      <td>4.854848</td>\n",
       "      <td>1.803095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>171.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>260.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1024.000000</td>\n",
       "      <td>1025.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1729.000000</td>\n",
       "      <td>1737.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2268.000000</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:41.797686Z",
     "start_time": "2025-10-30T13:31:41.717549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "REPORT_FILE = Path(r'E:\\Uni_PGT\\counts-data\\data_quality_report.txt')\n",
    "# Ensure correct data types\n",
    "df['start_station_id'] = pd.to_numeric(df['start_station_id'], errors='coerce').astype('Int64')\n",
    "df['end_station_id'] = pd.to_numeric(df['end_station_id'], errors='coerce').astype('Int64')\n",
    "df['hour'] = pd.to_numeric(df['hour'], errors='coerce')\n",
    "df['trip_count'] = pd.to_numeric(df['trip_count'], errors='coerce').astype('Int64')\n",
    "\n",
    "# ----- MISSING VALUES -----\n",
    "missing_summary = df.isna().sum()\n",
    "missing_rows = df[df.isna().any(axis=1)]\n",
    "\n",
    "if not missing_rows.empty:\n",
    "    print(f\"Found {len(missing_rows):,} rows with missing values — will drop them.\")\n",
    "    df = df.dropna()"
   ],
   "id": "c1a1ae76fbf985c3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:43.816364Z",
     "start_time": "2025-10-30T13:31:43.730501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- DUPLICATES -----\n",
    "num_dupes = df.duplicated().sum()\n",
    "if num_dupes > 0:\n",
    "    print(f\"Dropping {num_dupes:,} duplicate rows.\")\n",
    "    df = df.drop_duplicates()"
   ],
   "id": "80502a5ea7e47868",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:45.871083Z",
     "start_time": "2025-10-30T13:31:45.858495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- VALUE VALIDATION -----\n",
    "# Check valid range for hour (0–23 expected)\n",
    "invalid_hours = df[~df['hour'].between(0, 24)]\n",
    "if not invalid_hours.empty:\n",
    "    print(f\"Found {len(invalid_hours):,} rows with invalid hour values (outside 0–24). Fixing...\")\n",
    "    df = df[df['hour'].between(0, 24)]"
   ],
   "id": "211dc1e4af4c94a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:47.795709Z",
     "start_time": "2025-10-30T13:31:47.749531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check station ID ranges\n",
    "df_s=pd.read_csv(r'E:\\Uni_PGT\\station_data.csv')\n",
    "m=list(df_s['station_id'].unique())\n",
    "w=df['start_station_id'].unique()\n",
    "missing_ids = [s for s in w if s not in m]\n",
    "print(missing_ids)\n",
    "print(len(missing_ids))\n"
   ],
   "id": "29fbcd4e747abd69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(171), np.int64(255), np.int64(257), np.int64(261), np.int64(266), np.int64(273), np.int64(275), np.int64(277), np.int64(284), np.int64(285), np.int64(290), np.int64(297), np.int64(340), np.int64(341), np.int64(342), np.int64(343), np.int64(344), np.int64(345), np.int64(346), np.int64(347), np.int64(350), np.int64(351), np.int64(352), np.int64(353), np.int64(354), np.int64(355), np.int64(356), np.int64(357), np.int64(359), np.int64(365), np.int64(366), np.int64(648), np.int64(820), np.int64(860), np.int64(862), np.int64(863), np.int64(864), np.int64(865), np.int64(866), np.int64(867), np.int64(868), np.int64(869), np.int64(870), np.int64(871), np.int64(872), np.int64(873), np.int64(874), np.int64(875), np.int64(876), np.int64(877), np.int64(878), np.int64(879), np.int64(880), np.int64(881), np.int64(882), np.int64(885), np.int64(887), np.int64(888), np.int64(889), np.int64(883), np.int64(884), np.int64(890), np.int64(891), np.int64(901), np.int64(964), np.int64(965), np.int64(980), np.int64(981), np.int64(982), np.int64(991), np.int64(1018), np.int64(1026), np.int64(299), np.int64(1027), np.int64(1030), np.int64(1031), np.int64(1032), np.int64(1033), np.int64(1040), np.int64(1041), np.int64(1042), np.int64(1055), np.int64(1056), np.int64(1057), np.int64(1094), np.int64(1095), np.int64(1723), np.int64(1724), np.int64(1731), np.int64(1740), np.int64(1743), np.int64(1746), np.int64(1747), np.int64(1752), np.int64(1764), np.int64(1766), np.int64(1799), np.int64(1800), np.int64(1808), np.int64(1857), np.int64(1859), np.int64(1864), np.int64(1865), np.int64(1866), np.int64(1868), np.int64(1869), np.int64(1870), np.int64(1871), np.int64(1874), np.int64(1877), np.int64(1860)]\n",
      "111\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:31:51.010383Z",
     "start_time": "2025-10-30T13:31:50.960333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ----- REPORT -----\n",
    "with open(REPORT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('OD Matrix Data Quality Report\\n')\n",
    "    f.write('=' * 40 + '\\n\\n')\n",
    "    f.write(f'Total rows after cleaning: {len(df):,}\\n')\n",
    "    f.write(f'Duplicates removed: {num_dupes}\\n')\n",
    "    f.write(f'Missing rows removed: {len(missing_rows)}\\n')\n",
    "    f.write(f'Invalid hour rows removed: {len(invalid_hours)}\\n\\n')\n",
    "\n",
    "\n",
    "    f.write('Trip count summary (post-clean):\\n')\n",
    "    f.write(str(df['trip_count'].describe()) + '\\n\\n')\n",
    "\n",
    "\n",
    "print(f'Data quality report saved to: {REPORT_FILE}')\n",
    "\n"
   ],
   "id": "c9a8a18a268937df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality report saved to: E:\\Uni_PGT\\counts-data\\data_quality_report.txt\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:32:40.561463Z",
     "start_time": "2025-10-30T13:31:55.905085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "generate_od_heatmaps.py\n",
    "\n",
    "Generates OD heatmap images for each month (36 files) plus one combined OD heatmap\n",
    "from the combined OD CSV created earlier.\n",
    "\n",
    "Notes / behavior:\n",
    " - The script reads 'combined_od_with_date.csv' and expects columns:\n",
    "   ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    " - It will order stations by numeric station id (ascending). If you have a station\n",
    "   reference file and want a specific ordering, set STATION_REF_FILE.\n",
    " - For visualization, the script uses np.log1p on counts to reduce skew. The saved\n",
    "   CSVs keep raw aggregated counts.\n",
    "\n",
    "Run:\n",
    "    python generate_od_heatmaps.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps')\n",
    "STATION_REF_FILE = Path(r'E:\\Uni_PGT\\station_data.csv')  # set to None to order by numeric id\n",
    "LOG_DISPLAY = True   # use log1p for display to reduce skew\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# If no station reference, build station list from data\n",
    "all_stations = np.union1d(df['start_station_id'].unique(), df['end_station_id'].unique()).astype(int)\n",
    "all_stations_sorted = sorted([int(x) for x in all_stations])\n",
    "n_stations = len(all_stations_sorted)\n",
    "stations = sorted([int(x) for x in all_stations])\n",
    "print(f'Total stations used for matrices: {n_stations}')\n",
    "\n",
    "# helper to make pivot and plot\n",
    "\n",
    "def make_pivot(df_subset, stations, aggcol='trip_count'):\n",
    "    # aggregate counts to ensure one cell per pair\n",
    "    agg = df_subset.groupby(['start_station_id', 'end_station_id'])[aggcol].sum().reset_index()\n",
    "    pivot = agg.pivot(index='start_station_id', columns='end_station_id', values=aggcol).reindex(index=stations, columns=stations).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "\n",
    "def plot_matrix(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    # dynamic figsize: cap sizes to avoid enormous images\n",
    "    height, width = arr.shape\n",
    "    figsize = (min(20, max(6, width/10)), min(20, max(6, height/10)))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('end_station_id (ordered)')\n",
    "    ax.set_ylabel('start_station_id (ordered)')\n",
    "\n",
    "    # reduce tick labels for readability: show first, middle, last\n",
    "    if width <= 30:\n",
    "        xticks = range(width)\n",
    "        xtick_labels = [str(int(v)) for v in matrix_df.columns]\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xtick_labels, rotation=90, fontsize=6)\n",
    "    else:\n",
    "        ax.set_xticks([0, width//2, width-1])\n",
    "        ax.set_xticklabels([str(int(matrix_df.columns[0])), str(int(matrix_df.columns[width//2])), str(int(matrix_df.columns[-1]))], fontsize=8)\n",
    "\n",
    "    if height <= 30:\n",
    "        yticks = range(height)\n",
    "        ytick_labels = [str(int(v)) for v in matrix_df.index]\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(ytick_labels, fontsize=6)\n",
    "    else:\n",
    "        ax.set_yticks([0, height//2, height-1])\n",
    "        ax.set_yticklabels([str(int(matrix_df.index[0])), str(int(matrix_df.index[height//2])), str(int(matrix_df.index[-1]))], fontsize=8)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved heatmap: {outpath}')\n",
    "\n",
    "\n",
    "# 1) Per-month heatmaps\n",
    "unique_months = sorted(df['year_month'].unique())\n",
    "print(f'Found {len(unique_months)} unique months (expected 36): {unique_months}')\n",
    "\n",
    "for ym in unique_months:\n",
    "    df_month = df[df['year_month'] == ym]\n",
    "    pivot = make_pivot(df_month, stations)\n",
    "    outpath = OUTPUT_DIR / f'heatmap_{ym}.png'\n",
    "    title = f'OD heatmap {ym} (log display={LOG_DISPLAY})'\n",
    "    plot_matrix(pivot, title, outpath)\n",
    "\n",
    "# 2) Combined heatmap for all data\n",
    "pivot_combined = make_pivot(df, stations)\n",
    "# save numeric combined matrix\n",
    "pivot_combined.to_csv(OUTPUT_DIR / 'od_matrix_combined.csv')\n",
    "plot_matrix(pivot_combined, 'OD heatmap combined (log display={})'.format(LOG_DISPLAY), OUTPUT_DIR / 'heatmap_combined.png')\n",
    "\n",
    "print('All done. Generated per-month and combined heatmaps in:')\n",
    "print(OUTPUT_DIR.resolve())"
   ],
   "id": "4d87a987f75f2586",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stations used for matrices: 198\n",
      "Found 36 unique months (expected 36): ['2018-10', '2018-11', '2018-12', '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07', '2019-08', '2019-09', '2019-10', '2019-11', '2019-12', '2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06', '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09']\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2018-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2019-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-10.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-11.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2020-12.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-01.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-02.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-03.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-04.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-05.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-06.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-07.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-08.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_2021-09.png\n",
      "Saved heatmap: E:\\Uni_PGT\\counts-data\\heatmaps\\heatmap_combined.png\n",
      "All done. Generated per-month and combined heatmaps in:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:32:42.812066Z",
     "start_time": "2025-10-30T13:32:40.561463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "seasonal_hourly_heatmaps.py\n",
    "\n",
    "Generates seasonal-hourly OD trip heatmaps:\n",
    " - One heatmap per year (season x hour matrix) for each year in the data\n",
    " - One combined heatmap over all years\n",
    " - Saves matrices as CSV and images to OUTPUT_DIR\n",
    "\n",
    "Seasons used (Northern Hemisphere standard):\n",
    " - Winter: Dec, Jan, Feb (DJF)\n",
    " - Spring: Mar, Apr, May (MAM)\n",
    " - Summer: Jun, Jul, Aug (JJA)\n",
    " - Autumn: Sep, Oct, Nov (SON)\n",
    "\n",
    "Expect input columns: ['year_month','month_year','start_station_id','end_station_id','hour','trip_count']\n",
    "\n",
    "Run:\n",
    "    python seasonal_hourly_heatmaps.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\counts-data\\heatmaps_seasonal')\n",
    "DPI = 150\n",
    "LOG_DISPLAY = False  # For seasonal-hour heatmaps we keep linear counts by default\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def parse_year_month(ym):\n",
    "    if isinstance(ym, str):\n",
    "        if '-' in ym:\n",
    "            parts = ym.split('-')\n",
    "            return int(parts[0]), int(parts[1])\n",
    "        if '/' in ym:\n",
    "            parts = ym.split('/')\n",
    "            # assume MM/YYYY -> return (YYYY, MM)\n",
    "            return int(parts[1]), int(parts[0])\n",
    "    raise ValueError(f'Unrecognized year_month format: {ym}')\n",
    "\n",
    "parsed = df['year_month'].apply(parse_year_month)\n",
    "df['year'] = parsed.apply(lambda x: x[0])\n",
    "df['month'] = parsed.apply(lambda x: x[1])\n",
    "\n",
    "# season mapping\n",
    "season_map = {12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "              3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "              6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "              9: 'Autumn', 10: 'Autumn', 11: 'Autumn'}\n",
    "\n",
    "df['season'] = df['month'].map(season_map)\n",
    "\n",
    "# Helper: build season-hour pivot for a given dataframe\n",
    "\n",
    "def season_hour_pivot(df_subset):\n",
    "    # aggregate trip counts by season and hour\n",
    "    agg = df_subset.groupby(['season', 'hour'])['trip_count'].sum().reset_index()\n",
    "    # ensure all seasons and hours 0-23 present\n",
    "    seasons = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
    "    hours = list(range(24))\n",
    "    pivot = agg.pivot(index='season', columns='hour', values='trip_count').reindex(index=seasons, columns=hours).fillna(0)\n",
    "    return pivot\n",
    "\n",
    "# plotting helper\n",
    "\n",
    "def plot_season_hour(matrix_df, title, outpath, log_display=LOG_DISPLAY, dpi=DPI):\n",
    "    arr = matrix_df.values.astype(float)\n",
    "    display_arr = np.log1p(arr) if log_display else arr\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    im = ax.imshow(display_arr, aspect='auto')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Hour of day')\n",
    "    ax.set_ylabel('Season')\n",
    "\n",
    "    ax.set_xticks(range(0, 24, 2))\n",
    "    ax.set_xticklabels([str(h) for h in range(0, 24, 2)])\n",
    "\n",
    "    ax.set_yticks(range(len(matrix_df.index)))\n",
    "    ax.set_yticklabels(matrix_df.index)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.set_label('log1p(trip_count)' if log_display else 'trip_count')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(outpath, dpi=dpi)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved: {outpath}')\n",
    "\n",
    "# 1) Per-year seasonal-hour heatmaps\n",
    "years = sorted(df['year'].unique())\n",
    "print(f'Found years: {years}')\n",
    "\n",
    "for y in years:\n",
    "    df_year = df[df['year'] == y]\n",
    "    pivot = season_hour_pivot(df_year)\n",
    "    csv_out = OUTPUT_DIR / f'season_hour_matrix_{y}.csv'\n",
    "    img_out = OUTPUT_DIR / f'season_hour_heatmap_{y}.png'\n",
    "    pivot.to_csv(csv_out)\n",
    "    plot_season_hour(pivot, f'Season vs Hour - {y}', img_out)\n",
    "\n",
    "# 2) Combined (all years)\n",
    "pivot_all = season_hour_pivot(df)\n",
    "pivot_all.to_csv(OUTPUT_DIR / 'season_hour_matrix_all_years.csv')\n",
    "plot_season_hour(pivot_all, 'Season vs Hour - All years', OUTPUT_DIR / 'season_hour_heatmap_all_years.png')\n",
    "\n",
    "print('Done. Results saved to:')\n",
    "print(OUTPUT_DIR.resolve())\n"
   ],
   "id": "160179ad6a1012bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found years: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021)]\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2018.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2019.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2020.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_2021.png\n",
      "Saved: E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\\season_hour_heatmap_all_years.png\n",
      "Done. Results saved to:\n",
      "E:\\Uni_PGT\\counts-data\\heatmaps_seasonal\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:33:22.281786Z",
     "start_time": "2025-10-30T13:33:09.803686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "combine_36_csvs_no_date.py\n",
    "\n",
    "Reads all CSV files from cyclehire-cleandata named like 2018_10.csv ... 2021_09.csv\n",
    "(or containing that yyyy_mm pattern in the filename), concatenates them into a single CSV\n",
    "and saves it to cyclehire-cleandata\\combined_all_periods.csv\n",
    "\n",
    "Notes:\n",
    " - This script does NOT add a date column (as requested).\n",
    " - It will align columns by name; missing columns in some files will be filled with NaN.\n",
    " - It prints a short summary of files read and total rows combined.\n",
    "\n",
    "Usage:\n",
    "    python combine_36_csvs_no_date.py\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "INPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata')\n",
    "OUTPUT_FILE = INPUT_DIR / 'combined_all_periods.csv'\n",
    "GLOB_PATTERN = '*_*.csv'  # matches files with yyyy_mm in name like 2018_10.csv\n",
    "FILE_FILTER_REGEX = re.compile(r'(20\\d{2})[_-](0[1-9]|1[0-2])')  # restrict to yyyy_mm patterns\n",
    "# ------------------------\n",
    "\n",
    "# collect candidate files\n",
    "files = sorted(INPUT_DIR.glob(GLOB_PATTERN))\n",
    "selected_files = [f for f in files if FILE_FILTER_REGEX.search(f.name)]\n",
    "\n",
    "if not selected_files:\n",
    "    raise FileNotFoundError(f'No files matching yyyy_mm pattern found in {INPUT_DIR}')\n",
    "\n",
    "print(f'Found {len(selected_files)} files to combine:')\n",
    "for f in selected_files:\n",
    "    print(' -', f.name)\n",
    "\n",
    "# read and concatenate\n",
    "dfs = []\n",
    "for f in selected_files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df['__source_file'] = f.name  # optional: keep which file the row came from\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed to read {f.name}: {e}')\n",
    "\n",
    "if not dfs:\n",
    "    raise ValueError('No files were successfully read.')\n",
    "\n",
    "combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "combined.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f'Combined {len(dfs)} files into {OUTPUT_FILE} (total rows: {len(combined):,})')\n",
    "\n",
    "# optional quick sanity print\n",
    "print('\\nColumn summary (name : non-null count):')\n",
    "print(combined.notna().sum().sort_values(ascending=False).head(50))\n",
    "\n",
    "print('\\nDone.')\n"
   ],
   "id": "5973531e49f04c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_5428\\2259740896.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 files to combine:\n",
      " - 2018_10.csv\n",
      " - 2018_11.csv\n",
      " - 2018_12.csv\n",
      " - 2019_01.csv\n",
      " - 2019_02.csv\n",
      " - 2019_03.csv\n",
      " - 2019_04.csv\n",
      " - 2019_05.csv\n",
      " - 2019_06.csv\n",
      " - 2019_07.csv\n",
      " - 2019_08.csv\n",
      " - 2019_09.csv\n",
      " - 2019_10.csv\n",
      " - 2019_11.csv\n",
      " - 2019_12.csv\n",
      " - 2020_01.csv\n",
      " - 2020_02.csv\n",
      " - 2020_03.csv\n",
      " - 2020_04.csv\n",
      " - 2020_05.csv\n",
      " - 2020_06.csv\n",
      " - 2020_07.csv\n",
      " - 2020_08.csv\n",
      " - 2020_09.csv\n",
      " - 2020_10.csv\n",
      " - 2020_11.csv\n",
      " - 2020_12.csv\n",
      " - 2021_01.csv\n",
      " - 2021_02.csv\n",
      " - 2021_03.csv\n",
      " - 2021_04.csv\n",
      " - 2021_05.csv\n",
      " - 2021_06.csv\n",
      " - 2021_07.csv\n",
      " - 2021_08.csv\n",
      " - 2021_09.csv\n",
      "Combined 36 files into E:\\Uni_PGT\\cyclehire-cleandata\\combined_all_periods.csv (total rows: 460,655)\n",
      "\n",
      "Column summary (name : non-null count):\n",
      "started_at                   460655\n",
      "ended_at                     460655\n",
      "duration                     460655\n",
      "start_station_id             460655\n",
      "start_station_name           460655\n",
      "start_station_latitude       460655\n",
      "end_station_latitude         460655\n",
      "start_station_longitude      460655\n",
      "end_station_id               460655\n",
      "end_station_name             460655\n",
      "__source_file                460655\n",
      "end_station_longitude        460655\n",
      "start_station_description    456167\n",
      "end_station_description      455560\n",
      "dtype: int64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:33:29.901916Z",
     "start_time": "2025-10-30T13:33:28.626079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(combined.info())\n",
    "print(combined.describe())"
   ],
   "id": "3e84a0d30fe9b22b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 460655 entries, 0 to 460654\n",
      "Data columns (total 14 columns):\n",
      " #   Column                     Non-Null Count   Dtype  \n",
      "---  ------                     --------------   -----  \n",
      " 0   started_at                 460655 non-null  object \n",
      " 1   ended_at                   460655 non-null  object \n",
      " 2   duration                   460655 non-null  int64  \n",
      " 3   start_station_id           460655 non-null  int64  \n",
      " 4   start_station_name         460655 non-null  object \n",
      " 5   start_station_description  456167 non-null  object \n",
      " 6   start_station_latitude     460655 non-null  float64\n",
      " 7   start_station_longitude    460655 non-null  float64\n",
      " 8   end_station_id             460655 non-null  int64  \n",
      " 9   end_station_name           460655 non-null  object \n",
      " 10  end_station_description    455560 non-null  object \n",
      " 11  end_station_latitude       460655 non-null  float64\n",
      " 12  end_station_longitude      460655 non-null  float64\n",
      " 13  __source_file              460655 non-null  object \n",
      "dtypes: float64(4), int64(3), object(7)\n",
      "memory usage: 49.2+ MB\n",
      "None\n",
      "           duration  start_station_id  start_station_latitude  \\\n",
      "count  4.606550e+05     460655.000000           460655.000000   \n",
      "mean   1.945961e+03        936.878545               55.950615   \n",
      "std    5.531311e+03        671.956842                0.013497   \n",
      "min    6.100000e+01        171.000000               55.908404   \n",
      "25%    6.280000e+02        261.000000               55.940238   \n",
      "50%    1.166000e+03       1024.000000               55.947600   \n",
      "75%    2.527000e+03       1729.000000               55.958566   \n",
      "max    2.363348e+06       2268.000000               55.992957   \n",
      "\n",
      "       start_station_longitude  end_station_id  end_station_latitude  \\\n",
      "count            460655.000000   460655.000000         460655.000000   \n",
      "mean                 -3.196412      983.399353             55.952535   \n",
      "std                   0.039072      678.334765              0.015748   \n",
      "min                  -3.407156      171.000000             53.395525   \n",
      "25%                  -3.207964      262.000000             55.941791   \n",
      "50%                  -3.192444     1025.000000             55.951501   \n",
      "75%                  -3.180693     1737.000000             55.962487   \n",
      "max                  -3.058307     2268.000000             55.992957   \n",
      "\n",
      "       end_station_longitude  \n",
      "count          460655.000000  \n",
      "mean               -3.195134  \n",
      "std                 0.041796  \n",
      "min                -3.407156  \n",
      "25%                -3.208070  \n",
      "50%                -3.191421  \n",
      "75%                -3.176351  \n",
      "max                -2.990138  \n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:33:44.571161Z",
     "start_time": "2025-10-30T13:33:38.681717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the path where your data is stored\n",
    "data_path = r'E:\\Uni_PGT\\cyclehire-cleandata'\n",
    "\n",
    "# Convert started_at column to datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "\n",
    "# Extract weekday and hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# Assign seasons based on month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Autumn'\n",
    "\n",
    "combined['season'] = combined['started_at'].dt.month.apply(get_season)\n",
    "\n",
    "# Order weekdays for consistent plotting\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Create output folder for heatmaps\n",
    "output_dir = os.path.join(data_path, 'heatmaps_weekday_hour')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to plot and save heatmap\n",
    "def plot_heatmap(data, title, filename):\n",
    "    pivot_table = data.pivot_table(index='weekday', columns='hour', values='duration', aggfunc='count').fillna(0)\n",
    "    pivot_table = pivot_table.reindex(weekday_order)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(pivot_table, cmap='YlGnBu')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Weekday')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Generate heatmap for each season\n",
    "for season, data in combined.groupby('season'):\n",
    "    plot_heatmap(data, f'Trip Count by Hour and Weekday - {season}', f'heatmap_{season}.png')\n",
    "\n",
    "# Generate heatmap for all data combined\n",
    "plot_heatmap(combined, 'Trip Count by Hour and Weekday - All Data', 'heatmap_all_data.png')\n",
    "\n",
    "print(f\"Heatmaps saved in: {output_dir}\")"
   ],
   "id": "ffcd021ad0f256a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmaps saved in: E:\\Uni_PGT\\cyclehire-cleandata\\heatmaps_weekday_hour\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T13:34:12.441290Z",
     "start_time": "2025-10-30T13:34:08.942748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "duration_and_od_analysis.py\n",
    "\n",
    "Produces:\n",
    " - Histogram of trip durations (linear and log-scaled)\n",
    " - CSV and plot for average duration by hour of day\n",
    " - CSV and plot for average duration by weekday\n",
    " - OD heatmap (average duration per start_station_id x end_station_id)\n",
    "\n",
    "Assumptions:\n",
    " - Combined CSV of the 36 files exists at: cyclehire-cleandata\\combined_all_periods.csv\n",
    " - Columns include: started_at, ended_at, duration, start_station_id, end_station_id\n",
    "\n",
    "Outputs are written to cyclehire-cleandata\\analysis_outputs\n",
    "\n",
    "Run:\n",
    "    python duration_and_od_analysis.py\n",
    "\"\"\"\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUTPUT_DIR = Path(r'E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs')\n",
    "DPI = 150\n",
    "# ------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Convert started_at to datetime (coerce errors)\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "# Filter out rows with missing datetime or non-positive duration\n",
    "initial_rows = len(combined)\n",
    "combined = combined[combined['duration'].notna()]\n",
    "combined = combined[combined['duration'] > 0]\n",
    "combined = combined[combined['started_at'].notna()]\n",
    "print(f'Kept {len(combined):,} rows (removed {initial_rows - len(combined):,} invalid rows)')\n",
    "\n",
    "# --- 1) Histograms of duration ---\n",
    "# Linear histogram\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(combined['duration'], bins=100, range=(0, combined['duration'].quantile(0.99)))\n",
    "plt.title('Trip duration distribution (0-99th percentile)')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_linear.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Log-scaled histogram (log1p)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(np.log1p(combined['duration']), bins=100)\n",
    "plt.title('Trip duration distribution (log1p)')\n",
    "plt.xlabel('log1p(Duration)')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'duration_histogram_log.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# Save basic stats\n",
    "desc = combined['duration'].describe()\n",
    "desc.to_csv(OUTPUT_DIR / 'duration_summary_stats.csv')\n",
    "\n",
    "# --- 2) Average duration by hour of day ---\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "avg_by_hour = combined.groupby('hour')['duration'].mean().reindex(range(24)).fillna(0)\n",
    "avg_by_hour.to_csv(OUTPUT_DIR / 'avg_duration_by_hour.csv')\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(avg_by_hour.index, avg_by_hour.values, marker='o')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Hour of day')\n",
    "plt.ylabel('Average duration')\n",
    "plt.title('Average trip duration by hour of day')\n",
    "plt.xticks(range(0,24))\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_hour.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 3) Average duration by weekday ---\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "weekday_order = ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']\n",
    "avg_by_weekday = combined.groupby('weekday')['duration'].mean().reindex(weekday_order)\n",
    "avg_by_weekday.to_csv(OUTPUT_DIR / 'avg_duration_by_weekday.csv')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(avg_by_weekday.index, avg_by_weekday.values)\n",
    "plt.xlabel('Weekday')\n",
    "plt.ylabel('Average duration (seconds)')\n",
    "plt.title('Average trip duration by weekday')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'avg_duration_by_weekday.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "# --- 4) OD heatmap: average duration per (start_station_id x end_station_id) ---\n",
    "# To limit memory use, we will restrict to top N stations by activity, but also\n",
    "# save a CSV of aggregated averages for all pairs.\n",
    "\n",
    "# Aggregate per pair\n",
    "pair_agg = combined.groupby(['start_station_id','end_station_id'])['duration'].agg(['mean','count']).reset_index()\n",
    "pair_agg.rename(columns={'mean':'avg_duration','count':'trip_count'}, inplace=True)\n",
    "pair_agg.to_csv(OUTPUT_DIR / 'od_pair_avg_duration_all_pairs.csv', index=False)\n",
    "\n",
    "# Choose top stations by total trips (to make a manageable heatmap)\n",
    "station_activity = pd.concat([combined['start_station_id'], combined['end_station_id']]).value_counts()\n",
    "top_n = 100  # adjust if you want larger/smaller matrix\n",
    "top_stations = station_activity.index[:top_n].astype(int).tolist()\n",
    "print(f'Creating OD heatmap for top {len(top_stations)} stations by activity')\n",
    "\n",
    "# Pivot for top stations\n",
    "subset = pair_agg[pair_agg['start_station_id'].isin(top_stations) & pair_agg['end_station_id'].isin(top_stations)]\n",
    "heat = subset.pivot(index='start_station_id', columns='end_station_id', values='avg_duration').reindex(index=top_stations, columns=top_stations).fillna(0)\n",
    "\n",
    "# Plot heatmap (use log scale for color or linear depending on spread)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(np.log1p(heat.values), aspect='auto')\n",
    "plt.colorbar(label='log1p(avg_duration)')\n",
    "plt.title(f'OD average duration heatmap (top {len(top_stations)} stations)')\n",
    "plt.xlabel('end_station_id (ordered by activity)')\n",
    "plt.ylabel('start_station_id (ordered by activity)')\n",
    "# keep tick labels sparse for readability\n",
    "n = len(top_stations)\n",
    "plt.xticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.yticks([0, n//2, n-1], [str(top_stations[0]), str(top_stations[n//2]), str(top_stations[-1])])\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'od_avg_duration_heatmap_top{top_n}.png', dpi=DPI)\n",
    "plt.close()\n",
    "\n",
    "print('Analysis complete. Outputs saved to:', OUTPUT_DIR.resolve())\n"
   ],
   "id": "a1785be491b730e0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_5428\\3145085024.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 460,232 rows (removed 423 invalid rows)\n",
      "Creating OD heatmap for top 100 stations by activity\n",
      "Analysis complete. Outputs saved to: E:\\Uni_PGT\\cyclehire-cleandata\\analysis_outputs\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T23:32:02.055251Z",
     "start_time": "2025-10-29T23:31:24.484579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "save_hourly_maps_with_basemap.py\n",
    "\n",
    "Saves per-hour origin-demand maps to disk (one PNG per hour) and also saves\n",
    "weekday-hour heatmaps. Uses the in-memory `combined` DataFrame if present; otherwise\n",
    "expects you to load it prior to running this script.\n",
    "\n",
    "Outputs are written to: E:/Uni_PGT/visualisation_outputs/station_hour_maps\n",
    "\n",
    "Requirements:\n",
    " - geopandas, matplotlib, seaborn, contextily (optional, for web basemap tiles)\n",
    "   Install with: pip install geopandas matplotlib seaborn contextily\n",
    "\n",
    "Notes on basemap: contextily fetches tiles from the web. If you have no internet,\n",
    "this script will fall back to plotting station points on plain axes.\n",
    "\n",
    "Run this in the same Python session where `combined` exists (not by re-loading the CSV).\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ----- USER CONFIG -----\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/station_hour_maps')\n",
    "HEATMAP_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/heatmaps_weekday_hour')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HEATMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 200\n",
    "POINT_SCALE = 2000  # adjust to scale marker sizes (increase for larger markers)\n",
    "USE_CONTEXTILY = True  # set False if you don't want to fetch basemap tiles\n",
    "TILE_SOURCE = None  # default contextily source (None uses provider's default)\n",
    "# ------------------------\n",
    "\n",
    "print('Using in-memory DataFrame `combined`')\n",
    "try:\n",
    "    combined  # must exist in the environment\n",
    "except NameError:\n",
    "    raise RuntimeError('DataFrame `combined` not found in memory. Load it first.')\n",
    "\n",
    "# Ensure datetime\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "combined['weekday'] = combined['started_at'].dt.day_name()\n",
    "\n",
    "# Prepare station aggregated counts by hour\n",
    "agg = combined.groupby(['start_station_id','start_station_latitude','start_station_longitude','hour']).size().reset_index(name='starts')\n",
    "# If lat/lon columns have different names, try alternatives\n",
    "if agg['start_station_latitude'].isna().all() or agg['start_station_longitude'].isna().all():\n",
    "    # try alternative names in combined\n",
    "    lat_col = None\n",
    "    lon_col = None\n",
    "    for c in combined.columns:\n",
    "        if c.lower().endswith('latitude') and 'start' in c.lower():\n",
    "            lat_col = c\n",
    "        if c.lower().endswith('longitude') and 'start' in c.lower():\n",
    "            lon_col = c\n",
    "    if lat_col and lon_col:\n",
    "        agg = combined.groupby(['start_station_id', lat_col, lon_col, 'hour']).size().reset_index(name='starts')\n",
    "        agg = agg.rename(columns={lat_col:'start_station_latitude', lon_col:'start_station_longitude'})\n",
    "\n",
    "# drop rows without coordinates\n",
    "agg = agg.dropna(subset=['start_station_latitude','start_station_longitude'])\n",
    "\n",
    "# Build GeoDataFrame (EPSG:4326)\n",
    "agg['geometry'] = [Point(xy) for xy in zip(agg['start_station_longitude'].astype(float), agg['start_station_latitude'].astype(float))]\n",
    "gdf = gpd.GeoDataFrame(agg, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Project to Web Mercator for contextily (if using basemap)\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_web = gdf.to_crs(epsg=3857)\n",
    "    except Exception as e:\n",
    "        print('Could not reproject to WebMercator, disabling contextily basemap:', e)\n",
    "        USE_CONTEXTILY = False\n",
    "        gdf_web = gdf\n",
    "else:\n",
    "    gdf_web = gdf\n",
    "\n",
    "# Determine map extent (in web mercator if using contextily)\n",
    "minx, miny, maxx, maxy = gdf_web.total_bounds\n",
    "xpad = (maxx - minx) * 0.08 if maxx > minx else 100\n",
    "ypad = (maxy - miny) * 0.08 if maxy > miny else 100\n",
    "extent = (minx - xpad, maxx + xpad, miny - ypad, maxy + ypad)\n",
    "\n",
    "# Save one PNG per hour\n",
    "print('Saving hourly station-origin maps to:', OUT_DIR)\n",
    "for h in range(24):\n",
    "    hour_gdf = gdf_web[gdf_web['hour'] == h]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # plot all stations faintly as background\n",
    "    gdf_web.plot(ax=ax, color='lightgrey', markersize=5, alpha=0.5)\n",
    "\n",
    "    if not hour_gdf.empty:\n",
    "        # marker size scaled by sqrt to reduce dynamic range\n",
    "        sizes = np.sqrt(hour_gdf['starts'].astype(float) + 1) * (POINT_SCALE / max(1, np.sqrt(hour_gdf['starts'].max()*200+ 1)))\n",
    "        hour_gdf.plot(ax=ax, markersize=sizes, column='starts', cmap='Reds', legend=True, alpha=0.9)\n",
    "\n",
    "    if USE_CONTEXTILY:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            if TILE_SOURCE is None:\n",
    "                ctx.add_basemap(ax, crs=gdf_web.crs.to_string())\n",
    "            else:\n",
    "                ctx.add_basemap(ax, source=TILE_SOURCE, crs=gdf_web.crs.to_string())\n",
    "        except Exception as e:\n",
    "            print('contextily failed; continuing without basemap:', e)\n",
    "\n",
    "    ax.set_xlim(extent[0], extent[1])\n",
    "    ax.set_ylim(extent[2], extent[3])\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(f'Origin starts — hour {h}')\n",
    "\n",
    "    outpath = OUT_DIR / f'station_starts_hour_{h:02d}.png'\n",
    "    fig.savefig(outpath, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print('Hourly maps saved.')\n",
    "\n",
    "# --- Save weekday-hour heatmap (counts) ---\n",
    "print('Saving weekday-hour heatmap...')\n",
    "combined['weekday'] = pd.Categorical(combined['weekday'], categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], ordered=True)\n",
    "pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(pivot, cmap='YlOrRd')\n",
    "plt.title('Trip starts by weekday and hour')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Weekday')\n",
    "hm_path = HEATMAP_DIR / 'weekday_hour_heatmap_all_data.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(hm_path, dpi=DPI)\n",
    "plt.close()\n",
    "print('Weekday-hour heatmap saved to', hm_path)\n",
    "\n",
    "print('\\nAll outputs written to:', OUT_DIR.parent)\n"
   ],
   "id": "567fec6fb072f56a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory DataFrame `combined`\n",
      "Saving hourly station-origin maps to: E:\\Uni_PGT\\visualisation_outputs\\station_hour_maps\n",
      "Hourly maps saved.\n",
      "Saving weekday-hour heatmap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frequensy\\AppData\\Local\\Temp\\ipykernel_8268\\650905050.py:131: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = combined.pivot_table(index='weekday', columns=combined['hour'], values='start_station_id', aggfunc='count').fillna(0).reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekday-hour heatmap saved to E:\\Uni_PGT\\visualisation_outputs\\heatmaps_weekday_hour\\weekday_hour_heatmap_all_data.png\n",
      "\n",
      "All outputs written to: E:\\Uni_PGT\\visualisation_outputs\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T14:02:42.743096Z",
     "start_time": "2025-10-30T14:02:07.273139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hourly_maps_and_separate_bars_fixed_extent.py\n",
    "#\n",
    "# Creates per-hour maps (origins left, destinations right) and saves them to disk\n",
    "# (maps and separate bar charts). This version computes a robust map extent using\n",
    "# percentiles (2nd-98th) to ignore coordinate outliers so plots zoom into Edinburgh.\n",
    "#\n",
    "# Run in the same session where `combined` exists (the combined DataFrame you made).\n",
    "# Requires: geopandas, matplotlib, seaborn, contextily (optional for basemap tiles).\n",
    "#\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# ---------- USER CONFIG ----------\n",
    "OUT_DIR = Path(r'E:/Uni_PGT/visualisation_outputs/hourly_maps_v3_fixed_extent')\n",
    "MAP_DIR = OUT_DIR / 'maps'\n",
    "BARS_DIR = OUT_DIR / 'bars'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BARS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DPI = 180\n",
    "USE_CONTEXTILY = True     # set False if no internet or you don't want basemap tiles\n",
    "POINT_BASE = 35           # base marker size (smaller => less overlap)\n",
    "TOP_K_BARS = 12\n",
    "CMAP_ORIG = 'Reds'\n",
    "CMAP_DEST = 'Blues'\n",
    "# ---------------------------------\n",
    "\n",
    "# ensure combined exists\n",
    "try:\n",
    "    combined\n",
    "except NameError:\n",
    "    raise RuntimeError(\"DataFrame `combined` not found in memory. Load it first.\")\n",
    "\n",
    "# parse datetimes and hour\n",
    "combined['started_at'] = pd.to_datetime(combined['started_at'], errors='coerce')\n",
    "combined = combined.dropna(subset=['started_at'])\n",
    "combined['hour'] = combined['started_at'].dt.hour\n",
    "\n",
    "# flexible column detection\n",
    "def find_col(df, candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "s_lat = find_col(combined, ['start_station_latitude','start_latitude','start_lat','start_station_lat'])\n",
    "s_lon = find_col(combined, ['start_station_longitude','start_longitude','start_lon','start_station_lon'])\n",
    "e_lat = find_col(combined, ['end_station_latitude','end_latitude','end_lat','end_station_lat'])\n",
    "e_lon = find_col(combined, ['end_station_longitude','end_longitude','end_lon','end_station_lon'])\n",
    "\n",
    "if not all([s_lat, s_lon, e_lat, e_lon]):\n",
    "    raise RuntimeError('Could not find necessary start/end latitude/longitude columns in combined dataframe.')\n",
    "\n",
    "# aggregate\n",
    "orig_agg = combined.groupby(['start_station_id', s_lat, s_lon, 'hour']).size().reset_index(name='count')\n",
    "orig_agg = orig_agg.rename(columns={s_lat:'lat', s_lon:'lon', 'start_station_id':'station_id'})\n",
    "\n",
    "dest_agg = combined.groupby(['end_station_id', e_lat, e_lon, 'hour']).size().reset_index(name='count')\n",
    "dest_agg = dest_agg.rename(columns={e_lat:'lat', e_lon:'lon', 'end_station_id':'station_id'})\n",
    "\n",
    "orig_agg = orig_agg.dropna(subset=['lat','lon'])\n",
    "dest_agg = dest_agg.dropna(subset=['lat','lon'])\n",
    "\n",
    "# GeoDataFrames (WGS84)\n",
    "gdf_o = gpd.GeoDataFrame(orig_agg, geometry=[Point(xy) for xy in zip(orig_agg['lon'].astype(float), orig_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "gdf_d = gpd.GeoDataFrame(dest_agg, geometry=[Point(xy) for xy in zip(dest_agg['lon'].astype(float), dest_agg['lat'].astype(float))], crs='EPSG:4326')\n",
    "\n",
    "# project to WebMercator for basemap if desired\n",
    "use_ctx = False\n",
    "if USE_CONTEXTILY:\n",
    "    try:\n",
    "        gdf_o_web = gdf_o.to_crs(epsg=3857)\n",
    "        gdf_d_web = gdf_d.to_crs(epsg=3857)\n",
    "        use_ctx = True\n",
    "    except Exception as e:\n",
    "        print('Contextily disabled (reprojection failed):', e)\n",
    "        gdf_o_web = gdf_o\n",
    "        gdf_d_web = gdf_d\n",
    "else:\n",
    "    gdf_o_web = gdf_o\n",
    "    gdf_d_web = gdf_d\n",
    "\n",
    "# ---------------------------\n",
    "# ROBUST EXTENT (2nd - 98th percentile)\n",
    "# ---------------------------\n",
    "# collect x,y arrays (projected)\n",
    "xs = pd.concat([\n",
    "    gdf_o_web.geometry.x.rename('x') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.x.rename('x') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "ys = pd.concat([\n",
    "    gdf_o_web.geometry.y.rename('y') if not gdf_o_web.empty else pd.Series(dtype=float),\n",
    "    gdf_d_web.geometry.y.rename('y') if not gdf_d_web.empty else pd.Series(dtype=float)\n",
    "], ignore_index=True)\n",
    "\n",
    "if len(xs) == 0 or len(ys) == 0 or not np.isfinite(xs.to_numpy()).any():\n",
    "    raise RuntimeError('No valid projected coordinates found to compute map extent.')\n",
    "\n",
    "low_pct, high_pct = 2, 98\n",
    "minx, maxx = np.percentile(xs, [low_pct, high_pct])\n",
    "miny, maxy = np.percentile(ys, [low_pct, high_pct])\n",
    "\n",
    "# fallback to full min/max if needed\n",
    "full_minx, full_maxx = xs.min(), xs.max()\n",
    "full_miny, full_maxy = ys.min(), ys.max()\n",
    "\n",
    "# add padding (at least 300 m)\n",
    "span_x = maxx - minx\n",
    "span_y = maxy - miny\n",
    "pad_x = max(span_x * 0.06, 300)\n",
    "pad_y = max(span_y * 0.06, 300)\n",
    "extent = (minx - pad_x, maxx + pad_x, miny - pad_y, maxy + pad_y)\n",
    "\n",
    "# safety clamp if extent absurdly large\n",
    "max_allowed_span = 200_000  # 200 km\n",
    "if (extent[1] - extent[0] > max_allowed_span) or (extent[3] - extent[2] > max_allowed_span):\n",
    "    print('WARNING: computed extent is very large. Falling back to full bounds.')\n",
    "    pad_x_f = max((full_maxx - full_minx) * 0.06, 300)\n",
    "    pad_y_f = max((full_maxy - full_miny) * 0.06, 300)\n",
    "    extent = (full_minx - pad_x_f, full_maxx + pad_x_f, full_miny - pad_y_f, full_maxy + pad_y_f)\n",
    "\n",
    "# print sample outliers (stations outside percentile window) to help debugging\n",
    "outlier_mask_o = (gdf_o_web.geometry.x < minx) | (gdf_o_web.geometry.x > maxx) | (gdf_o_web.geometry.y < miny) | (gdf_o_web.geometry.y > maxy)\n",
    "outlier_mask_d = (gdf_d_web.geometry.x < minx) | (gdf_d_web.geometry.x > maxx) | (gdf_d_web.geometry.y < miny) | (gdf_d_web.geometry.y > maxy)\n",
    "outliers_o = gdf_o_web.loc[outlier_mask_o, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "outliers_d = gdf_d_web.loc[outlier_mask_d, ['station_id', 'lat', 'lon']].drop_duplicates().head(10)\n",
    "if not outliers_o.empty or not outliers_d.empty:\n",
    "    print('Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):')\n",
    "    if not outliers_o.empty:\n",
    "        print(' Origin outliers:')\n",
    "        print(outliers_o.to_string(index=False))\n",
    "    if not outliers_d.empty:\n",
    "        print(' Destination outliers:')\n",
    "        print(outliers_d.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# plotting helpers\n",
    "# ---------------------------\n",
    "def sizes_from_counts(series, base=POINT_BASE):\n",
    "    arr = np.sqrt(series.fillna(0).astype(float) + 1.0)\n",
    "    if arr.max() > 0:\n",
    "        scaled = base * (arr / arr.max())\n",
    "    else:\n",
    "        scaled = np.full_like(arr, base * 0.2)\n",
    "    return np.clip(scaled, 2, base * 1.1)\n",
    "\n",
    "# main loop: produce map PNG and a separate bar PNG for each hour\n",
    "for h in range(24):\n",
    "    o_h = gdf_o_web[gdf_o_web['hour'] == h].copy()\n",
    "    d_h = gdf_d_web[gdf_d_web['hour'] == h].copy()\n",
    "\n",
    "    # compute maximum count for color scaling\n",
    "    max_count = int(max(o_h['count'].max() if not o_h.empty else 0,\n",
    "                        d_h['count'].max() if not d_h.empty else 0, 1))\n",
    "\n",
    "    # ---------- MAP FIGURE (orig left, dest right) ----------\n",
    "    fig, (ax_o, ax_d) = plt.subplots(1, 2, figsize=(14, 8), constrained_layout=True)\n",
    "\n",
    "    # set extent and add basemap first (so tiles cover area)\n",
    "    for ax in (ax_o, ax_d):\n",
    "        ax.set_xlim(extent[0], extent[1])\n",
    "        ax.set_ylim(extent[2], extent[3])\n",
    "\n",
    "    if use_ctx:\n",
    "        try:\n",
    "            import contextily as ctx\n",
    "            ctx.add_basemap(ax_o, crs=gdf_o_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "            ctx.add_basemap(ax_d, crs=gdf_d_web.crs.to_string(), source=ctx.providers.CartoDB.Positron)\n",
    "        except Exception as e:\n",
    "            print('contextily error (continuing without basemap):', e)\n",
    "\n",
    "    # faint background of all stations (for context)\n",
    "    if not gdf_o_web.empty:\n",
    "        ax_o.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "        ax_d.scatter(gdf_o_web.geometry.x, gdf_o_web.geometry.y, s=3, color='lightgrey', alpha=0.6, zorder=0)\n",
    "\n",
    "    # plot origins (red)\n",
    "    if not o_h.empty:\n",
    "        sizes_o = sizes_from_counts(o_h['count'])\n",
    "        cmap_o = plt.colormaps.get_cmap(CMAP_ORIG)\n",
    "        norm_o = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_o = cmap_o(norm_o(o_h['count'].astype(float)))\n",
    "        ax_o.scatter(o_h.geometry.x, o_h.geometry.y, s=sizes_o, color=colors_o, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_o = cm.ScalarMappable(norm=norm_o, cmap=CMAP_ORIG)\n",
    "        sm_o._A = []\n",
    "        fig.colorbar(sm_o, ax=ax_o, fraction=0.046, pad=0.02).set_label('Origin count (hour)')\n",
    "\n",
    "    ax_o.set_title(f'Origins — hour {h}')\n",
    "    ax_o.axis('off')\n",
    "\n",
    "    # plot destinations (blue)\n",
    "    if not d_h.empty:\n",
    "        sizes_d = sizes_from_counts(d_h['count'])\n",
    "        cmap_d = plt.colormaps.get_cmap(CMAP_DEST)\n",
    "        norm_d = Normalize(vmin=0, vmax=max_count)\n",
    "        colors_d = cmap_d(norm_d(d_h['count'].astype(float)))\n",
    "        ax_d.scatter(d_h.geometry.x, d_h.geometry.y, s=sizes_d, color=colors_d, edgecolors='k', linewidth=0.2, zorder=3)\n",
    "        sm_d = cm.ScalarMappable(norm=norm_d, cmap=CMAP_DEST)\n",
    "        sm_d._A = []\n",
    "        fig.colorbar(sm_d, ax=ax_d, fraction=0.046, pad=0.02).set_label('Destination count (hour)')\n",
    "\n",
    "    ax_d.set_title(f'Destinations — hour {h}')\n",
    "    ax_d.axis('off')\n",
    "\n",
    "    map_out = MAP_DIR / f'hour_{h:02d}_maps.png'\n",
    "    fig.savefig(map_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # ---------- BARCHART FIGURE (separate) ----------\n",
    "    fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5), constrained_layout=True)\n",
    "\n",
    "    if not o_h.empty:\n",
    "        top_o = o_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_o = top_o['station_id'].astype(str).values[::-1]   # reversed for horizontal bars\n",
    "        counts_o = top_o['count'].values[::-1]\n",
    "        ax1.barh(labels_o, counts_o, color='tab:red')\n",
    "        ax1.set_title(f'Top {TOP_K_BARS} origin stations (hour {h})')\n",
    "        ax1.set_xlabel('Starts')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'No origin data', ha='center', va='center')\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "    if not d_h.empty:\n",
    "        top_d = d_h.sort_values('count', ascending=False).head(TOP_K_BARS)\n",
    "        labels_d = top_d['station_id'].astype(str).values[::-1]\n",
    "        counts_d = top_d['count'].values[::-1]\n",
    "        ax2.barh(labels_d, counts_d, color='tab:blue')\n",
    "        ax2.set_title(f'Top {TOP_K_BARS} destination stations (hour {h})')\n",
    "        ax2.set_xlabel('Ends')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No destination data', ha='center', va='center')\n",
    "        ax2.set_axis_off()\n",
    "\n",
    "    bars_out = BARS_DIR / f'hour_{h:02d}_bars.png'\n",
    "    fig2.savefig(bars_out, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close(fig2)\n",
    "\n",
    "print('Saved maps to', MAP_DIR)\n",
    "print('Saved bar charts to', BARS_DIR)\n"
   ],
   "id": "8bb4164da9370389",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found station coordinates outside the central 2-98 percentile (sample, up to 10 shown each):\n",
      " Origin outliers:\n",
      " station_id       lat       lon\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "       1870 55.990530 -3.385597\n",
      " Destination outliers:\n",
      " station_id       lat       lon\n",
      "        280 53.395525 -2.990138\n",
      "       1033 55.931935 -3.073046\n",
      "       1723 55.908786 -3.320170\n",
      "       1723 55.908810 -3.320142\n",
      "       1724 55.908404 -3.328825\n",
      "       1819 55.908823 -3.320113\n",
      "       1820 55.908413 -3.328784\n",
      "       1866 55.989900 -3.397773\n",
      "       1868 55.987743 -3.403752\n",
      "       1869 55.990182 -3.404604\n",
      "Saved maps to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\maps\n",
      "Saved bar charts to E:\\Uni_PGT\\visualisation_outputs\\hourly_maps_v3_fixed_extent\\bars\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
